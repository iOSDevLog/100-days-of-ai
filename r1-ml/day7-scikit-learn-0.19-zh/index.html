<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link href="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/style.css" rel="stylesheet" type="text/css" />
    <title>scikit-learn 0.19 中文文档</title></head><body><div class="calibre" id="calibre_link-95">
<h1 class="calibre1">scikit-learn 0.19 中文文档</h1>
<p class="calibre2">来源：<a href="http://sklearn.apachecn.org/cn/0.19.0/" class="calibre3 pcalibre">ApacheCN</a></p>
</div>


<div class="calibre" id="calibre_link-260">
<h1 class="calibre1">用户指南</h1>
</div>


<div class="calibre" id="calibre_link-96">
<span id="calibre_link-261" class="calibre4"></span><h1 class="calibre5">1. 监督学习</h1>
<div class="toctree-wrapper">
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html">1.1. 广义线性模型</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#ordinary-least-squares">1.1.1. 普通最小二乘法</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id5">1.1.1.1. 普通最小二乘法复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#ridge-regression">1.1.2. 岭回归</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id11">1.1.2.1. 岭回归的复杂度</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id12">1.1.2.2. 设置正则化参数：广义交叉验证</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#lasso">1.1.3. Lasso</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id14">1.1.3.1. 设置正则化参数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id15">1.1.3.1.1. 使用交叉验证</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id18">1.1.3.1.2. 基于信息标准的模型选择</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#svm">1.1.3.1.3. 与 SVM 的正则化参数的比较</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#multi-task-lasso">1.1.4. 多任务 Lasso</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#elastic-net">1.1.5. 弹性网络</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#multi-task-elastic-net">1.1.6. 多任务弹性网络</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#least-angle-regression">1.1.7. 最小角回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#lars-lasso">1.1.8. LARS Lasso</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id25">1.1.8.1. 数学表达式</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#omp">1.1.9. 正交匹配追踪法（OMP）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#bayesian-regression">1.1.10. 贝叶斯回归</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#bayesian-ridge-regression">1.1.10.1. 贝叶斯岭回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#ard">1.1.10.2. 主动相关决策理论 - ARD</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#logistic">1.1.11. logistic 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#sgd">1.1.12. 随机梯度下降, SGD</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#perceptron">1.1.13. Perceptron（感知器）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#passive-aggressive-algorithms">1.1.14. Passive Aggressive Algorithms（被动攻击算法）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#robustness-regression-outliers">1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id51">1.1.15.1. 各种使用场景与相关概念</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#ransac-random-sample-consensus">1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id52">1.1.15.2.1. 算法细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#theil-sen">1.1.15.3. Theil-Sen 预估器: 广义中值估计</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id53">1.1.15.3.1. 算法理论细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#huber">1.1.15.4. Huber 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#id56">1.1.15.5. 注意</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/linear_model.html#polynomial-regression">1.1.16. 多项式回归：用基函数展开线性模型</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html">1.2. 线性和二次判别分析</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html#id2">1.2.1. 使用线性判别分析来降维</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html#id3">1.2.2. LDA 和 QDA 分类器的数学公式</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html#lda">1.2.3. LDA 的降维数学公式</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html#shrinkage">1.2.4. Shrinkage（收缩）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/lda_qda.html#id6">1.2.5. 预估算法</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_ridge.html">1.3. 内核岭回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html">1.4. 支持向量机</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-classification">1.4.1. 分类</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-multi-class">1.4.1.1. 多元分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#scores-probabilities">1.4.1.2. 得分和概率</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#id5">1.4.1.3. 非均衡问题</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-regression">1.4.2. 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#novelty">1.4.3. 密度估计, 异常（novelty）检测</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#id7">1.4.4. 复杂度</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#id8">1.4.5. 使用诀窍</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-kernels">1.4.6. 核函数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#id10">1.4.6.1. 自定义核</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#python">1.4.6.1.1. 使用 python 函数作为内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#gram">1.4.6.1.2. 使用 Gram 矩阵</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#rbf">1.4.6.1.3. RBF 内核参数</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-mathematical-formulation">1.4.7. 数学公式</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svc">1.4.7.1. SVC</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#nusvc">1.4.7.2. NuSVC</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svr">1.4.7.3. SVR</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/svm.html#svm-implementation-details">1.4.8. 实现细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html">1.5. 随机梯度下降</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id3">1.5.1. 分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id4">1.5.2. 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id5">1.5.3. 稀疏数据的随机梯度下降</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id6">1.5.4. 复杂度</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id7">1.5.5. 实用小贴士</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#sgd-mathematical-formulation">1.5.6. 数学描述</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id9">1.5.6.1. SGD</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/sgd.html#id10">1.5.7. 实现细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html">1.6. 最近邻</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#unsupervised-neighbors">1.6.1. 无监督最近邻</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#id3">1.6.1.1. 找到最近邻</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#kdtree-balltree">1.6.1.2. KDTree 和 BallTree 类</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#classification">1.6.2. 最近邻分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#regression">1.6.3. 最近邻回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#id6">1.6.4. 最近邻算法</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#brute-force">1.6.4.1. 暴力计算</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#k-d">1.6.4.2. K-D 树</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#ball">1.6.4.3. Ball 树</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#id10">1.6.4.4. 最近邻算法的选择</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#leaf-size">1.6.4.5. <code class="docutils"><span class="calibre4">leaf_size</span></code> 的影响</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#nearest-centroid-classifier">1.6.5. 最近质心分类</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neighbors.html#id12">1.6.5.1. 最近缩小质心</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html">1.7. 高斯过程</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#gpr">1.7.1. 高斯过程回归（GPR）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id4">1.7.2. GPR 示例</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id5">1.7.2.1. 具有噪声级的 GPR 估计</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#gpr-kernel-ridge-regression">1.7.2.2. GPR 和内核岭回归(Kernel Ridge Regression)的比较</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#mauna-loa-co2-grr">1.7.2.3. Mauna Loa CO2 数据中的 GRR</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#gpc">1.7.3. 高斯过程分类（GPC）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id7">1.7.4. GPC 示例</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id8">1.7.4.1. GPC 概率预测</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#gpc-xor">1.7.4.2. GPC 在 XOR 数据集上的举例说明</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#iris-gpc">1.7.4.3. iris 数据集上的高斯过程分类（GPC）</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#gp-kernels">1.7.5. 高斯过程内核</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#api">1.7.5.1. 高斯过程内核 API</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id11">1.7.5.2. 基础内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id12">1.7.5.3. 内核操作</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id13">1.7.5.4. 径向基函数内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#matern">1.7.5.5. Matérn 内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id15">1.7.5.6. 有理二次内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id16">1.7.5.7. 正弦平方内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id17">1.7.5.8. 点乘内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id18">1.7.5.9. 参考文献</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id19">1.7.6. 传统高斯过程</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id20">1.7.6.1. 回归实例介绍</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id21">1.7.6.2. 噪声数据拟合</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id22">1.7.6.3. 数学形式</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id23">1.7.6.3.1. 初始假设</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#blup">1.7.6.3.2. 最佳线性无偏预测（BLUP）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#eblup">1.7.6.3.3. 经验最佳线性无偏估计（EBLUP）</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#correlation-models">1.7.6.4. 关联模型</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#regression-models">1.7.6.5. 回归模型</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/gaussian_process.html#id26">1.7.6.6. 实现细节</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_decomposition.html">1.8. 交叉分解</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/naive_bayes.html">1.9. 朴素贝叶斯</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/naive_bayes.html#gaussian-naive-bayes">1.9.1. 高斯朴素贝叶斯</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/naive_bayes.html#multinomial-naive-bayes">1.9.2. 多项分布朴素贝叶斯</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/naive_bayes.html#bernoulli-naive-bayes">1.9.3. 伯努利朴素贝叶斯</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/naive_bayes.html#id5">1.9.4. 堆外朴素贝叶斯模型拟合</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html">1.10. 决策树</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-classification">1.10.1. 分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-regression">1.10.2. 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-multioutput">1.10.3. 多值输出问题</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-complexity">1.10.4. 复杂度分析</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#id6">1.10.5. 实际使用技巧</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-algorithms">1.10.6. 决策树算法: ID3, C4.5, C5.0 和 CART</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#tree-mathematical-formulation">1.10.7. 数学表达</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#id9">1.10.7.1. 分类标准</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/tree.html#id10">1.10.7.2. 回归标准</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html">1.11. 集成方法</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#bagging-meta-estimator-bagging">1.11.1. Bagging meta-estimator（Bagging 元估计器）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#forest">1.11.2. 由随机树组成的森林</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id8">1.11.2.1. 随机森林</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id10">1.11.2.2. 极限随机树</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id11">1.11.2.3. 参数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id12">1.11.2.4. 并行化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#random-forest-feature-importance">1.11.2.5. 特征重要性评估</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#random-trees-embedding">1.11.2.6. 完全随机树嵌入</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#adaboost">1.11.3. AdaBoost</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id20">1.11.3.1. 使用方法</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#gradient-tree-boosting">1.11.4. Gradient Tree Boosting（梯度树提升）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id22">1.11.4.1. 分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id23">1.11.4.2. 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#gradient-boosting-warm-start">1.11.4.3. 训练额外的弱学习器</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#gradient-boosting-tree-size">1.11.4.4. 控制树的大小</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#mathematical-formulation">1.11.4.5. Mathematical formulation（数学公式）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#loss-functions">1.11.4.5.1. Loss Functions（损失函数）</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#regularization">1.11.4.6. Regularization（正则化）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#shrinkage">1.11.4.6.1. 收缩率 (Shrinkage)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#subsampling">1.11.4.6.2. 子采样 (Subsampling)</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#interpretation">1.11.4.7. Interpretation（解释性）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#feature-importance">1.11.4.7.1. Feature importance（特征重要性）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#partial-dependence">1.11.4.7.2. Partial dependence（部分依赖）</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#voting-classifier">1.11.5. Voting Classifier（投票分类器）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id38">1.11.5.1. 多数类标签 (又称为 多数/硬投票)</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id39">1.11.5.1.1. 用法</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id40">1.11.5.2. 加权平均概率 （软投票）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#votingclassifier-gridsearch">1.11.5.3. 投票分类器（VotingClassifier）在网格搜索（GridSearch）应用</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/ensemble.html#id41">1.11.5.3.1. 用法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html">1.12. 多类和多标签算法</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id4">1.12.1. 多标签分类格式</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#ovr-classification">1.12.2. 1对其余</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id6">1.12.2.1. 多类学习</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id7">1.12.2.2. 多标签学习</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#ovo-classification">1.12.3. 1对1</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id9">1.12.3.1. 多类别学习</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#ecoc">1.12.4. 误差校正输出代码</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id12">1.12.4.1. 多类别学习</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id14">1.12.5. 多输出回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id15">1.12.6. 多输出分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/multiclass.html#id16">1.12.7. 链式分类器</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html">1.13. 特征选择</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#variance-threshold">1.13.1. 移除低方差特征</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#univariate-feature-selection">1.13.2. 单变量特征选择</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#rfe">1.13.3. 递归式特征消除</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#selectfrommodel">1.13.4. 使用 SelectFromModel 选取特征</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#l1">1.13.4.1. 基于 L1 的特征选取</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#tree">1.13.4.2. 基于 Tree（树）的特征选取</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_selection.html#pipeline">1.13.5. 特征选取作为 pipeline（管道）的一部分</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/label_propagation.html">1.14. 半监督学习</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/label_propagation.html#label-propagation">1.14.1. 标签传播</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/isotonic.html">1.15. 等式回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/calibration.html">1.16. 概率校准</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html">1.17. 神经网络模型（有监督）</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#multilayer-perceptron">1.17.1. 多层感知器</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id5">1.17.2. 分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id6">1.17.3. 回归</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id7">1.17.4. 正则化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id8">1.17.5. 算法</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id9">1.17.6. 复杂性</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#id10">1.17.7. 数学公式</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#mlp-tips">1.17.8. 实用技巧</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_supervised.html#warm-start">1.17.9. 使用 warm_start 的更多控制</a></li>
</ul>
</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-97">
<span id="calibre_link-262" class="calibre4"></span><h1 class="calibre5">1.1. 广义线性模型</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@专业吹牛逼的小明</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Gladiator</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@瓜牛</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@年纪大了反应慢了</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Hazekiah</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@BWM-蜜蜂</a><br class="calibre9" />  
    </div>
<p class="calibre10">下面是一组用于回归的方法，其中目标期望值 y是输入变量 x 的线性组合。 在数学概念中，如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000505.jpg" alt="\hat{y}" /> 是预测值
value.</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000161.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p" class="math" /></p>
</div>
<p class="calibre10">在整个模块中，我们定义向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000336.jpg" alt="w = (w_1,..., w_p)" /> 作为 <code class="docutils"><span class="calibre4">coef_</span></code> 定义 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000023.jpg" alt="w_0" /> 作为 <code class="docutils"><span class="calibre4">intercept_</span></code>.</p>
<p class="calibre10">如果需要使用广义线性模型进行分类，请参阅 <a class="calibre3 pcalibre" href="#calibre_link-98"><span class="calibre4">logistic 回归</span></a> .
<a class="calibre3 pcalibre" href="#calibre_link-98"><span class="calibre4">logistic 回归</span></a>.</p>
<div class="toctree-wrapper" id="calibre_link-100">
<span id="calibre_link-263" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.1. 普通最小二乘法</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="docutils"><span class="calibre4">LinearRegression</span></code></a> 适合一个带有系数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000473.jpg" alt="w = (w_1, ..., w_p)" /> 的线性模型,使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。其数学表达式为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000352.jpg" alt="\underset{w}{min\,} {|| X w - y||_2}^2" class="math" /></p>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ols.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ols_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000735.jpg" class="calibre11" /></a>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="docutils"><span class="calibre4">LinearRegression</span></code></a> 会调用 <code class="docutils"><span class="calibre4">fit</span></code> 方法来拟合数组 X, y，并且将线性模型的系数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 存储在其成员变量 <a href="#calibre_link-99" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-264">``</span></a>coef_``中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LinearRegression</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span> <span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">coef_</span>
<span class="calibre4">array([ 0.5,  0.5])</span>
</pre>
</div>
</div>
<p class="calibre10">然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。当各项是相关的，且设计矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 的各列近似线性相关，那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。例如，在没有实验设计的情况下收集到的数据，这种多重共线性(multicollinearity) 的情况可能真的会出现。</p>
<div class="toctree-wrapper">
<p class="calibre10">举例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"><span class="calibre4">Linear Regression Example</span></a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-265">
<h3 class="sigil_not_in_toc1">1.1.1.1. 普通最小二乘法复杂度</h3>
<p class="calibre2">该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个 size 为 (n, p) 的矩阵，设 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000397.jpg" alt="n \geq p" /> ，则该方法花费的成本为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000821.jpg" alt="O(n p^2)" /></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-266">
<span id="calibre_link-267" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.2. 岭回归</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 回归通过对系数的大小施加惩罚来解决
<a class="calibre3 pcalibre" href="#calibre_link-100"><span class="calibre4">普通最小二乘法</span></a> (普通最小二乘)的一些问题。 岭系数最小化一个带罚项的残差平方和，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000843.jpg" alt="\underset{w}{min\,} {{|| X w - y||_2}^2 + \alpha {||w||_2}^2}" class="math" /></p>
</div>
<p class="calibre10">其中， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000448.jpg" alt="\alpha \geq 0" /> 是控制收缩量复杂性的参数： <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" />, 的值越大，收缩量越大，因此系数对共线性变得更加鲁棒。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ridge_path.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ridge_path_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000485.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">与其他线性模型一样， <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 采用 <code class="docutils"><span class="calibre4">fit</span></code> 将采用其 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 存储在其 <code class="docutils"><span class="calibre4">coef_</span></code> 成员中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">Ridge</span> <span class="calibre4">(</span><span class="calibre4">alpha</span> <span class="calibre4">=</span> <span class="calibre4">.</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span> <span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span> 
<span class="calibre4">Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,</span>
<span class="calibre4">      normalize=False, random_state=None, solver='auto', tol=0.001)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">coef_</span>
<span class="calibre4">array([ 0.34545455,  0.34545455])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">intercept_</span> 
<span class="calibre4">0.13636...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">举例:</p>
<ul class="calibre6">
<li class="toctree-l">:ref:<a href="#calibre_link-101" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-268">`</span></a>sphx_glr_auto_examples_linear_model_plot_ridge_path.py`( 作为正则化的函数，绘制岭系数 )</li>
<li class="toctree-l">:ref:<a href="#calibre_link-102" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-269">`</span></a>sphx_glr_auto_examples_text_document_classification_20newsgroups.py`( 使用稀疏特征的文本文档分类 )</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-270">
<h3 class="sigil_not_in_toc1">1.1.2.1. 岭回归的复杂度</h3>
<p class="calibre2">这种方法与 <a class="calibre3 pcalibre" href="#calibre_link-100"><span class="calibre4">普通最小二乘法</span></a> (普通最小二乘方法)的复杂度是相同的.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-271">
<h3 class="sigil_not_in_toc1">1.1.2.2. 设置正则化参数：广义交叉验证</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="docutils"><span class="calibre4">RidgeCV</span></code></a> 通过内置的 Alpha 参数的交叉验证来实现岭回归。 该对象与 GridSearchCV 的使用方法相同，只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的留一验证方法（LOO-CV）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">RidgeCV</span><span class="calibre4">(</span><span class="calibre4">alphas</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">1.0</span><span class="calibre4">,</span> <span class="calibre4">10.0</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>       
<span class="calibre4">RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,</span>
<span class="calibre4">    normalize=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">alpha_</span>                                      
<span class="calibre4">0.1</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l">“Notes on Regularized Least Squares”, Rifkin &amp; Lippert (<a class="calibre3 pcalibre" href="http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf">technical report</a>,
<a class="calibre3 pcalibre" href="http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf">course slides</a>).</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-272">
<span id="calibre_link-273" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.3. Lasso</h2>
<p class="calibre2">The <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="docutils"><span class="calibre4">Lasso</span></code></a> 是估计稀疏系数的线性模型。 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。 因此，Lasso及其变体是压缩感知领域的基础。 在一定条件下，它可以恢复一组非零权重的精确集  (见 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="calibre4">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a>).</p>
<p class="calibre10">在数学上，它由一个线性模型组成，以 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> 为准。 其目标函数的最小化是:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000617.jpg" alt="\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}" class="math" /></p>
</div>
<p class="calibre10">lasso estimate 解决了加上罚项 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000749.jpg" alt="\alpha ||w||_1" /> 的最小二乘法的最小化，其中， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> 是一个常数， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000050.jpg" alt="||w||_1" /> 是参数向量的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" />-norm 范数。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="docutils"><span class="calibre4">Lasso</span></code></a> 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数。 查看 <a class="calibre3 pcalibre" href="#calibre_link-103"><span class="calibre4">最小角回归</span></a> 用于另一个实现:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">Lasso</span><span class="calibre4">(</span><span class="calibre4">alpha</span> <span class="calibre4">=</span> <span class="calibre4">0.1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="calibre4">   normalize=False, positive=False, precompute=False, random_state=None,</span>
<span class="calibre4">   selection='cyclic', tol=0.0001, warm_start=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">array([ 0.8])</span>
</pre>
</div>
</div>
<p class="calibre10">对于较低级别的任务，同样有用的是:func:<cite class="calibre13">lasso_path</cite>。它能够通过搜索所有可能的路径上的值来计算系数。</p>
<div class="toctree-wrapper">
<p class="calibre10">举例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="calibre4">Lasso and Elastic Net for Sparse Signals</span></a> (稀疏信号的套索和弹性网)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="calibre4">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a> (压缩感知：L1先验(Lasso)的断层扫描重建)</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><strong class="calibre14">Feature selection with Lasso(使用 Lasso 进行 Feature 的选择)</strong></p>
<p class="calibre10">由于 Lasso 回归产生稀疏模型，因此可以用于执行特征选择，详见
<a class="calibre3 pcalibre" href="feature_selection.html#l1-feature-selection"><span class="calibre4">基于 L1 的特征选取</span></a> (基于L1的特征选择).</p>
</div>
<div class="toctree-wrapper" id="calibre_link-274">
<h3 class="sigil_not_in_toc1">1.1.3.1. 设置正则化参数</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><code class="docutils"><span class="calibre4">alpha</span></code> 参数控制估计系数的稀疏度。</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-275">
<h4 class="sigil_not_in_toc1">1.1.3.1.1. 使用交叉验证</h4>
<p class="calibre2">scikit-learn 通过交叉验证来公开设置 Lasso <code class="docutils"><span class="calibre4">alpha</span></code> 参数的对象: <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="docutils"><span class="calibre4">LassoCV</span></code></a> and <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="docutils"><span class="calibre4">LassoLarsCV</span></code></a>。
<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="docutils"><span class="calibre4">LassoLarsCV</span></code></a> 是基于下面解释的 :ref:<a href="#calibre_link-104" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-276">`</span></a>least_angle_regression`(最小角度回归)算法。</p>
<p class="calibre10">对于具有许多线性回归的高维数据集， <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="docutils"><span class="calibre4">LassoCV</span></code></a> 最常见。 然而，<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="docutils"><span class="calibre4">LassoLarsCV</span></code></a> 在寻找 <cite class="calibre13">alpha</cite> parameter 参数值上更具有优势，而且如果样本数量与特征数量相比非常小时，通常 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="docutils"><span class="calibre4">LassoLarsCV</span></code></a> 比 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="docutils"><span class="calibre4">LassoCV</span></code></a> 要快。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000037.jpg" class="calibre16" /></a> <a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000706.jpg" class="calibre16" /></a></strong></p>
</div>
<div class="toctree-wrapper" id="calibre_link-277">
<h4 class="sigil_not_in_toc1">1.1.3.1.2. 基于信息标准的模型选择</h4>
<p class="calibre2">有多种选择时，估计器 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code class="docutils"><span class="calibre4">LassoLarsIC</span></code></a> 建议使用 Akaike information criterion （Akaike 信息准则）（AIC）和 Bayes Information criterion （贝叶斯信息准则）（BIC）。 当使用 k-fold 交叉验证时，正则化路径只计算一次而不是k + 1次，所以找到α的最优值是一种计算上更便宜的替代方法。 然而，这样的标准需要对解决方案的自由度进行适当的估计，对于大样本（渐近结果）导出，并假设模型是正确的，即数据实际上是由该模型生成的。 当问题严重受限（比样本更多的特征）时，他们也倾向于打破。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lasso_model_selection_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000187.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">举例:</p>
<ul class="calibre6">
<li class="toctree-l">:ref:<a href="#calibre_link-105" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-278">`</span></a>sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`(Lasso 型号选择：交叉验证/AIC/BIC)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-279">
<h4 class="sigil_not_in_toc1">1.1.3.1.3. 与 SVM 的正则化参数的比较</h4>
<p class="calibre2">根据估计器和模型优化的精确目标函数，在 <code class="docutils"><span class="calibre4">alpha</span></code> 和 SVM 的正则化参数之间是等值的,其中
<code class="docutils"><span class="calibre4">C</span></code> 是通过 <code class="docutils"><span class="calibre4">alpha</span> <span class="calibre4">=</span> <span class="calibre4">1</span> <span class="calibre4">/</span> <span class="calibre4">C</span></code> 或者 <code class="docutils"><span class="calibre4">alpha</span> <span class="calibre4">=</span> <span class="calibre4">1</span> <span class="calibre4">/</span> <span class="calibre4">(n_samples</span> <span class="calibre4">*</span> <span class="calibre4">C)</span></code> 得到的。</p>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-280">
<span id="calibre_link-281" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.4. 多任务 Lasso</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="docutils"><span class="calibre4">MultiTaskLasso</span></code></a> 是一个估计多元回归稀疏系数的线性模型： <code class="docutils"><span class="calibre4">y</span></code> 是一个 <code class="docutils"><span class="calibre4">(n_samples,</span> <span class="calibre4">n_tasks)</span></code> 的二维数组，其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。</div>
</blockquote>
<p class="calibre10">下图比较了通过使用简单的 Lasso 或 MultiTaskLasso 得到的 W 中非零的位置。 Lasso 估计分散的产生着非零值，而 MultiTaskLasso 的所有列都是非零的。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000532.jpg" class="calibre17" /></a> <a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000411.jpg" class="calibre16" /></a></strong></p>
<p class="calibre10">
<strong class="calibre14">拟合 time-series model ( 时间序列模型 )，强制任何活动的功能始终处于活动状态。</strong></p>
<div class="toctree-wrapper">
<p class="calibre10">举例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py"><span class="calibre4">Joint feature selection with multi-task Lasso</span></a> (联合功能选择与多任务Lasso)</li>
</ul>
</div>
<p class="calibre10">在数学上，它由一个线性模型组成，以混合的
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000282.jpg" alt="\ell_2" /> 作为正则化器进行训练。目标函数最小化是：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000791.jpg" alt="\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro} ^ 2 + \alpha ||W||_{21}}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000631.jpg" alt="Fro" /> 表示 Frobenius 标准：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000527.jpg" alt="||A||_{Fro} = \sqrt{\sum_{ij} a_{ij}^2}" class="math" /></p>
</div>
<p class="calibre10">并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000282.jpg" alt="\ell_2" /> 读取为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000378.jpg" alt="||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}" class="math" /></p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="docutils"><span class="calibre4">MultiTaskLasso</span></code></a> 类中的实现使用了坐标下降作为拟合系数的算法。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-282">
<span id="calibre_link-283" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.5. 弹性网络</h2>
<p class="calibre2"><code class="docutils"><span class="calibre4">弹性网络</span></code> 是一种使用L1,L2范数作为先验正则项训练的线性回归模型。 这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="docutils"><span class="calibre4">Lasso</span></code></a> 一样, 但是它仍然保持
一些像 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 的正则性质。我们可利用 <code class="docutils"><span class="calibre4">l1_ratio</span></code> 参数控制L1和L2的凸组合。</p>
<p class="calibre10">弹性网络在很多特征互相联系的情况下是非常有用的。Lasso很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p class="calibre10">在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在循环过程（Under rotate）中继承 Ridge 的稳定性。</p>
<p class="calibre10">在这里，最小化的目标函数是</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000835.jpg" alt="\underset{w}{min\,} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 + \frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}" class="math" /></p>
</div>
<div class="toctree-wrapper" id="calibre_link-284">

<p class="calibre10"><span class="calibre4"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="docutils"><span class="calibre4">ElasticNetCV</span></code></a> 类可以通过交叉验证来设置参数</span></p>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">alpha</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" />) 和 <code class="docutils"><span class="calibre4">l1_ratio</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" />) 。</p>
<div class="toctree-wrapper">
<p class="calibre10">Examples:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="calibre4">Lasso and Elastic Net for Sparse Signals</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py"><span class="calibre4">Lasso and Elastic Net</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-285">
<span id="calibre_link-286" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.6. 多任务弹性网络</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="docutils"><span class="calibre4">MultiTaskElasticNet</span></code></a> 是一个对多回归问题估算稀疏参数的弹性网络: <code class="docutils"><span class="calibre4">Y</span></code> 是一个二维数组，形状是 <code class="docutils"><span class="calibre4">(n_samples,n_tasks)</span></code>。  其限制条件是和其他回归问题一样，是选择的特征，也称为 tasks.。</div>
</blockquote>
<p class="calibre10">从数学上来说， 它包含一个用
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000282.jpg" alt="\ell_2" /> 先验 and <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000282.jpg" alt="\ell_2" /> 先验为正则项训练的线性模型
目标函数就是最小化:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000328.jpg" alt="\underset{W}{min\,} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro}^2 + \alpha \rho ||W||_{2 1} + \frac{\alpha(1-\rho)}{2} ||W||_{Fro}^2}" class="math" /></p>
</div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="docutils"><span class="calibre4">MultiTaskElasticNet</span></code></a> 类中的实现采用了坐标下降法求解参数。</p>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="docutils"><span class="calibre4">MultiTaskElasticNetCV</span></code></a> 中可以通过交叉验证来设置参数
<code class="docutils"><span class="calibre4">alpha</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" />) 和 <code class="docutils"><span class="calibre4">l1_ratio</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" />) 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-103">
<span id="calibre_link-287" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.7. 最小角回归</h2>
<p class="calibre2">最小角回归 (LARS) 是对高维数据的回归算法， 由Bradley Efron, Trevor Hastie, Iain
Johnstone 和 Robert Tibshirani开发完成。 LARS和逐步回归很像。 在每一步，它寻找与响应最有关联的
预测。当有很多预测由相同的关联时，它没有继续利用相同的预测，而是在这些预测中找出应该等角的方向。</p>
<p class="calibre10">LARS的优点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">当p &gt;&gt; n，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)</li>
<li class="toctree-l">它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。</li>
<li class="toctree-l">它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。</li>
<li class="toctree-l">如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉
上的判断一样，而且还更加稳定。</li>
<li class="toctree-l">它也很容易改变，为其他估算器提供解，比如Lasso。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">LARS的缺点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">因为LARS是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。这个问题，在2004年统计年鉴的文章由Weisberg详细讨论。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">LARS模型可以在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="docutils"><span class="calibre4">Lars</span></code></a> ，或者它的底层实现 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="docutils"><span class="calibre4">lars_path</span></code></a> 中被使用。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-288">
<h2 class="sigil_not_in_toc">1.1.8. LARS Lasso</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="docutils"><span class="calibre4">LassoLars</span></code></a> 是一个使用LARS算法的lasso模型，
不同于基于坐标下降法的实现，它可以得到一个精确解，也就是一个
关于自身参数标准化后的一个分段线性解。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_lars.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lasso_lars_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000523.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LassoLars</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=.</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>  
<span class="calibre4">LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,</span>
<span class="calibre4">     fit_path=True, max_iter=500, normalize=True, positive=False,</span>
<span class="calibre4">     precompute='auto', verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">coef_</span>    
<span class="calibre4">array([ 0.717157...,  0.        ])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py"><span class="calibre4">Lasso path using LARS</span></a></li>
</ul>
</div>
<p class="calibre10">Lars算法提供了一个可以几乎无代价的给出正则化系数的完整路径，因此常利用函数 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="docutils"><span class="calibre4">lars_path</span></code></a> 来取回路径。</p>
<div class="toctree-wrapper" id="calibre_link-289">
<h3 class="sigil_not_in_toc1">1.1.8.1. 数学表达式</h3>
<p class="calibre2">该算法和逐步回归非常相似，但是它没有在每一步包含变量，它估计的参数是根据与
其他剩余变量的联系来增加的。</p>
<p class="calibre10">该算法没有给出一个向量的结果，而是在LARS的解中，对每一个变量进行总体变量的L1正则化后显示的一条曲线。
完全的参数路径存在``coef_path_``下。它的尺寸是 (n_features, max_features+1)。 其中第一列通常是全0列。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Original Algorithm is detailed in the paper <a class="calibre3 pcalibre" href="http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">Least Angle Regression</a>
by Hastie et al.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-290">
<span id="calibre_link-291" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.9. 正交匹配追踪法（OMP）</h2>
<p class="calibre2"><code class="docutils"><span class="calibre4">OrthogonalMatchingPursuit(正交匹配追踪法)</span></code> 和 <code class="docutils"><span class="calibre4">orthogonal_mp(正交匹配追踪)</span></code>
使用了OMP算法近似拟合了一个带限制的线性模型，该限制限制了模型的非0系数(例：L0范数)。</p>
<p class="calibre10">就像最小角回归一样，作为一个前向特征选择方法，正交匹配追踪法可以近似一个固定非0元素的最优
向量解:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000335.jpg" alt="\text{arg\,min\,} ||y - X\gamma||_2^2 \text{ subject to } \ ||\gamma||_0 \leq n_{nonzero\_coefs}" class="math" /></p>
</div>
<p class="calibre10">正交匹配追踪法也可以不用特定的非0参数元素个数做限制，而是利用别的特定函数定义其损失函数。
这个可以表示为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000535.jpg" alt="\text{arg\,min\,} ||\gamma||_0 \text{ subject to } ||y-X\gamma||_2^2 \ \leq \text{tol}" class="math" /></p>
</div>
<p class="calibre10">OMP是基于每一步的贪心算法，其每一步元素都是与当前残差高度相关的。它跟较为简单的匹配追踪
（MP）很相似，但是相比MP更好，在每一次迭代中，可以利用正交投影到之前选择的字典元素重新计算残差。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py"><span class="calibre4">Orthogonal Matching Pursuit</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">Matching pursuits with time-frequency dictionaries</a>,
S. G. Mallat, Z. Zhang,</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-292">
<span id="calibre_link-293" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.10. 贝叶斯回归</h2>
<p class="calibre2">贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。</p>
<p class="calibre10">上述过程可以通过引入 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors">无信息先验</a>
于模型中的超参数来完成。
在 <cite class="calibre13">岭回归</cite> 中使用的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000253.jpg" alt="\ell_{2}" /> 正则项相当于在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 为高斯先验条件下，且此先验的精确度为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000691.jpg" alt="\lambda^{-1}" />
求最大后验估计。在这里，我们没有手工调参数lambda，而是让他作为一个变量，通过数据中估计得到。</p>
<p class="calibre10">为了得到一个全概率模型，输出 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 也被认为是关于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000143.jpg" alt="X w" />:的高斯分布。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000700.jpg" alt="p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha)" class="math" /></p>
</div>
<p class="calibre10">Alpha 在这里也是作为一个变量，通过数据中估计得到.</p>
<p class="calibre10">贝叶斯回归有如下几个优点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">它能根据已有的数据进行改变。</li>
<li class="toctree-l">它能在估计过程中引入正则项。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">贝叶斯回归有如下缺点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">它的推断过程是非常耗时的。</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<ul class="calibre6">
<li class="toctree-l">一个对于贝叶斯方法的很好的介绍 C. Bishop: Pattern
Recognition and Machine learning</li>
</ul>
<dl class="calibre10">
<dt class="calibre18"><a href="#calibre_link-106" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-294">*</span></a>详细介绍原创算法的一本书 <a href="#calibre_link-107" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-295">`</span></a>Bayesian learning for neural</dt>
<dd class="calibre19">networks` by Radford M. Neal</dd>
</dl>
</div>
<div class="toctree-wrapper" id="calibre_link-296">
<span id="calibre_link-297" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.1.10.1. 贝叶斯岭回归</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><code class="docutils"><span class="calibre4">贝叶斯岭回归</span></code> 利用概率模型估算了上述的回归问题，其先验参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 是由以下球面高斯公式得出的：</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000534.jpg" alt="p(w|\lambda) = \mathcal{N}(w|0,\lambda^{-1}\bold{I_{p}})" class="math" /></p>
</div>
<p class="calibre10">先验参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000719.jpg" alt="\lambda" /> 一般是服从 <cite class="calibre13">gamma 分布 &lt;https://en.wikipedia.org/wiki/Gamma_distribution&gt;</cite> , 这个分布与高斯成共轭先验关系。</p>
<p class="calibre10">得到的模型一般称为 <em class="calibre13">贝叶斯岭回归</em>, 并且这个与传统的 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 非常相似。参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000719.jpg" alt="\lambda" /> 是在模型拟合的时候一起被估算出来的。 剩下的超参数就是 gamma 分布的先验了。
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000719.jpg" alt="\lambda" /> 。  它们通常被选择为 <em class="calibre13">没有信息量</em> 。模型参数的估计一般利用 <em class="calibre13">最大似然对数估计法</em> 。</p>
<p class="calibre10">默认 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000519.jpg" alt="\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}" />.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_bayesian_ridge.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_bayesian_ridge_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000304.jpg" class="calibre20" /></a>
</div>
<p class="calibre10">贝叶斯岭回归用来解决回归问题:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3.</span><span class="calibre4">,</span> <span class="calibre4">3.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">3.</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">BayesianRidge</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span>
<span class="calibre4">BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,</span>
<span class="calibre4">       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,</span>
<span class="calibre4">       normalize=False, tol=0.001, verbose=False)</span>
</pre>
</div>
</div>
<p class="calibre10">在模型训练完成后，可以用来预测新值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">predict</span> <span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">]])</span>
<span class="calibre4">array([ 0.50000013])</span>
</pre>
</div>
</div>
<p class="calibre10">权值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 可以被这样访问:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reg</span><span class="calibre4">.</span><span class="calibre4">coef_</span>
<span class="calibre4">array([ 0.49999993,  0.49999993])</span>
</pre>
</div>
</div>
<p class="calibre10">由于贝叶斯框架的缘故，权值与 <a class="calibre3 pcalibre" href="#calibre_link-100"><span class="calibre4">普通最小二乘法</span></a> 产生的不太一样。
但是，贝叶斯岭回归对病态问题（ill-posed）的鲁棒性要更好。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子s:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py"><span class="calibre4">Bayesian Ridge Regression</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<ul class="calibre6">
<li class="toctree-l">更多细节可以参考 <a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;rep=rep1&amp;type=pdf">Bayesian Interpolation</a>
by MacKay, David J. C.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-298">
<h3 class="sigil_not_in_toc1">1.1.10.2. 主动相关决策理论 - ARD</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><code class="docutils"><span class="calibre4">主动相关决策理论</span></code> 和 <cite class="calibre13">贝叶斯岭回归</cite> 非常相似，</div>
</blockquote>
<p class="calibre10">但是会导致一个更加稀疏的权重 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> <a class="calibre3 pcalibre" href="#calibre_link-108" id="calibre_link-112">[1]</a> <a class="calibre3 pcalibre" href="#calibre_link-109" id="calibre_link-113">[2]</a>。
<code class="docutils"><span class="calibre4">主动相关决策理论</span></code> 提出了一个不同于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 的先验假设。具体来说，就是弱化了高斯分布为球形的假设。
它采用的是关于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 轴平行的椭圆高斯分布。</p>
<p class="calibre10">也就是说，每个权值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000309.jpg" alt="w_{i}" /> 精确度来自于一个中心在0点，精度为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000745.jpg" alt="\lambda_{i}" /> 的分布中采样得到的。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000604.jpg" alt="p(w|\lambda) = \mathcal{N}(w|0,A^{-1})" class="math" /></p>
</div>
<p class="calibre10">并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000484.jpg" alt="diag \; (A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}" />.</p>
<p class="calibre10">与 <cite class="calibre13">贝叶斯岭回归</cite> 不同， 每个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000309.jpg" alt="w_{i}" /> 都有一个标准差 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000644.jpg" alt="\lambda_i" /> 。所有的关于方差的系数
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000644.jpg" alt="\lambda_i" />  和由给定的超参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000160.jpg" alt="\lambda_1" /> 、 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000888.jpg" alt="\lambda_2" />
由相同的gamma分布。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ard.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ard_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000788.jpg" class="calibre20" /></a>
</div>
<p class="calibre10">ARD 也被称为 <em class="calibre13">稀疏贝叶斯学习</em> 或
<em class="calibre13">相关向量机</em> <a class="calibre3 pcalibre" href="#calibre_link-110" id="calibre_link-114">[3]</a> <a class="calibre3 pcalibre" href="#calibre_link-111" id="calibre_link-115">[4]</a>.</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py"><span class="calibre4">Automatic Relevance Determination Regression (ARD)</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-108" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-112">[1]</a></td>
<td class="label1">Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-109" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-113">[2]</a></td>
<td class="label1">David Wipf and Srikantan Nagarajan: <a class="calibre3 pcalibre" href="http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf">A new view of automatic relevance determination</a></td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-110" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-114">[3]</a></td>
<td class="label1">Michael E. Tipping: <a class="calibre3 pcalibre" href="http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf">Sparse Bayesian Learning and the Relevance Vector Machine</a></td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-111" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-115">[4]</a></td>
<td class="label1">Tristan Fletcher: <a class="calibre3 pcalibre" href="http://www.tristanfletcher.co.uk/RVM%20Explained.pdf">Relevance Vector Machines explained</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-299">
<span id="calibre_link-98" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.11. logistic 回归</h2>
<p class="calibre2">logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。在某些文献中，logistic 回归又被称作 logit regression（logit 回归），maximum-entropy classification(MaxEnt，最大熵分类)，或 log-linear classifier（线性对数分类器）。该模型利用函数 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> 将单次试验（single trial）的输出转化并描述为概率。</p>
<p class="calibre10">scikit-learn 中 logistic 回归在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 类中实现了二元（binary）、一对余（one-vs-rest）及多元 logistic 回归，并带有可选的 L1 和 L2 正则化。</p>
<p class="calibre10">若视为一优化问题，带 L2 罚项的二分类 logistic 回归要最小化以下代价函数（cost function）：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000210.jpg" alt="\underset{w, c}{min\,} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) ." class="math" /></p>
</div>
<p class="calibre10">类似地，带 L1 正则的 logistic 回归需要求解下式：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000531.jpg" alt="\underset{w, c}{min\,} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) ." class="math" /></p>
</div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 类中实现了这些求解器: “liblinear”, “newton-cg”, “lbfgs”, “sag” 和 “saga”。</p>
<p class="calibre10">“liblinear” 应用了坐标下降算法（Coordinate Descent, CD），并基于 scikit-learn 内附的高性能C++库 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR library</a> 实现。不过CD算法训练的模型不是真正意义上的多分类模型，而是基于 “one-vs-rest” 思想分解了这个优化问题，为每个类别都训练了一个二元分类器。因为实现在底层使用该求解器的 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 实例对象表面上看是一个多元分类器。 <a class="calibre3 pcalibre" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code class="docutils"><span class="calibre4">sklearn.svm.l1_min_c</span></code></a> 可以计算使用 L1 罚项时 C 的下界，以避免模型为空（即全部特征分量的权重为零）。</p>
<p class="calibre10">“lbfgs”, “sag” 和 “newton-cg” solvers （求解器）只支持 L2 罚项，对某些高维数据收敛更快。这些求解器的参数 <a href="#calibre_link-116" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-300">`</span></a>multi_class`设为 “multinomial” 即可训练一个真正的多元 logistic 回归 <a class="calibre3 pcalibre" href="#calibre_link-117" id="calibre_link-120">[5]</a>，其预测的概率比默认的 “one-vs-rest” 设定更为准确。</p>
<p class="calibre10">“sag” 求解器基于平均随机梯度下降算法（Stochastic Average Gradient descent） <a class="calibre3 pcalibre" href="#calibre_link-118" id="calibre_link-121">[6]</a>。在大数据集上的表现更快，大数据集指样本量大且特征数多。</p>
<p class="calibre10">“saga” solver <a class="calibre3 pcalibre" href="#calibre_link-119" id="calibre_link-122">[7]</a> 是 “sag” 的一类变体，它支持非平滑（non-smooth）的 L1 正则选项 <code class="docutils"><span class="calibre4">penalty="l1"</span></code> 。因此对于稀疏多元 logistic 回归 ，往往选用该求解器。</p>
<p class="calibre10">一言以蔽之，选用求解器可遵循如下规则:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="45%" class="label"></col>
<col width="55%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">Case</th>
<th class="head">Solver</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">L1正则</td>
<td class="label1">“liblinear” or “saga”</td>
</tr>
<tr class="row-odd"><td class="label1">多元损失（multinomial loss）</td>
<td class="label1">“lbfgs”, “sag”, “saga” or “newton-cg”</td>
</tr>
<tr class="calibre23"><td class="label1">大数据集（<cite class="calibre13">n_samples</cite>）</td>
<td class="label1">“sag” or “saga”</td>
</tr>
</tbody>
</table>
<p class="calibre10">“saga” 一般都是最佳的选择，但出于一些历史遗留原因默认的是 “liblinear”。</p>
<p class="calibre10">对于大数据集，还可以用 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> ，并使用对数损失（’log’ loss）</p>
<div class="toctree-wrapper">
<p class="calibre10">示例：</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py"><span class="calibre4">L1 Penalty and Sparsity in Logistic Regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py"><span class="calibre4">Path with L1- Logistic Regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py"><span class="calibre4">Plot multinomial and One-vs-Rest Logistic Regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py"><span class="calibre4">Multiclass sparse logisitic regression on newgroups20</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py"><span class="calibre4">MNIST classfification using multinomial logistic + L1</span></a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-301">
<p class="calibre10">与 liblinear 的区别:</p>
<p class="calibre10">当 <code class="docutils"><span class="calibre4">fit_intercept=False</span></code> 、回归得到的 <code class="docutils"><span class="calibre4">coef_</span></code> 以及待预测的数据为零时， <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 用 <code class="docutils"><span class="calibre4">solver=liblinear</span></code>
及 <code class="docutils"><span class="calibre4">LinearSVC</span></code> 与直接使用外部liblinear库预测得分会有差异。这是因为，
对于 <code class="docutils"><span class="calibre4">decision_function</span></code> 为零的样本， <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 和 <code class="docutils"><span class="calibre4">LinearSVC</span></code>
将预测为负类，而liblinear预测为正类。
注意，设定了 <code class="docutils"><span class="calibre4">fit_intercept=False</span></code> ，又有很多样本使得 <code class="docutils"><span class="calibre4">decision_function</span></code> 为零的模型，很可能会欠拟合，其表现往往比较差。建议您设置 <code class="docutils"><span class="calibre4">fit_intercept=True</span></code> 并增大 <code class="docutils"><span class="calibre4">intercept_scaling</span></code>。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><strong class="calibre14">利用稀疏 logistic 回归（sparse logisitic regression）进行特征选择</strong></p>
<blockquote class="calibre15">
<div class="toctree-wrapper">带 L1 罚项的 logistic 回归 将得到稀疏模型（sparse model），相当于进行了特征选择（feature selection），详情参见 <a class="calibre3 pcalibre" href="feature_selection.html#l1-feature-selection"><span class="calibre4">基于 L1 的特征选取</span></a> 。</div>
</blockquote>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="docutils"><span class="calibre4">LogisticRegressionCV</span></code></a> 对 logistic 回归 的实现内置了交叉验证（cross-validation），可以找出最优的参数 C。”newton-cg”, “sag”, “saga” 和 “lbfgs” 在高维数据上更快，因为采用了热启动（warm-starting）。在多分类设定下，若 <cite class="calibre13">multi_class</cite> 设为”ovr”，会为每类求一个最佳的C值；若 <cite class="calibre13">multi_class</cite> 设为”multinomial”，会通过交叉熵损失（cross-entropy loss）求出一个最佳 C 值。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献：</p>
<table class="docutils1" frame="void" id="calibre_link-117" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-120">[5]</a></td>
<td class="label1">Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-118" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-121">[6]</a></td>
<td class="label1">Mark Schmidt, Nicolas Le Roux, and Francis Bach: <a class="calibre3 pcalibre" href="https://hal.inria.fr/hal-00860051/document">Minimizing Finite Sums with the Stochastic Average Gradient.</a></td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-119" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-122">[7]</a></td>
<td class="label1">Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <a class="calibre3 pcalibre" href="https://arxiv.org/abs/1407.0202">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</a></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-302">
<h2 class="sigil_not_in_toc">1.1.12. 随机梯度下降, SGD</h2>
<p class="calibre2">随机梯度下降是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。
方法 <code class="docutils"><span class="calibre4">partial_fit</span></code> 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。
例如，设定 <code class="docutils"><span class="calibre4">loss="log"</span></code> ，则 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 拟合一个逻辑斯蒂回归模型，而 <code class="docutils"><span class="calibre4">loss="hinge"</span></code> 拟合线性支持向量机(SVM).</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="sgd.html#sgd"><span class="calibre4">随机梯度下降</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-303">
<span id="calibre_link-304" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.13. Perceptron（感知器）</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="docutils"><span class="calibre4">Perceptron</span></code></a> 是适用于 large scale learning（大规模学习）的一种简单算法。默认地，</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">不需要设置学习率（learning rate）。</li>
<li class="toctree-l">不需要正则化处理。</li>
<li class="toctree-l">仅使用错误样本更新模型。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">最后一点表明使用合页损失（hinge loss）的感知机比SGD略快，所得模型更稀疏。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-305">
<span id="calibre_link-306" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.14. Passive Aggressive Algorithms（被动攻击算法）</h2>
<p class="calibre2">被动攻击算法是大规模学习的一类算法。和感知机类似，它也不需要设置学习率，不过比感知机多出一个正则化参数 <code class="docutils"><span class="calibre4">C</span></code> 。</p>
<p class="calibre10">对于分类问题， <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code class="docutils"><span class="calibre4">PassiveAggressiveClassifier</span></code></a> 可设定
<code class="docutils"><span class="calibre4">loss='hinge'</span></code> (PA-I)或 <code class="docutils"><span class="calibre4">loss='squared_hinge'</span></code> (PA-II)。对于回归问题，
<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code class="docutils"><span class="calibre4">PassiveAggressiveRegressor</span></code></a> 可设置
<code class="docutils"><span class="calibre4">loss='epsilon_insensitive'</span></code> (PA-I)或
<code class="docutils"><span class="calibre4">loss='squared_epsilon_insensitive'</span></code> (PA-II).</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献：</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">“Online Passive-Aggressive Algorithms”</a>
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-307">
<h2 class="sigil_not_in_toc">1.1.15. 稳健回归（Robustness regression）: 处理离群点（outliers）和模型错误</h2>
<p class="calibre2">稳健回归（robust regression）特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_theilsen_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000824.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-308">
<h3 class="sigil_not_in_toc1">1.1.15.1. 各种使用场景与相关概念</h3>
<p class="calibre2">处理包含离群点的数据时牢记以下几点:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first"><strong class="calibre14">离群值在X上还是在y方向上</strong>?</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">离群值在y方向上</th>
<th class="head">离群值在X方向上</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000003.jpg" class="calibre25" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="X_outliers" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000796.jpg" class="calibre25" /></a></td>
</tr>
</tbody>
</table>
</li>
<li class="toctree-l"><p class="first"><strong class="calibre14">离群点的比例 vs. 错误的量级（amplitude）</strong></p>
<p class="calibre10">离群点的数量很重要，离群程度也同样重要。</p>
</li>
</ul>
<p class="calibre10">稳健拟合（robust fitting）的一个重要概念是崩溃点（breakdown point），即拟合模型（仍准确预测）所能承受的离群值最大比例。</p>
<p class="calibre10">注意，在高维数据条件下（ <cite class="calibre13">n_features</cite> 大），一般而言很难完成稳健拟合，很可能完全不起作用。</p>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">折中： 预测器的选择</strong></p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">Scikit-learn提供了三种稳健回归的预测器（estimator）:
<a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> ,
<a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a> 和
<a class="calibre3 pcalibre" href="#calibre_link-125"><span class="calibre4">HuberRegressor</span></a></p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="#calibre_link-125"><span class="calibre4">HuberRegressor</span></a> 一般快于
<a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> 和 <a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a> ，
除非样本数很大，即 <code class="docutils"><span class="calibre4">n_samples</span></code> &gt;&gt; <code class="docutils"><span class="calibre4">n_features</span></code> 。
这是因为 <a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> 和 <a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a>
都是基于数据的较小子集进行拟合。但使用默认参数时， <a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a>
和 <a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> 可能不如
<a class="calibre3 pcalibre" href="#calibre_link-125"><span class="calibre4">HuberRegressor</span></a> 鲁棒。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> 比 <a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a> 更快，在样本数量上的伸缩性（适应性）更好。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a> 能更好地处理y方向的大值离群点（通常情况下）。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="#calibre_link-124"><span class="calibre4">Theil Sen</span></a> 能更好地处理x方向中等大小的离群点，但在高维情况下无法保证这一特点。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">实在决定不了的话，请使用 <a class="calibre3 pcalibre" href="#calibre_link-123"><span class="calibre4">RANSAC</span></a></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-309">
<span id="calibre_link-123" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.1.15.2. RANSAC： 随机抽样一致性算法（RANdom SAmple Consensus）</h3>
<p class="calibre2">随机抽样一致性算法（RANdom SAmple Consensus, RANSAC）利用全体数据中局内点（inliers）的一个随机子集拟合模型。</p>
<p class="calibre10">RANSAC是一种非确定性算法，以一定概率输出一个可能的合理结果，依赖于迭代次数（参数 <cite class="calibre13">max_trials</cite> ）。这种算法主要解决线性或非线性回归问题，在计算机视觉摄影测量领域尤为流行。</p>
<p class="calibre10">算法从全体样本输入中分出一个局内点集合，全体样本可能由于测量错误或对数据的假设错误而含有噪点、离群点。最终的模型仅从这个局内点集合中得出。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ransac.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ransac_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000300.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-310">
<h4 class="sigil_not_in_toc1">1.1.15.2.1. 算法细节</h4>
<p class="calibre2">每轮迭代执行以下步骤:</p>
<ol class="arabic">
<li class="toctree-l">从原始数据中抽样 <code class="docutils"><span class="calibre4">min_samples</span></code> 数量的随机样本，检查数据是否合法（见 <code class="docutils"><span class="calibre4">is_data_valid</span></code> ）.</li>
<li class="toctree-l">用一个随机子集拟合模型（ <code class="docutils"><span class="calibre4">base_estimator.fit</span></code> ）。检查模型是否合法（见 <code class="docutils"><span class="calibre4">is_model_valid</span></code> ）。</li>
<li class="toctree-l">计算预测模型的残差（residual），将全体数据分成局内点和离群点（ <code class="docutils"><span class="calibre4">base_estimator.predict(X)</span> <span class="calibre4">-</span> <span class="calibre4">y</span></code> ）</li>
</ol>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">绝对残差小于 <code class="docutils"><span class="calibre4">residual_threshold</span></code> 的全体数据认为是局内点。</li>
</ul>
</div>
</blockquote>
<ol class="arabic" start="4">
<li class="toctree-l">若局内点样本数最大，保存当前模型为最佳模型。以免当前模型离群点数量恰好相等（而出现未定义情况），规定仅当数值大于当前最值时认为是最佳模型。</li>
</ol>
<p class="calibre10">上述步骤或者迭代到最大次数（ <code class="docutils"><span class="calibre4">max_trials</span></code> ），或者某些终止条件满足时停下（见 <code class="docutils"><span class="calibre4">stop_n_inliers</span></code> 和 <code class="docutils"><span class="calibre4">stop_score</span></code> )。最终模型由之前确定的最佳模型的局内点样本（一致性集合，consensus
set）预测。</p>
<p class="calibre10">函数 <code class="docutils"><span class="calibre4">is_data_valid</span></code> 和 <code class="docutils"><span class="calibre4">is_model_valid</span></code> 可以识别出随机样本子集中的退化组合（degenerate combinations）并予以丢弃（reject）。即便不需要考虑退化情况，也会使用 <code class="docutils"><span class="calibre4">is_data_valid</span></code> ，因为在拟合模型之前调用它能得到更高的计算性能。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例：</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py"><span class="calibre4">Robust linear model estimation using RANSAC</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="calibre4">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献：</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf">“Random Sample Consensus: A Paradigm for Model Fitting with Applications to
Image Analysis and Automated Cartography”</a>
Martin A. Fischler and Robert C. Bolles - SRI International (1981)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">“Performance Evaluation of RANSAC Family”</a>
Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-311">
<span id="calibre_link-124" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.1.15.3. Theil-Sen 预估器: 广义中值估计</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="docutils"><span class="calibre4">TheilSenRegressor</span></code></a> 估计器：使用中位数在多个维度推广，因此对多维离散值是有帮助，但问题是，随着维数的增加，估计器的准确性在迅速下降。准确性的丢失，导致在高维上的估计值比不上普通的最小二乘法。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="calibre4">Theil-Sen Regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="calibre4">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-312">
<h4 class="sigil_not_in_toc1">1.1.15.3.1. 算法理论细节</h4>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="docutils"><span class="calibre4">TheilSenRegressor</span></code></a> 媲美 <a class="calibre3 pcalibre" href="#calibre_link-100"><span class="calibre4">Ordinary Least Squares (OLS)</span></a> （普通最小二乘法（OLS））渐近效率和无偏估计。在对比 OLS, Theil-Sen 是一种非参数方法，这意味着它没有对底层数据的分布假设。由于 Theil-Sen 是基于中位数的估计，它是更适合的对损坏的数据。在单变量的设置，Theil-Sen 在一个简单的线性回归，这意味着它可以容忍任意损坏的数据高达 29.3% 的情况下，约 29.3% 的一个崩溃点。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_theilsen_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000824.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">在 scikit-learn 中  <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="docutils"><span class="calibre4">TheilSenRegressor</span></code></a> 实施如下的学习推广到多元线性回归模型 <a class="calibre3 pcalibre" href="#calibre_link-126" id="calibre_link-128">[8]</a> 利用空间中这是一个概括的中位数多维度 <a class="calibre3 pcalibre" href="#calibre_link-127" id="calibre_link-129">[9]</a> 。</p>
<p class="calibre10">在时间复杂度和空间复杂度，根据 Theil-Sen 量表</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000608.jpg" alt="\binom{n_{samples}}{n_{subsamples}}" class="math" /></p>
</div>
<p class="calibre10">这使得它不适用于大量样本和特征的问题。因此，可以选择一个亚群的大小来限制时间和空间复杂度，只考虑所有可能组合的随机子集。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="calibre4">Theil-Sen Regression</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-126" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-128">[8]</a></td>
<td class="label1">Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <a class="calibre3 pcalibre" href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">Theil-Sen Estimators in a Multiple Linear Regression Model.</a></td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-127" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-129">[9]</a></td>
<td class="label1"><ol class="arabic" start="20">
<li class="toctree-l">Kärkkäinen and S. Äyrämö: <a class="calibre3 pcalibre" href="http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf">On Computation of Spatial Median for Robust Data Mining.</a></li>
</ol>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-313">
<span id="calibre_link-125" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.1.15.4. Huber 回归</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="docutils"><span class="calibre4">HuberRegressor</span></code></a> 不同，因为它适用于 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 损耗的样品被分类为离群值。如果这个样品的绝对误差小于某一阈值，样品就分为一层。
它不同于 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="docutils"><span class="calibre4">TheilSenRegressor</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor" title="sklearn.linear_model.RANSACRegressor"><code class="docutils"><span class="calibre4">RANSACRegressor</span></code></a> 因为它无法忽略对离群值的影响，但对它们的权重较小。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_huber_vs_ridge.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_huber_vs_ridge_001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000687.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">这个 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="docutils"><span class="calibre4">HuberRegressor</span></code></a> 最小化损失函数是由</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000571.jpg" alt="\underset{w, \sigma}{min\,} {\sum_{i=1}^n\left(\sigma + H_m\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}" class="math" /></p>
</div>
<p class="calibre10">其中</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000638.jpg" alt="H_m(z) = \begin{cases}        z^2, &amp; \text {if } |z| &lt; \epsilon, \\        2\epsilon|z| - \epsilon^2, &amp; \text{otherwise} \end{cases}" class="math" /></p>
</div>
<p class="calibre10">建议设置参数 <code class="docutils"><span class="calibre4">epsilon</span></code> 为 1.35 以实现 95% 统计效率。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-314">
<h3 class="sigil_not_in_toc1">1.1.15.5. 注意</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="docutils"><span class="calibre4">HuberRegressor</span></code></a> 与将损失设置为 <cite class="calibre13">huber</cite> 的 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 在以下方面的使用方式上是不同的。</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="docutils"><span class="calibre4">HuberRegressor</span></code></a> 是标度不变性的. 一旦设置了 <code class="docutils"><span class="calibre4">epsilon</span></code> , 通过不同的值向上或向下缩放 <code class="docutils"><span class="calibre4">X</span></code> 和 <code class="docutils"><span class="calibre4">y</span></code> ，就会跟以前一样对异常值产生同样的键壮性。相比 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 其中 <code class="docutils"><span class="calibre4">epsilon</span></code> 在 <code class="docutils"><span class="calibre4">X</span></code> 和 <code class="docutils"><span class="calibre4">y</span></code> 是缩放的时候必须再次设置。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="docutils"><span class="calibre4">HuberRegressor</span></code></a> 应该更有效地使用在小样本数据，同时 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 需要在训练数据的次数来产生相同的键壮性。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="calibre4">HuberRegressor vs Ridge on dataset with strong outliers</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</li>
</ul>
</div>
<p class="calibre10">另外，这个估计是不同于 R 实现的 Robust Regression (<a class="calibre3 pcalibre" href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>) 因为 R 不实现加权最小二乘实现每个样本上给出多少剩余的基础重量大于某一阈值。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-315">
<span id="calibre_link-316" class="calibre4"></span><h2 class="sigil_not_in_toc">1.1.16. 多项式回归：用基函数展开线性模型</h2>
<p class="calibre2">机器学习中一种常见的模式，是使用线性模型训练数据的非线性函数。这种方法保持了一般快速的线性方法的性能，同时允许它们适应更广泛的数据范围。</p>
<p class="calibre10">例如，可以通过构造系数的 <strong class="calibre14">polynomial features</strong> 来扩展一个简单的线性回归。在标准线性回归的情况下，你可能有一个类似于二维数据的模型:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000237.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2" class="math" /></p>
</div>
<p class="calibre10">如果我们想把抛物面拟合成数据而不是平面，我们可以结合二阶多项式的特征，使模型看起来像这样:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000725.jpg" alt="\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2" class="math" /></p>
</div>
<p class="calibre10">（这有时候是令人惊讶的）观察，这还是 <em class="calibre13">still a linear model</em> : 看到这个，想象创造一个新的变量</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000295.jpg" alt="z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]" class="math" /></p>
</div>
<p class="calibre10">有了这些数据的重新标记的数据，我们的问题就可以写了。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000718.jpg" alt="\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5" class="math" /></p>
</div>
<p class="calibre10">我们看到，导致 <em class="calibre13">polynomial regression</em> 是线性模型中的同一类，我们认为以上（即模型是线性&nbsp;），可以用同样的方法解决。通过考虑在用这些基函数建立的高维空间中的线性拟合，该模型具有灵活性，可以适应更广泛的数据范围。</p>
<p class="calibre10">这里是一个例子，应用这个想法，一维数据，使用不同程度的多项式特征:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_polynomial_interpolation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_polynomial_interpolation_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000667.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">这个图是使用&nbsp;<a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="docutils"><span class="calibre4">PolynomialFeatures</span></code></a>&nbsp;预创建。该预处理器将输入数据矩阵转换为给定度的新数据矩阵。它可以使用如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">PolynomialFeatures</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">6</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span>
<span class="calibre4">array([[0, 1],</span>
<span class="calibre4">       [2, 3],</span>
<span class="calibre4">       [4, 5]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span> <span class="calibre4">=</span> <span class="calibre4">PolynomialFeatures</span><span class="calibre4">(</span><span class="calibre4">degree</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span>
<span class="calibre4">       [  1.,   2.,   3.,   4.,   6.,   9.],</span>
<span class="calibre4">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">X</span></code> 的特征已经从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000026.jpg" alt="[x_1, x_2]" /> 转换到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000533.jpg" alt="[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]" />, 并且现在可以用在任何线性模型。</p>
<p class="calibre10">这种预处理可以通过 <a class="calibre3 pcalibre" href="pipeline.html#pipeline"><span class="calibre4">Pipeline</span></a> 工具进行简化。可以创建一个表示简单多项式回归的单个对象，并使用如下所示:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">PolynomialFeatures</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">LinearRegression</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">Pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([(</span><span class="calibre4">'poly'</span><span class="calibre4">,</span> <span class="calibre4">PolynomialFeatures</span><span class="calibre4">(</span><span class="calibre4">degree</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)),</span>
<span class="calibre4">... </span>                  <span class="calibre4">(</span><span class="calibre4">'linear'</span><span class="calibre4">,</span> <span class="calibre4">LinearRegression</span><span class="calibre4">(</span><span class="calibre4">fit_intercept</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">))])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># fit to an order-3 polynomial data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">x</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">3</span> <span class="calibre4">-</span> <span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">x</span> <span class="calibre4">+</span> <span class="calibre4">x</span> <span class="calibre4">**</span> <span class="calibre4">2</span> <span class="calibre4">-</span> <span class="calibre4">x</span> <span class="calibre4">**</span> <span class="calibre4">3</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">x</span><span class="calibre4">[:,</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">newaxis</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">[</span><span class="calibre4">'linear'</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">coef_</span>
<span class="calibre4">array([ 3., -2.,  1., -1.])</span>
</pre>
</div>
</div>
<p class="calibre10">利用多项式特征训练的线性模型能够准确地恢复输入多项式系数。</p>
<p class="calibre10">在某些情况下，没有必要包含任何单个特征的更高的幂，但只需要在大多数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 不同的特征上相乘的所谓 <em class="calibre13">interaction features（交互特征）</em> 。这些可以与设定的 <code class="docutils"><span class="calibre4">interaction_only=True</span></code> 的 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="docutils"><span class="calibre4">PolynomialFeatures</span></code></a> 得到。</p>
<p class="calibre10">例如，当处理布尔属性， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000145.jpg" alt="x_i^n = x_i" /> 所有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> ，因此是无用的；但 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000011.jpg" alt="x_i x_j" /> 代表两布尔合取。这样我们就可以用线性分类器解决异或问题:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">Perceptron</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">PolynomialFeatures</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[:,</span> <span class="calibre4">0</span><span class="calibre4">]</span> <span class="calibre4">^</span> <span class="calibre4">X</span><span class="calibre4">[:,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span>
<span class="calibre4">array([0, 1, 1, 0])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">PolynomialFeatures</span><span class="calibre4">(</span><span class="calibre4">interaction_only</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">astype</span><span class="calibre4">(</span><span class="calibre4">int</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span>
<span class="calibre4">array([[1, 0, 0, 0],</span>
<span class="calibre4">       [1, 0, 1, 0],</span>
<span class="calibre4">       [1, 1, 0, 0],</span>
<span class="calibre4">       [1, 1, 1, 1]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">Perceptron</span><span class="calibre4">(</span><span class="calibre4">fit_intercept</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">,</span> <span class="calibre4">max_iter</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">tol</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                 <span class="calibre4">shuffle</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">分类器的 “predictions” 是完美的:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 1, 1, 0])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>
</pre>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-130">
<span id="calibre_link-317" class="calibre4"></span><h1 class="calibre5">1.2. 线性和二次判别分析</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/AnybodyHome" class="calibre3 pcalibre">@AnybodyHome</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@numpy</a><br class="calibre9" />      
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@FAME</a><br class="calibre9" />
    </div>
<p class="calibre10">Linear Discriminant Analysis（线性判别分析）(<a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis</span></code></a>) 和 Quadratic Discriminant Analysis （二次判别分析）(<a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="docutils"><span class="calibre4">discriminant_analysis.QuadraticDiscriminantAnalysis</span></code></a>) 是两个经典的分类器。
正如他们名字所描述的那样，他们分别代表了线性决策平面和二次决策平面。</p>
<p class="calibre10">这些分类器十分具有吸引力，因为他们可以很容易计算得到闭式解(即解析解)，其天生具有多分类的特性，在实践中已经被证明很有效，并且无需调参。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/classification/plot_lda_qda.html"><img alt="ldaqda" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000354.jpg" class="calibre26" /></a></strong></p>
<p class="calibre10">以上这些图像展示了 Linear Discriminant Analysis （线性判别分析）以及 Quadratic Discriminant Analysis （二次判别分析）的决策边界。其中，最后一行表明了线性判别分析只能学习线性边界，
而二次判别分析则可以学习二次边界，因此它相对而言更加灵活。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py"><span class="calibre4">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</span></a>: LDA和QDA在特定数据上的对比</p>
</div>
<div class="toctree-wrapper" id="calibre_link-318">
<h2 class="sigil_not_in_toc">1.2.1. 使用线性判别分析来降维</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis</span></code></a> 通过把输入的数据投影到由最大化类之间分离的方向所组成的线性子空间，可以执行有监督降维（详细的内容见下面的数学推导）。输出的维度必然会比原来的类别数量更少的。因此它总体而言是十分强大的降维方式，同样也仅仅在多分类环境下才能感觉到。</p>
<p class="calibre10">实现方式在 <a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis.transform</span></code></a> 中。关于维度的数量可以通过 <code class="docutils"><span class="calibre4">n_components</span></code> 参数来调节。
值得注意的是，这个参数不会对 <a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis.fit</span></code></a> 或者 <a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis.predict</span></code></a> 产生影响。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="calibre4">Comparison of LDA and PCA 2D projection of Iris dataset</span></a>: 在 Iris 数据集对比 LDA 和 PCA 之间的降维差异</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-319">
<h2 class="sigil_not_in_toc">1.2.2. LDA 和 QDA 分类器的数学公式</h2>
<p class="calibre2">LDA 和 QDA 都是源于简单的概率模型，这些模型对于每一个类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的相关分布 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000603.jpg" alt="P(X|y=k)" /> 都可以通过贝叶斯定理所获得。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000704.jpg" alt="P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} = \frac{P(X | y=k) P(y = k)}{ \sum_{l} P(X | y=l) \cdot P(y=l)}" class="math" /></p>
</div>
<p class="calibre10">我们选择最大化条件概率的类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" />.</p>
<p class="calibre10">更具体地说，对于线性以及二次判别分析， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000486.jpg" alt="P(X|y)" /> 被建模成密度多变量高斯分布:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000569.jpg" alt="p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)" class="math" /></p>
</div>
<p class="calibre10">为了把该模型作为分类器使用，我们只需要从训练数据中估计出类的先验概率 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000513.jpg" alt="P(y=k)" /> （通过每个类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的实例的比例得到）
类别均值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000260.jpg" alt="\mu_k" /> （通过经验样本的类别均值得到）以及协方差矩阵（通过经验样本的类别协方差或者正则化的估计器 estimator 得到: 见下面的 shrinkage 章节）。</p>
<dl class="calibre10">
<dt class="calibre18">在 LDA 中，每个类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的高斯分布共享相同的协方差矩阵：<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000846.jpg" alt="\Sigma_k = \Sigma" /> for all <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" />。这导致了两者之间的线性决策表面，这可以通过比较对数概率比看出来</dt>
<dd class="calibre19"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000758.jpg" alt="\log[P(y=k | X) / P(y=l | X)]" /> 。</dd>
</dl>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000425.jpg" alt="\log\left(\frac{P(y=k|X)}{P(y=l | X)}\right) = 0 \Leftrightarrow (\mu_k-\mu_l)\Sigma^{-1} X = \frac{1}{2} (\mu_k^t \Sigma^{-1} \mu_k - \mu_l^t \Sigma^{-1} \mu_l)" class="math" /></p>
</div>
<p class="calibre10">在 QDA 中，没有关于高斯协方差矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000096.jpg" alt="\Sigma_k" /> 的假设，因此有了二次决策平面. 更多细节见 <a class="calibre3 pcalibre" href="#calibre_link-131" id="calibre_link-132">[3]</a> .</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><strong class="calibre14">与高斯朴素贝叶斯的关系</strong></p>
<p class="calibre10">如果在QDA模型中假设协方差矩阵是对角的，则输入被假设为在每个类中是条件独立的，所得的分类器等同于高斯朴素贝叶斯分类器 <a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="docutils"><span class="calibre4">naive_bayes.GaussianNB</span></code></a> 相同。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-320">
<h2 class="sigil_not_in_toc">1.2.3. LDA 的降维数学公式</h2>
<p class="calibre2">为了理解 LDA 在降维上的应用，从上面解释的 LDA 分类规则的几何重构开始是十分有用的。我们用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 表示目标类别的总数。
由于在 LDA 中我们假设所有类别都有相同估计的协方差 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000351.jpg" alt="\Sigma" /> ,所以我们可重新调节数据从而让协方差相同。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000202.jpg" alt="X^* = D^{-1/2}U^t X\text{ with }\Sigma = UDU^t" class="math" /></p>
</div>
<dl class="calibre10">
<dt class="calibre18">在缩放之后对数据点进行分类相当于找到与欧几里得距离中的数据点最接近的估计类别均值。但是它也可以在投影到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000633.jpg" alt="K-1" /> 个由所有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000817.jpg" alt="\mu^*_k" /> 个类生成的仿射子空间</dt>
<dd class="calibre19"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000568.jpg" alt="H_K" /> 之后完成。这也表明，LDA 分类器中存在一个利用线性投影到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000633.jpg" alt="K-1" /> 个维度空间的降维工具。</dd>
</dl>
<p class="calibre10">通过投影到线性子空间 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000656.jpg" alt="H_L" /> 上，我们可以进一步将维数减少到一个选定的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> ，从而使投影后的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000817.jpg" alt="\mu^*_k" /> 的方差最大化（实际上，为了实现转换类均值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000817.jpg" alt="\mu^*_k" />，我们正在做一种形式的 PCA）。
这里的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> 对应于 <a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis.transform</span></code></a> 方法中使用的 <code class="docutils"><span class="calibre4">n_components</span></code> 参数。 详情参考
<a class="calibre3 pcalibre" href="#calibre_link-131" id="calibre_link-133">[3]</a> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-321">
<h2 class="sigil_not_in_toc">1.2.4. Shrinkage（收缩）</h2>
<p class="calibre2">收缩是一种在训练样本数量相比特征而言很小的情况下可以提升的协方差矩阵预测（准确性）的工具。
在这个情况下，经验样本协方差是一个很差的预测器。收缩 LDA 可以通过设置 <a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="docutils"><span class="calibre4">discriminant_analysis.LinearDiscriminantAnalysis</span></code></a> 类的 <code class="docutils"><span class="calibre4">shrinkage</span></code> 参数为 ‘auto’ 来实现。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">shrinkage</span></code> parameter （收缩参数）的值同样也可以手动被设置为 0-1 之间。特别地，0 值对应着没有收缩（这意味着经验协方差矩阵将会被使用），
而 1 值则对应着完全使用收缩（意味着方差的对角矩阵将被当作协方差矩阵的估计）。设置该参数在两个极端值之间会估计一个（特定的）协方差矩阵的收缩形式</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/classification/plot_lda.html"><img alt="shrinkage" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000189.jpg" class="calibre27" /></a></strong></p>
</div>
<div class="toctree-wrapper" id="calibre_link-322">
<h2 class="sigil_not_in_toc">1.2.5. 预估算法</h2>
<p class="calibre2">默认的 solver 是 ‘svd’。它可以进行classification (分类) 以及 transform (转换),而且它不会依赖于协方差矩阵的计算（结果）。这在特征数量特别大的时候十分具有优势。然而，’svd’ solver 无法与 shrinkage （收缩）同时使用。</p>
<p class="calibre10">‘lsqr’ solver 则是一个高效的算法，它仅用于分类使用。它支持 shrinkage （收缩）。</p>
<p class="calibre10">‘eigen’（特征） solver 是基于 class scatter （类散度）与 class scatter ratio （类内离散率）之间的优化。
它可以被用于 classification （分类）以及 transform （转换），此外它还同时支持收缩。然而，该解决方案需要计算协方差矩阵，因此它可能不适用于具有大量特征的情况。</p>
<div class="toctree-wrapper">
<p class="calibre10">Examples:</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../auto_examples/classification/plot_lda.html#sphx-glr-auto-examples-classification-plot-lda-py"><span class="calibre4">Normal and Shrinkage Linear Discriminant Analysis for classification</span></a>: Comparison of LDA classifiers
with and without shrinkage.</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">References:</p>
<table class="docutils1" frame="void" id="calibre_link-131" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[3]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-132">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-133">2</a>)</em> “The Elements of Statistical Learning”, Hastie T., Tibshirani R.,
Friedman J., Section 4.3, p.106-119, 2008.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-323" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[4]</td>
<td class="label1">Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
The Journal of Portfolio Management 30(4), 110-119, 2004.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-134">
<span id="calibre_link-324" class="calibre4"></span><h1 class="calibre5">1.3. 内核岭回归</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@不吃曲奇的趣多多</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Counting stars</a><br class="calibre9" />     
    </div>
<p class="calibre10">Kernel ridge regression (KRR) （内核岭回归）[M2012]_ 由 使用内核方法的 :ref:<a href="#calibre_link-135" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-325">`</span></a>ridge_regression`（岭回归）（使用 l2 正则化的最小二乘法）所组成。因此，它所学习到的在空间中不同的线性函数是由不同的内核和数据所导致的。对于非线性的内核，它与原始空间中的非线性函数相对应。</p>
<p class="calibre10">由 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 学习的模型的形式与支持向量回归( <code class="docutils"><span class="calibre4">SVR</span></code> ) 是一样的。但是他们使用不同的损失函数：内核岭回归（KRR）使用 squared error loss （平方误差损失函数）而 support vector regression （支持向量回归）（SVR）使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000516.jpg" alt="\epsilon" />-insensitive loss ( ε-不敏感损失 )，两者都使用 l2 regularization （l2 正则化）。与 <code class="docutils"><span class="calibre4">SVR</span></code> 相反，拟合 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 可以以 closed-form （封闭形式）完成，对于中型数据集通常更快。另一方面，学习的模型是非稀疏的，因此比 SVR 慢， 在预测时间，SVR 学习了:math:<cite class="calibre13">epsilon &gt; 0</cite> 的稀疏模型。</p>
<p class="calibre10">下图比较了人造数据集上的 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 和 <code class="docutils"><span class="calibre4">SVR</span></code> 的区别，它由一个正弦目标函数和每五个数据点产生一个强噪声组成。图中分别绘制了由 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 和 <code class="docutils"><span class="calibre4">SVR</span></code> 学习到的回归曲线。两者都使用网格搜索优化了 RBF 内核的 complexity/regularization （复杂性/正则化）和 bandwidth （带宽）。它们的 learned functions （学习函数）非常相似;但是，拟合 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 大约比拟合 <code class="docutils"><span class="calibre4">SVR</span></code> 快七倍（都使用 grid-search  ( 网格搜索 ) ）。然而，由于 SVR 只学习了一个稀疏模型，所以 SVR 预测 10 万个目标值比使用 KernelRidge 快三倍以上。SVR 只使用了百分之三十的数据点做为支撑向量。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_kernel_ridge_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_ridge_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000223.jpg" class="math" /></a>
</div>
<p class="calibre10">下图显示不同大小训练集的 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 和 <code class="docutils"><span class="calibre4">SVR</span></code> 的 fitting （拟合）和 prediction （预测）时间。
对于中型训练集（小于 1000 个样本），拟合 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 比 <code class="docutils"><span class="calibre4">SVR</span></code> 快; 然而，对于更大的训练集 <code class="docutils"><span class="calibre4">SVR</span></code> 通常更好。 关于预测时间，由于学习的稀疏解，<code class="docutils"><span class="calibre4">SVR</span></code> 对于所有不同大小的训练集都比 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code class="docutils"><span class="calibre4">KernelRidge</span></code></a> 快。 注意，稀疏度和预测时间取决于 <code class="docutils"><span class="calibre4">SVR</span></code> 的参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000516.jpg" alt="\epsilon" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> ; <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000372.jpg" alt="\epsilon = 0" /> 将对应于密集模型。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_kernel_ridge_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_ridge_regression_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000112.jpg" class="math" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<table class="docutils1" frame="void" id="calibre_link-326" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[M2012]</td>
<td class="label1">“Machine Learning: A Probabilistic Perspective”
Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012</td>
</tr>
</tbody>
</table>
</div>
</div>


<div class="calibre" id="calibre_link-137">
<span id="calibre_link-327" class="calibre4"></span><h1 class="calibre5">1.4. 支持向量机</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@维</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@子浪</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Damon</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Leon晋</a><br class="calibre9" />
    </div>
<p class="calibre10"><strong class="calibre14">支持向量机 (SVMs)</strong> 可用于以下监督学习算法 <a class="calibre3 pcalibre" href="#calibre_link-138"><span class="calibre4">分类</span></a>, <a class="calibre3 pcalibre" href="#calibre_link-139"><span class="calibre4">回归</span></a> and  <a class="calibre3 pcalibre" href="#calibre_link-140"><span class="calibre4">异常检测</span></a>.</p>
<p class="calibre10">支持向量机的优势在于:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">在高维空间中非常高效.</li>
<li class="toctree-l">即使在数据维度比样本数量大的情况下仍然有效.</li>
<li class="toctree-l">在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的.</li>
<li class="toctree-l">通用性: 不同的核函数 <a class="calibre3 pcalibre" href="#calibre_link-141"><span class="calibre4">核函数</span></a> 与特定的决策函数一一对应.常见的 kernel 已</li>
</ul>
<p class="calibre10">经提供,也可以指定定制的内核.</p>
</div>
</blockquote>
<p class="calibre10">支持向量机的缺点包括:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">如果特征数量比样本数量大得多,在选择核函数 <a class="calibre3 pcalibre" href="#calibre_link-141"><span class="calibre4">核函数</span></a> 时要避免过拟合,</li>
</ul>
<p class="calibre10">而且正则化项是非常重要的.</p>
<ul class="calibre6">
<li class="toctree-l">支持向量机不直接提供概率估计,这些都是使用昂贵的五次交叉验算计算的.
(详情见 <a class="calibre3 pcalibre" href="#calibre_link-142"><span class="calibre4">Scores and probabilities</span></a>, 在下文中).</li>
</ul>
</div>
</blockquote>
<p class="calibre10">在 scikit-learn 中,支持向量机提供 dense(<code class="docutils"><span class="calibre4">numpy.ndarray</span></code> ,可以通过 <code class="docutils"><span class="calibre4">numpy.asarray</span></code> 进行转换) 和 sparse (任何 <code class="docutils"><span class="calibre4">scipy.sparse</span></code>) 样例向量作为输出.然而,要使用支持向量机来对 sparse 数据作预测,它必须已经拟合这样的数据.使用 C 代码的 <code class="docutils"><span class="calibre4">numpy.ndarray</span></code> (dense) 或者带有 <code class="docutils"><span class="calibre4">dtype=float64</span></code> 的 <code class="docutils"><span class="calibre4">scipy.sparse.csr_matrix</span></code> (sparse) 来优化性能.</p>
<div class="toctree-wrapper" id="calibre_link-138">
<span id="calibre_link-328" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.1. 分类</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 能在数据集中实现多元分类.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/svm/plot_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_0012.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000087.jpg" class="math" /></a>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 是相似的方法, 但是接受稍许不同的参数设置并且有不同的数学方程(在这部分看 <a class="calibre3 pcalibre" href="#calibre_link-143"><span class="calibre4">数学公式</span></a>). 另一方面, <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 是另一个实现线性核函数的支持向量分类. 记住 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 不接受关键词 <code class="docutils"><span class="calibre4">kernel</span></code>, 因为它被假设为线性的. 它也缺少一些 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 的成员(members) 比如 <code class="docutils"><span class="calibre4">support_</span></code> .</p>
<p class="calibre10">和其他分类器一样, <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 将两个数组作为输入:  <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_features]</span></code> 大小的数组 X 作为训练样本, <code class="docutils"><span class="calibre4">[n_samples]</span></code> 大小的数组 y 作为类别标签(字符串或者整数):</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">    tol=0.001, verbose=False)</span>
</pre>
</div>
</div>
<p class="calibre10">在拟合后, 这个模型可以用来预测新的值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([1])</span>
</pre>
</div>
</div>
<p class="calibre10">SVMs 决策函数取决于训练集的一些子集, 称作支持向量. 这些支持向量的部分特性可以在 <code class="docutils"><span class="calibre4">support_vectors_</span></code>, <code class="docutils"><span class="calibre4">support_</span></code> 和 <code class="docutils"><span class="calibre4">n_support</span></code> 找到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 获得支持向量</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">support_vectors_</span>
<span class="calibre4">array([[ 0.,  0.],</span>
<span class="calibre4">       [ 1.,  1.]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 获得支持向量的索引get indices of support vectors</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">support_</span> 
<span class="calibre4">array([0, 1]...)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 为每一个类别获得支持向量的数量</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">n_support_</span> 
<span class="calibre4">array([1, 1]...)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-329">
<span id="calibre_link-330" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.4.1.1. 多元分类</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 为多元分类实现了 “one-against-one” 的方法 (Knerr et al., 1990) 如果 <code class="docutils"><span class="calibre4">n_class</span></code> 是类别的数量, 那么 <code class="docutils"><span class="calibre4">n_class</span> <span class="calibre4">*</span> <span class="calibre4">(n_class</span> <span class="calibre4">-</span> <span class="calibre4">1)</span> <span class="calibre4">/</span> <span class="calibre4">2</span></code> 分类器被重构, 而且每一个从两个类别中训练数据. 为了给其他分类器提供一致的交互, <code class="docutils"><span class="calibre4">decision_function_shape</span></code> 选项允许聚合 “one-against-one” 分类器的结果成 <code class="docutils"><span class="calibre4">(n_samples,</span> <span class="calibre4">n_classes)</span></code> 的大小到决策函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">decision_function_shape</span><span class="calibre4">=</span><span class="calibre4">'ovo'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span> 
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">    tol=0.001, verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span> <span class="calibre4"># 4 classes: 4*3/2 = 6</span>
<span class="calibre4">6</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">decision_function_shape</span> <span class="calibre4">=</span> <span class="calibre4">"ovr"</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span> <span class="calibre4"># 4 classes</span>
<span class="calibre4">4</span>
</pre>
</div>
</div>
<p class="calibre10">另一方面, <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 实现 “one-vs-the-rest” 多类别策略, 从而训练 n 类别的模型. 如果只有两类, 只训练一个模型.:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lin_clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">LinearSVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lin_clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span> 
<span class="calibre4">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="calibre4">     intercept_scaling=1, loss='squared_hinge', max_iter=1000,</span>
<span class="calibre4">     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,</span>
<span class="calibre4">     verbose=0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span> <span class="calibre4">=</span> <span class="calibre4">lin_clf</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dec</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">4</span>
</pre>
</div>
</div>
<p class="calibre10">参见 <a class="calibre3 pcalibre" href="#calibre_link-143"><span class="calibre4">数学公式</span></a> 查看决策函数的完整描述.</p>
<p class="calibre10">记住 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 也实现了可选择的多类别策略, 通过使用选项 <code class="docutils"><span class="calibre4">multi_class='crammer_singer'</span></code>, 所谓的多元 SVM 由 Crammer 和 Singer 明确表达. 这个方法是一致的, 对于 one-vs-rest 是不正确的. 实际上, one-vs-rest 分类通常受到青睐, 因为结果大多数是相似的, 但是运行时间却显著减少.</p>
<p class="calibre10">对于 “one-vs-rest” <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a>, 属性 <code class="docutils"><span class="calibre4">coef_</span></code> 和 <code class="docutils"><span class="calibre4">intercept_</span></code> 分别具有 <code class="docutils"><span class="calibre4">[n_class,</span> <span class="calibre4">n_features]</span></code> 和 <code class="docutils"><span class="calibre4">[n_class]</span></code> 尺寸. 系数的每一行符合 <code class="docutils"><span class="calibre4">n_class</span></code> 的许多 one-vs-rest 分类器之一, 并且就以这一类的顺序与拦截器(intercepts)相似.</p>
<p class="calibre10">至于 one-vs-one <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, 属性特征的布局(layout)有少多些复杂. 考虑到有一种线性核函数, <code class="docutils"><span class="calibre4">coef_</span></code> 和 <code class="docutils"><span class="calibre4">intercept_</span></code> 的布局(layout)与上文描述成 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 相似, 除了 <code class="docutils"><span class="calibre4">coef_</span></code> 的形状 <code class="docutils"><span class="calibre4">[n_class</span> <span class="calibre4">*</span> <span class="calibre4">(n_class</span> <span class="calibre4">-</span> <span class="calibre4">1)</span> <span class="calibre4">/</span> <span class="calibre4">2,</span> <span class="calibre4">n_features]</span></code>, 与许多二元的分类器相似. 0到n的类别顺序是 “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, … “n-1 vs n”.</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">dual_coef_</span></code> 的 shape 是 <code class="docutils"><span class="calibre4">[n_class-1,</span> <span class="calibre4">n_SV]</span></code>, 这个结构有些难以理解.
对应于支持向量的列与 <code class="docutils"><span class="calibre4">n_class</span> <span class="calibre4">*</span> <span class="calibre4">(n_class</span> <span class="calibre4">-</span> <span class="calibre4">1)</span> <span class="calibre4">/</span> <span class="calibre4">2</span></code> “one-vs-one” 分类器相关.
每一个支持向量用于 <code class="docutils"><span class="calibre4">n_class</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code> 分类器中.对于这些分类器,每一行的 <code class="docutils"><span class="calibre4">n_class</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code>
条目对应于对偶系数(dual coefficients).</p>
<p class="calibre10">通过这个例子更容易说明:</p>
<p class="calibre10">考虑一个三类的问题,类0有三个支持向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000734.jpg" alt="v^{0}_0, v^{1}_0, v^{2}_0" /> 而类 1 和 2 分别有
如下两个支持向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000709.jpg" alt="v^{0}_1, v^{1}_1" /> and <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000195.jpg" alt="v^{0}_2, v^{1}_2" />.对于每个支持
向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000330.jpg" alt="v^{j}_i" />, 有两个对偶系数.在类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000567.jpg" alt="\alpha^{j}_{i,k}" /> 中,
我们将支持向量的系数记录为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000330.jpg" alt="v^{j}_i" />
那么 <code class="docutils"><span class="calibre4">dual_coef_</span></code> 可以表示为:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="36%" class="label"></col>
<col width="36%" class="label"></col>
<col width="27%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000164.jpg" alt="\alpha^{0}_{0,1}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000624.jpg" alt="\alpha^{0}_{0,2}" /></td>
<td rowspan="3" class="label1">Coefficients
for SVs of class 0</td>
</tr>
<tr class="row-odd"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000583.jpg" alt="\alpha^{1}_{0,1}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000756.jpg" alt="\alpha^{1}_{0,2}" /></td>
</tr>
<tr class="calibre23"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000878.jpg" alt="\alpha^{2}_{0,1}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000365.jpg" alt="\alpha^{2}_{0,2}" /></td>
</tr>
<tr class="row-odd"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000851.jpg" alt="\alpha^{0}_{1,0}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000800.jpg" alt="\alpha^{0}_{1,2}" /></td>
<td rowspan="2" class="label1">Coefficients
for SVs of class 1</td>
</tr>
<tr class="calibre23"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000673.jpg" alt="\alpha^{1}_{1,0}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000356.jpg" alt="\alpha^{1}_{1,2}" /></td>
</tr>
<tr class="row-odd"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000543.jpg" alt="\alpha^{0}_{2,0}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000607.jpg" alt="\alpha^{0}_{2,1}" /></td>
<td rowspan="2" class="label1">Coefficients
for SVs of class 2</td>
</tr>
<tr class="calibre23"><td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000349.jpg" alt="\alpha^{1}_{2,0}" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000150.jpg" alt="\alpha^{1}_{2,1}" /></td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-142">
<span id="calibre_link-331" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.4.1.2. 得分和概率</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 方法的 <code class="docutils"><span class="calibre4">decision_function</span></code> 给每一个样例每一个类别分值(scores)(或者在一个二元类中每一个样例一个分值).
当构造器(constructor)选项 <code class="docutils"><span class="calibre4">probability</span></code> 设置为 <code class="docutils"><span class="calibre4">True</span></code> 的时候, 类成员可能性评估开启.(来自 <code class="docutils"><span class="calibre4">predict_proba</span></code> 和 <code class="docutils"><span class="calibre4">predict_log_proba</span></code> 方法)
在二元分类中,概率使用 Platt scaling 进行标准化: 在 SVM 分数上的逻辑回归,在训练集上用额外的交叉验证来拟合.在多类情况下,这可以扩展为 per Wu et al.(2004)</p>
<p class="calibre10">不用说,对于大数据集来说,在 Platt scaling 中进行交叉验证是一项昂贵的操作.
另外,可能性预测可能与 scores 不一致,因为 scores 的 “argmax” 可能不是可能性的 argmax.
(例如,在二元分类中,一个样本可能被标记为一个有可能性的类 <code class="docutils"><span class="calibre4">predict</span></code> &lt;½ according to <code class="docutils"><span class="calibre4">predict_proba</span></code>.) Platt 的方法也有理论问题.
如果 confidence scores 必要,但是这些没必要是可能性, 那么建议设置 <code class="docutils"><span class="calibre4">probability=False</span></code> 并使用 <code class="docutils"><span class="calibre4">decision_function</span></code> 而不是 <code class="docutils"><span class="calibre4">predict_proba</span></code>.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Wu, Lin and Weng,
<a href="#calibre_link-144" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-332">`"Probability estimates for multi-class classification by pairwise coupling（成对耦合的多类分类的概率估计）"&lt;http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf&gt;`_</span></a>, JMLR 5:975-1005, 2004.</li>
<li class="toctree-l">Platt
<a href="#calibre_link-145" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-333">`"Probabilistic outputs for SVMs and comparisons to regularized likelihood methods（SVMs 的概率输出和与规则化似然方法的比较）"&lt;http://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf&gt;`_</span></a> .</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-334">
<h3 class="sigil_not_in_toc1">1.4.1.3. 非均衡问题</h3>
<p class="calibre2">这个问题期望给予某一类或某个别样例能使用的关键词 <code class="docutils"><span class="calibre4">class_weight</span></code> 和 <code class="docutils"><span class="calibre4">sample_weight</span></code> 提高权重(importance).</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> (而不是 <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a>) 在 <code class="docutils"><span class="calibre4">fit</span></code> 方法中生成了一个关键词 <code class="docutils"><span class="calibre4">class_weight</span></code>. 它是形如 <code class="docutils"><span class="calibre4">{class_label</span> <span class="calibre4">:</span> <span class="calibre4">value}</span></code> 的字典, value 是浮点数大于 0 的值, 把类 <code class="docutils"><span class="calibre4">class_label</span></code> 的参数 <code class="docutils"><span class="calibre4">C</span></code> 设置为 <code class="docutils"><span class="calibre4">C</span> <span class="calibre4">*</span> <span class="calibre4">value</span></code>.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000733.jpg" class="calibre27" /></a>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">OneClassSVM</span></code></a> 在 <code class="docutils"><span class="calibre4">fit</span></code> 方法中通过关键词 <code class="docutils"><span class="calibre4">sample_weight</span></code>  为单一样例实现权重weights.与 <code class="docutils"><span class="calibre4">class_weight</span></code> 相似, 这些把第i个样例的参数 <code class="docutils"><span class="calibre4">C</span></code> 换成 <code class="docutils"><span class="calibre4">C</span> <span class="calibre4">*</span> <span class="calibre4">sample_weight[i]</span></code>.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/svm/plot_weighted_samples.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_weighted_samples_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000047.jpg" class="calibre28" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py"><span class="calibre4">Plot different SVM classifiers in the iris dataset</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_separating_hyperplane.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-py"><span class="calibre4">SVM: Maximum margin separating hyperplane</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="calibre4">SVM: Separating hyperplane for unbalanced classes</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py"><span class="calibre4">SVM-Anova: SVM with univariate feature selection</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_svm_nonlinear.html#sphx-glr-auto-examples-svm-plot-svm-nonlinear-py"><span class="calibre4">Non-linear SVM</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_weighted_samples.html#sphx-glr-auto-examples-svm-plot-weighted-samples-py"><span class="calibre4">SVM: Weighted samples</span></a>,</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-139">
<span id="calibre_link-335" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.2. 回归</h2>
<p class="calibre2">支持向量分类的方法可以被扩展用作解决回归问题. 这个方法被称作支持向量回归.</p>
<p class="calibre10">支持向量分类生成的模型(如前描述)只依赖于训练集的子集,因为构建模型的 cost function 不在乎边缘之外的训练点. 类似的,支持向量回归生成的模型只依赖于训练集的子集, 因为构建模型的 cost function 忽略任何接近于模型预测的训练数据.</p>
<p class="calibre10">支持向量分类有三种不同的实现形式:
<a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code class="docutils"><span class="calibre4">LinearSVR</span></code></a>. 在只考虑线性核的情况下, <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code class="docutils"><span class="calibre4">LinearSVR</span></code></a>  比 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a> 提供一个更快的实现形式, 然而比起 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" title="sklearn.svm.LinearSVR"><code class="docutils"><span class="calibre4">LinearSVR</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a> 实现一个稍微不同的构思(formulation).细节参见 <a class="calibre3 pcalibre" href="#calibre_link-146"><span class="calibre4">实现细节</span></a>.</p>
<p class="calibre10">与分类的类别一样, fit方法会调用参数向量 X, y, 只在 y 是浮点数而不是整数型.:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2.5</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVR</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span> 
<span class="calibre4">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',</span>
<span class="calibre4">    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">array([ 1.5])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">样例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py"><span class="calibre4">Support Vector Regression (SVR) using linear and non-linear kernels</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-336">
<span id="calibre_link-140" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.3. 密度估计, 异常（novelty）检测</h2>
<p class="calibre2">但类别的 SVM 用于异常检测, 即给予一个样例集, 它会检测这个样例集的 soft boundary 以便给新的数据点分类,
看它是否属于这个样例集. 生成的类称作 <a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">OneClassSVM</span></code></a>.</p>
<p class="calibre10">这种情况下, 因为它属于非监督学习的一类, 没有类标签, fit 方法只会考虑输入数组X.</p>
<p class="calibre10">在章节 <a class="calibre3 pcalibre" href="outlier_detection.html#outlier-detection"><span class="calibre4">新奇和异常值检测</span></a> 查看这个应用的更多细节.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/svm/plot_oneclass.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_oneclass_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000466.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py"><span class="calibre4">One-class SVM with non-linear kernel (RBF)</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py"><span class="calibre4">Species distribution modeling</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-337">
<h2 class="sigil_not_in_toc">1.4.4. 复杂度</h2>
<p class="calibre2">支持向量机是个强大的工具，不过它的计算和存储空间要求也会随着要训练向量的数目增加而快速增加。
SVM的核心是一个二次规划问题(Quadratic Programming, QP)，是将支持向量和训练数据的其余部分分离开来。
在实践中(数据集相关)，会根据 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 的缓存有多效，在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000584.jpg" alt="O(n_{features} \times n_{samples}^2)" /> 和
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000359.jpg" alt="O(n_{features} \times n_{samples}^3)" /> 之间基于 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 的缩放操作才会调用这个 QP 解析器。
如果数据是非常稀疏，那 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000759.jpg" alt="n_{features}" />  就用样本向量中非零特征的平均数量去替换。</p>
<p class="calibre10">另外请注意，在线性情况下，由 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 操作的 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 算法要比由它的 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 对应的
<a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 更为高效，并且它几乎可以线性缩放到数百万样本或者特征。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-338">
<h2 class="sigil_not_in_toc">1.4.5. 使用诀窍</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><p class="first"><strong class="calibre14">避免数据复制</strong>: 对于 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>， <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a>， <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a> 和
<a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a>， 如果数据是通过某些方法而不是用 C 有序的连续双精度，那它先会调用底层的 C 命令再复制。
您可以通过检查它的 <code class="docutils"><span class="calibre4">flags</span></code> 属性，来确定给定的 numpy 数组是不是 C 连续的。</p>
<p class="calibre10">对于 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> (和 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a>) 的任何输入，都会以 numpy 数组形式，被复制和转换为
用 liblinear 内部稀疏数据去表达（双精度浮点型 float 和非零部分的 int32 索引）。
如果您想要一个适合大规模的线性分类器，又不打算复制一个密集的 C-contiguous 双精度 numpy 数组作为输入，
那我们建议您去使用 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 类作为替代。目标函数可以配置为和 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a>
模型差不多相同的。</p>
</li>
<li class="toctree-l"><p class="first"><strong class="calibre14">内核的缓存大小</strong>: 在大规模问题上，对于 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="docutils"><span class="calibre4">SVR</span></code></a>, <code class="docutils"><span class="calibre4">nuSVC</span></code> 和
<a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a>, 内核缓存的大小会特别影响到运行时间。如果您有足够可用的 RAM，不妨把它的 <code class="docutils"><span class="calibre4">缓存大小</span></code>
设得比默认的 200(MB) 要高，例如为 500(MB) 或者 1000(MB)。</p>
</li>
<li class="toctree-l"><p class="first"><strong class="calibre14">惩罚系数C的设置</strong>:在合理的情况下， <code class="docutils"><span class="calibre4">C</span></code> 的默认选择为 <code class="docutils"><span class="calibre4">1</span></code> 。如果您有很多混杂的观察数据，
您应该要去调小它。 <code class="docutils"><span class="calibre4">C</span></code> 越小，就能更好地去正规化估计。</p>
</li>
<li class="toctree-l"><p class="first">支持向量机算法本身不是用来扩大不变性，所以 <strong class="calibre14">我们强烈建议您去扩大数据量</strong>. 举个例子，对于输入向量 X，
规整它的每个数值范围为 [0, 1] 或 [-1, +1] ，或者标准化它的为均值为0方差为1的数据分布。请注意，
相同的缩放标准必须要应用到所有的测试向量，从而获得有意义的结果。 请参考章节
<a class="calibre3 pcalibre" href="preprocessing.html#preprocessing"><span class="calibre4">预处理数据</span></a> ，那里会提供到更多关于缩放和规整。</p>
</li>
<li class="toctree-l"><p class="first">在 <a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">NuSVC</span></code></a>/<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">OneClassSVM</span></code></a>/<a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="docutils"><span class="calibre4">NuSVR</span></code></a> 内的参数 <code class="docutils"><span class="calibre4">nu</span></code> ，
近似是训练误差和支持向量的比值。</p>
</li>
<li class="toctree-l"><p class="first">在 <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a>, ，如果分类器的数据不均衡（就是说，很多正例很少负例），设置 <code class="docutils"><span class="calibre4">class_weight='balanced'</span></code>
与/或尝试不同的惩罚系数 <code class="docutils"><span class="calibre4">C</span></code> 。</p>
</li>
<li class="toctree-l"><p class="first">在拟合模型时，底层 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a> 操作使用了随机数生成器去选择特征。
所以不要感到意外，对于相同的数据输入，也会略有不同的输出结果。如果这个发生了，
尝试用更小的 tol 参数。</p>
</li>
<li class="toctree-l"><p class="first">使用由 <code class="docutils"><span class="calibre4">LinearSVC(loss='l2',</span> <span class="calibre4">penalty='l1',</span>
<span class="calibre4">dual=False)</span></code> 提供的 L1 惩罚去产生稀疏解，也就是说，特征权重的子集不同于零，这样做有助于决策函数。
随着增加 <code class="docutils"><span class="calibre4">C</span></code> 会产生一个更复杂的模型（要做更多的特征选择）。可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code class="docutils"><span class="calibre4">l1_min_c</span></code></a> 去计算 <code class="docutils"><span class="calibre4">C</span></code> 的数值，去产生一个”null” 模型（所有的权重等于零）。</p>
</li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-141">
<span id="calibre_link-339" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.6. 核函数</h2>
<p class="calibre2"><em class="calibre13">核函数</em> 可以是以下任何形式：:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">线性: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000015.jpg" alt="\langle x, x&apos;\rangle" />.</li>
<li class="toctree-l">多项式: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000030.jpg" alt="(\gamma \langle x, x&apos;\rangle + r)^d" />.
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 是关键词 <code class="docutils"><span class="calibre4">degree</span></code>, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 指定 <code class="docutils"><span class="calibre4">coef0</span></code>。</li>
<li class="toctree-l">rbf: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000049.jpg" alt="\exp(-\gamma \|x-x&apos;\|^2)" />. <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000566.jpg" alt="\gamma" /> 是关键词 <code class="docutils"><span class="calibre4">gamma</span></code>, 必须大于 0。</li>
<li class="toctree-l">sigmoid (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000344.jpg" alt="\tanh(\gamma \langle x,x&apos;\rangle + r)" />),
其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 指定 <code class="docutils"><span class="calibre4">coef0</span></code>。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">初始化时，不同内核由不同的函数名调用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">linear_svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">linear_svc</span><span class="calibre4">.</span><span class="calibre4">kernel</span>
<span class="calibre4">'linear'</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rbf_svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rbf_svc</span><span class="calibre4">.</span><span class="calibre4">kernel</span>
<span class="calibre4">'rbf'</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-340">
<h3 class="sigil_not_in_toc1">1.4.6.1. 自定义核</h3>
<p class="calibre2">您可以自定义自己的核，通过使用python函数作为内核或者通过预计算 Gram 矩阵。</p>
<p class="calibre10">自定义内核的分类器和别的分类器一样，除了下面这几点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">空间 <code class="docutils"><span class="calibre4">support_vectors_</span></code> 现在不是空的, 只有支持向量的索引被存储在 <code class="docutils"><span class="calibre4">support_</span></code></li>
<li class="toctree-l">请把 <code class="docutils"><span class="calibre4">fit()</span></code> 模型中的第一个参数的引用（不是副本）存储为将来的引用。
如果在 <code class="docutils"><span class="calibre4">fit()</span></code> 和 <code class="docutils"><span class="calibre4">predict()</span></code> 之间有数组发生改变，您将会碰到意料外的结果。</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-341">
<h4 class="sigil_not_in_toc1">1.4.6.1.1. 使用 python 函数作为内核</h4>
<p class="calibre2">在构造时，您同样可以通过一个函数传递到关键词 <code class="docutils"><span class="calibre4">kernel</span></code> ，来使用您自己定义的内核。</p>
<p class="calibre10">您的内核必须要以两个矩阵作为参数，大小分别是
<code class="docutils"><span class="calibre4">(n_samples_1,</span> <span class="calibre4">n_features)</span></code>, <code class="docutils"><span class="calibre4">(n_samples_2,</span> <span class="calibre4">n_features)</span></code>
和返回一个内核矩阵，shape 是 <code class="docutils"><span class="calibre4">(n_samples_1,</span> <span class="calibre4">n_samples_2)</span></code>.</p>
<p class="calibre10">以下代码定义一个线性核，和构造一个使用该内核的分类器例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">my_kernel</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">return</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">dot</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">.</span><span class="calibre4">T</span><span class="calibre4">)</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">my_kernel</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_custom_kernel.html#sphx-glr-auto-examples-svm-plot-custom-kernel-py"><span class="calibre4">SVM with custom kernel</span></a>.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-342">
<h4 class="sigil_not_in_toc1">1.4.6.1.2. 使用 Gram 矩阵</h4>
<p class="calibre2">在适应算法中，设置 <code class="docutils"><span class="calibre4">kernel='precomputed'</span></code> 和把 X 替换为 Gram 矩阵。
此时，必须要提供在 <em class="calibre13">所有</em> 训练矢量和测试矢量中的内核值。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'precomputed'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 线性内核计算</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gram</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">dot</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">T</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">gram</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span> 
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">    decision_function_shape='ovr', degree=3, gamma='auto',</span>
<span class="calibre4">    kernel='precomputed', max_iter=-1, probability=False,</span>
<span class="calibre4">    random_state=None, shrinking=True, tol=0.001, verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 预测训练样本</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">gram</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 1])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-343">
<h4 class="sigil_not_in_toc1">1.4.6.1.3. RBF 内核参数</h4>
<p class="calibre2">当用 <em class="calibre13">径向基</em> (RBF) 内核去训练 SVM，有两个参数必须要去考虑： <code class="docutils"><span class="calibre4">C</span></code> 惩罚系数和 <code class="docutils"><span class="calibre4">gamma</span></code> 。参数 <code class="docutils"><span class="calibre4">C</span></code> ，
通用在所有 SVM 内核，与决策表面的简单性相抗衡，可以对训练样本的误分类进行有价转换。
较小的 <code class="docutils"><span class="calibre4">C</span></code> 会使决策表面更平滑，同时较高的 <code class="docutils"><span class="calibre4">C</span></code> 旨在正确地分类所有训练样本。 <code class="docutils"><span class="calibre4">Gamma</span></code> 定义了单一
训练样本能起到多大的影响。较大的 <code class="docutils"><span class="calibre4">gamma</span></code> 会更让其他样本受到影响。</p>
<p class="calibre10">选择合适的 <code class="docutils"><span class="calibre4">C</span></code> 和 <code class="docutils"><span class="calibre4">gamma</span></code> ，对SVM的性能起到很关键的作用。建议一点是
使用 &nbsp;<a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="docutils"><span class="calibre4">sklearn.model_selection.GridSearchCV</span></code></a> 与 <code class="docutils"><span class="calibre4">C</span></code> 和 <code class="docutils"><span class="calibre4">gamma</span></code> 相隔
成倍差距从而选择到好的数值。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py"><span class="calibre4">RBF SVM parameters</span></a></li>
</ul>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-143">
<span id="calibre_link-344" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.7. 数学公式</h2>
<p class="calibre2">支持向量机在高维度或无穷维度空间中，构建一个超平面或者一系列的超平面，可以用于分类、回归或者别的任务。
直观地看，借助超平面去实现一个好的分割， 能在任意类别中使最为接近的训练数据点具有最大的间隔距离（即所
谓的函数余量），这样做是因为通常更大的余量能有更低的分类器泛化误差。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_0011.png"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_separating_hyperplane_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000645.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-345">
<h3 class="sigil_not_in_toc1">1.4.7.1. SVC</h3>
<p class="calibre2">在两类中，给定训练向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000462.jpg" alt="x_i \in \mathbb{R}^p" />, i=1,…, n, 和一个向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000230.jpg" alt="y \in \{1, -1\}^n" />, SVC能解决
如下主要问题:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000052.jpg" alt="\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i    \textrm {subject to } &amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\ &amp; \zeta_i \geq 0, i=1, ..., n" class="math" /></p>
</div>
<p class="calibre10">它的对偶是</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000046.jpg" alt="\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha   \textrm {subject to } &amp; y^T \alpha = 0\\ &amp; 0 \leq \alpha_i \leq C, i=1, ..., n" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000402.jpg" alt="e" /> 是所有的向量， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000370.jpg" alt="C &gt; 0" /> 是上界，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000498.jpg" alt="Q" /> 是一个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 由 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个半正定矩阵，
而 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000104.jpg" alt="Q_{ij} \equiv y_i y_j K(x_i, x_j)" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000006.jpg" alt="K(x_i, x_j) = \phi (x_i)^T \phi (x_j)" /> 是内核。所以训练向量是通过函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" />，间接反映到一个更高维度的（无穷的）空间。</p>
<p class="calibre10">决策函数是:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000008.jpg" alt="\operatorname{sgn}(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)" class="math" /></p>
</div>
<p class="calibre10">注意:</p>
<p class="calibre10">虽然这些SVM模型是从 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 和 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 中派生出来，使用了 <code class="docutils"><span class="calibre4">C</span></code> 作为调整参数，但是大多数的
攻击使用了 <code class="docutils"><span class="calibre4">alpha</span></code>。两个模型的正则化量之间的精确等价，取决于模型优化的准确目标函数。举
个例子，当使用的估计器是 <code class="docutils"><span class="calibre4">sklearn.linear_model.Ridge</span></code> 做回归时，他们之间的相关性是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000798.jpg" alt="C = \frac{1}{alpha}" />。</p>
<p class="calibre10">这些参数能通过成员 <code class="docutils"><span class="calibre4">dual_coef_</span></code>、 <code class="docutils"><span class="calibre4">support_vectors_</span></code> 、 <code class="docutils"><span class="calibre4">intercept_</span></code> 去访问，这些成员分别控制了输出 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000803.jpg" alt="y_i \alpha_i" />、支持向量和无关项 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" /> ：</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215">“Automatic Capacity Tuning of Very Large VC-dimension Classifiers”</a>,
I. Guyon, B. Boser, V. Vapnik - Advances in neural information
processing 1993.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://link.springer.com/article/10.1007%2FBF00994018">“Support-vector networks”</a>,
C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995).</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-346">
<h3 class="sigil_not_in_toc1">1.4.7.2. NuSVC</h3>
<p class="calibre2">我们引入一个新的参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" /> 来控制支持向量的数量和训练误差。参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000439.jpg" alt="\nu \in (0, 1]" /> 是训练误差分数的上限和支持向量分数的下限。</p>
<p class="calibre10">可以看出， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" />-SVC 公式是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" />-SVC 的再参数化，所以数学上是等效的。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-347">
<h3 class="sigil_not_in_toc1">1.4.7.3. SVR</h3>
<p class="calibre2">给定训练向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000462.jpg" alt="x_i \in \mathbb{R}^p" />, i=1,…, n，向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000762.jpg" alt="y \in \mathbb{R}^n" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000806.jpg" alt="\varepsilon" />-SVR
能解决以下的主要问题：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000724.jpg" alt="\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)    \textrm {subject to } &amp; y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\                       &amp; w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\                       &amp; \zeta_i, \zeta_i^* \geq 0, i=1, ..., n" class="math" /></p>
</div>
<p class="calibre10">它的对偶是</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000264.jpg" alt="\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)   \textrm {subject to } &amp; e^T (\alpha - \alpha^*) = 0\\ &amp; 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000402.jpg" alt="e" /> 是所有的向量， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000370.jpg" alt="C &gt; 0" /> 是上界，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000498.jpg" alt="Q" /> 是一个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 由 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个半正定矩阵，
而 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000781.jpg" alt="Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)" /> 是内核。
所以训练向量是通过函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" />，间接反映到一个更高维度的（无穷的）空间。</p>
<p class="calibre10">决策函数是:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000573.jpg" alt="\sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + \rho" class="math" /></p>
</div>
<p class="calibre10">这些参数能通过成员 <code class="docutils"><span class="calibre4">dual_coef_</span></code>、 <code class="docutils"><span class="calibre4">support_vectors_</span></code> 、 <code class="docutils"><span class="calibre4">intercept_</span></code> 去访问，这些
成员分别控制了不同的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000348.jpg" alt="\alpha_i - \alpha_i^*" />、支持向量和无关项 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" />：</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288">“A Tutorial on Support Vector Regression”</a>,
Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive
Volume 14 Issue 3, August 2004, p. 199-222.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-146">
<span id="calibre_link-348" class="calibre4"></span><h2 class="sigil_not_in_toc">1.4.8. 实现细节</h2>
<p class="calibre2">在底层里，我们使用 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> 和 <a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> 去处理所有的计算。这些库都使用了 C 和 Cython 去包装。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<p class="calibre10">有关实现的描述和使用算法的细节，请参考</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a>.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR &ndash; A Library for Large Linear Classification</a>.</li>
</ul>
</div>
</blockquote>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-91">
<span id="calibre_link-349" class="calibre4"></span><h1 class="calibre5">1.5. 随机梯度下降</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@A</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@L</a><br class="calibre9" />   
    </div>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/HelloSilicat" class="calibre3 pcalibre">@HelloSilicat</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@A</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@L</a><br class="calibre9" />     
    </div>
<p class="calibre10"><strong class="calibre14">随机梯度下降(SGD)</strong> 是一种简单但又非常高效的方法，主要用于凸损失函数下线性分类器的判别式学习，例如(线性) <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a> 和 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic 回归</a> 。
尽管 SGD 在机器学习社区已经存在了很长时间, 但是最近在 large-scale learning （大规模学习）方面 SGD 获得了相当大的关注。</p>
<p class="calibre10">SGD 已成功应用于在文本分类和自然语言处理中经常遇到的大规模和稀疏的机器学习问题。对于稀疏数据，本模块的分类器可以轻易的处理超过 10^5 的训练样本和超过 10^5 的特征。</p>
<p class="calibre10">Stochastic Gradient Descent （随机梯度下降法）的优势:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">高效。</li>
<li class="toctree-l">易于实现 (有大量优化代码的机会)。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">Stochastic Gradient Descent （随机梯度下降法）的劣势:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">SGD 需要一些超参数，例如 regularization （正则化）参数和 number of iterations （迭代次数）。</li>
<li class="toctree-l">SGD 对 feature scaling （特征缩放）敏感。</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-350">
<h2 class="sigil_not_in_toc">1.5.1. 分类</h2>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">在拟合模型前，确保你重新排列了（打乱）)你的训练数据，或者在每次迭代后用 <code class="docutils"><span class="calibre4">shuffle=True</span></code> 来打乱。</p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 类实现了一个简单的随机梯度下降学习例程, 支持不同的 loss functions（损失函数）和 penalties for classification（分类处罚）。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000804.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">作为另一个 classifier （分类器）, 拟合 SGD 我们需要两个 array （数组）：保存训练样本的 size 为 [n_samples, n_features] 的数组 X 以及保存训练样本目标值（类标签）的 size 为 [n_samples] 的数组 Y</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">SGDClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SGDClassifier</span><span class="calibre4">(</span><span class="calibre4">loss</span><span class="calibre4">=</span><span class="calibre4">"hinge"</span><span class="calibre4">,</span> <span class="calibre4">penalty</span><span class="calibre4">=</span><span class="calibre4">"l2"</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span>
<span class="calibre4">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span>
<span class="calibre4">       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,</span>
<span class="calibre4">       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,</span>
<span class="calibre4">       shuffle=True, tol=None, verbose=0, warm_start=False)</span>
</pre>
</div>
</div>
<p class="calibre10">拟合后，我们可以用该模型来预测新值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([1])</span>
</pre>
</div>
</div>
<p class="calibre10">SGD 通过训练数据来拟合一个线性模型。成员 <code class="docutils"><span class="calibre4">coef_</span></code> 保存模型参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">coef_</span>                                         
<span class="calibre4">array([[ 9.9...,  9.9...]])</span>
</pre>
</div>
</div>
<p class="calibre10">成员 <code class="docutils"><span class="calibre4">intercept_</span></code> 保存 intercept（截距） （又称作 offset（偏移）或 bias（偏差））:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">intercept_</span>                                    
<span class="calibre4">array([-9.9...])</span>
</pre>
</div>
</div>
<p class="calibre10">模型是否使用 intercept（截距）, 即 a biased hyperplane(一个偏置的超平面), 是由参数 <code class="docutils"><span class="calibre4">fit_intercept</span></code> 控制的。</p>
<p class="calibre10">使用 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code class="docutils"><span class="calibre4">SGDClassifier.decision_function</span></code></a> 来获得到此超平面的 signed distance (符号距离)</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>                 
<span class="calibre4">array([ 29.6...])</span>
</pre>
</div>
</div>
<p class="calibre10">具体的 loss function（损失函数） 可以通过 <code class="docutils"><span class="calibre4">loss</span></code> 参数来设置。 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 支持以下的 loss functions（损失函数）：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="hinge"</span></code>: (soft-margin) linear Support Vector Machine （（软-间隔）线性支持向量机），</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="modified_huber"</span></code>: smoothed hinge loss  （平滑的 hinge 损失），</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="log"</span></code>: logistic regression （logistic 回归），</li>
<li class="toctree-l">and all regression losses below（以及所有的回归损失）。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">前两个 loss functions（损失函数）是懒惰的，如果一个例子违反了 margin constraint（边界约束），它们仅更新模型的参数, 这使得训练非常有效率,即使使用了 L2 penalty（惩罚）我们仍然可能得到稀疏的模型结果。</p>
<p class="calibre10">使用 <code class="docutils"><span class="calibre4">loss="log"</span></code> 或者 <code class="docutils"><span class="calibre4">loss="modified_huber"</span></code> 来启用 <code class="docutils"><span class="calibre4">predict_proba</span></code> 方法, 其给出每个样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 的概率估计 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000436.jpg" alt="P(y|x)" /> 的一个向量：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SGDClassifier</span><span class="calibre4">(</span><span class="calibre4">loss</span><span class="calibre4">=</span><span class="calibre4">"log"</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict_proba</span><span class="calibre4">([[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">]])</span>                      
<span class="calibre4">array([[ 0.00...,  0.99...]])</span>
</pre>
</div>
</div>
<p class="calibre10">具体的惩罚方法可以通过 <code class="docutils"><span class="calibre4">penalty</span></code> 参数来设定。
SGD 支持以下 penalties（惩罚）:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">penalty="l2"</span></code>: L2 norm penalty on <code class="docutils"><span class="calibre4">coef_</span></code>.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">penalty="l1"</span></code>: L1 norm penalty on <code class="docutils"><span class="calibre4">coef_</span></code>.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">penalty="elasticnet"</span></code>: Convex combination of L2 and L1（L2 型和 L1 型的凸组合）;
<code class="docutils"><span class="calibre4">(1</span> <span class="calibre4">-</span> <span class="calibre4">l1_ratio)</span> <span class="calibre4">*</span> <span class="calibre4">L2</span> <span class="calibre4">+</span> <span class="calibre4">l1_ratio</span> <span class="calibre4">*</span> <span class="calibre4">L1</span></code>.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">默认设置为 <code class="docutils"><span class="calibre4">penalty="l2"</span></code> 。 L1 penalty （惩罚）导致稀疏解，使得大多数系数为零。 Elastic Net（弹性网）解决了在特征高相关时 L1 penalty（惩罚）的一些不足。参数 <code class="docutils"><span class="calibre4">l1_ratio</span></code> 控制了 L1 和 L2 penalty（惩罚）的 convex combination （凸组合）。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 通过利用 “one versus all” （OVA）方法来组合多个二分类器，从而实现多分类。对于每一个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 类, 可以训练一个二分类器来区分自身和其他 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000633.jpg" alt="K-1" /> 个类。在测试阶段，我们计算每个分类器的 confidence score（置信度分数）（也就是与超平面的距离），并选择置信度最高的分类。下图阐释了基于 iris（鸢尾花）数据集上的 OVA 方法。虚线表示三个 OVA 分类器; 不同背景色代表由三个分类器产生的决策面。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_iris_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000676.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">在 multi-class classification （多类分类）的情况下， <code class="docutils"><span class="calibre4">coef_</span></code> 是 <code class="docutils"><span class="calibre4">shape=[n_classes,</span> <span class="calibre4">n_features]</span></code> 的一个二维数组， <code class="docutils"><span class="calibre4">intercept_</span></code> 是 <code class="docutils"><span class="calibre4">shape=[n_classes]</span></code> 的一个一维数组。 <code class="docutils"><span class="calibre4">coef_</span></code> 的第 i 行保存了第 i 类的 OVA 分类器的权重向量；类以升序索引 （参照属性 <code class="docutils"><span class="calibre4">classes_</span></code> ）。
注意，原则上，由于它们允许创建一个概率模型，所以 <code class="docutils"><span class="calibre4">loss="log"</span></code> 和 <code class="docutils"><span class="calibre4">loss="modified_huber"</span></code> 更适合于 one-vs-all 分类。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 通过拟合参数 <code class="docutils"><span class="calibre4">class_weight</span></code> 和 <code class="docutils"><span class="calibre4">sample_weight</span></code> 来支持 weighted classes （加权类）和 weighted instances（加权实例）。更多信息请参照下面的示例和 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code class="docutils"><span class="calibre4">SGDClassifier.fit</span></code></a> 的文档。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py"><span class="calibre4">SGD: Maximum margin separating hyperplane</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py"><span class="calibre4">Plot multi-class SGD on the iris dataset</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py"><span class="calibre4">SGD: Weighted samples</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py"><span class="calibre4">Comparing various online solvers</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="calibre4">SVM: Separating hyperplane for unbalanced classes</span></a> (参见 <cite class="calibre13">Note</cite>)</li>
</ul>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 支持 averaged SGD (ASGD)。Averaging（均值化）可以通过设置 <code class="docutils"><span class="calibre4">`average=True`</span></code> 来启用。AGSD 工作原理是在普通 SGD 的基础上，对每个样本的每次迭代后的系数取均值。当使用 ASGD 时，学习速率可以更大甚至是恒定，在一些数据集上能够加速训练过程。</p>
<p class="calibre10">对于带 logistic loss（logistic 损失）的分类，在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 中提供了另一个采取 averaging strategy（平均策略）的 SGD 变体，其使用了随机平均梯度 (SAG) 算法。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-351">
<h2 class="sigil_not_in_toc">1.5.2. 回归</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 类实现了一个简单的随机梯度下降学习例程，它支持用不同的损失函数和惩罚来拟合线性回归模型。 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 非常适用于有大量训练样本（&gt;10.000)的回归问题，对于其他问题，我们推荐使用 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> ，<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="docutils"><span class="calibre4">Lasso</span></code></a> ，或 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="docutils"><span class="calibre4">ElasticNet</span></code></a> 。</p>
<p class="calibre10">具体的损失函数可以通过 <code class="docutils"><span class="calibre4">loss</span></code> 参数设置。 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 支持以下的损失函数:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="squared_loss"</span></code>: Ordinary least squares（普通最小二乘法）,</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="huber"</span></code>: Huber loss for robust regression（Huber回归）,</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">loss="epsilon_insensitive"</span></code>: linear Support Vector Regression（线性支持向量回归）.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">Huber 和 epsilon-insensitive 损失函数可用于 robust regression（鲁棒回归）。不敏感区域的宽度必须通过参数 <code class="docutils"><span class="calibre4">epsilon</span></code> 来设定。这个参数取决于目标变量的规模。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">SGDRegressor</span></code></a> 支持 ASGD（平均随机梯度下降） 作为 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a>。
均值化可以通过设置 <code class="docutils"><span class="calibre4">`average=True`</span></code> 来启用。</p>
<p class="calibre10">对于利用了 squared loss（平方损失）和 l2 penalty（l2惩罚）的回归，在 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="docutils"><span class="calibre4">Ridge</span></code></a> 中提供了另一个采取 averaging strategy（平均策略）的 SGD 变体，其使用了随机平均梯度 (SAG) 算法。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-352">
<h2 class="sigil_not_in_toc">1.5.3. 稀疏数据的随机梯度下降</h2>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">由于在截距部分收敛学习速率的差异，稀疏实现与密集实现相比产生的结果略有不同。</p>
</div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="https://docs.scipy.org/doc/scipy/reference/sparse.html">scipy.sparse</a> 支持的格式中，任意矩阵都有对稀疏数据的内置支持方法。但是，为了获得最高的效率，请使用 <a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a> 中定义的 CSR 矩阵格式.</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-353">
<h2 class="sigil_not_in_toc">1.5.4. 复杂度</h2>
<p class="calibre2">SGD 主要的优势在于它的高效性，对于不同规模的训练样本，处理复杂度基本上是线性的。假如 X 是 size 为 (n, p) 的矩阵，训练成本为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000013.jpg" alt="O(k n \bar p)" />，其中 k 是迭代次数， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000095.jpg" alt="\bar p" /> 是每个样本非零特征的平均数。</p>
<p class="calibre10">但是，最近的理论结果表明，得到期望优化精度的运行时间并不会随着训练集规模扩大而增加。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-354">
<h2 class="sigil_not_in_toc">1.5.5. 实用小贴士</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><p class="first">随机梯度下降法对 feature scaling （特征缩放）很敏感，因此强烈建议您缩放您的数据。例如，将输入向量 X 上的每个特征缩放到 [0,1] 或 [- 1，+1]， 或将其标准化，使其均值为 0，方差为 1。请注意，必须将 <em class="calibre13">相同</em> 的缩放应用于对应的测试向量中，以获得有意义的结果。使用 <code class="docutils"><span class="calibre4">StandardScaler</span></code>: 很容易做到这一点：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)  # Don’t cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  # apply same transformation to test data</p>
</div>
</blockquote>
<p class="calibre10">假如你的 attributes （属性）有一个固有尺度（例如 word frequencies （词频）或 indicator features（指标特征））就不需要缩放。</p>
</li>
<li class="toctree-l"><p class="first">最好使用 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 找到一个合理的 regularization term （正则化项） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> ， 它的范围通常在 <code class="docutils"><span class="calibre4">10.0**-np.arange(1,7)</span></code> 。</p>
</li>
<li class="toctree-l"><p class="first">经验表明，SGD 在处理约 10^6 训练样本后基本收敛。因此，对于迭代次数第一个合理的猜想是 <code class="docutils"><span class="calibre4">n_iter</span> <span class="calibre4">=</span> <span class="calibre4">np.ceil(10**6</span> <span class="calibre4">/</span> <span class="calibre4">n)</span></code>，其中 <code class="docutils"><span class="calibre4">n</span></code> 是训练集的大小。</p>
</li>
<li class="toctree-l"><p class="first">假如将 SGD 应用于使用 PCA 做特征提取，我们发现通过某个常数 <cite class="calibre13">c</cite> 来缩放特征值是明智的，比如使训练数据的 L2 norm 平均值为 1。</p>
</li>
<li class="toctree-l"><p class="first">我们发现，当特征很多或 eta0 很大时， ASGD（平均随机梯度下降） 效果更好。</p>
</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-355">
<span id="calibre_link-356" class="calibre4"></span><h2 class="sigil_not_in_toc">1.5.6. 数学描述</h2>
<p class="calibre2">给定一组训练样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000135.jpg" alt="(x_1, y_1), \ldots, (x_n, y_n)" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000818.jpg" alt="x_i \in \mathbf{R}^m" /> ， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000228.jpg" alt="y_i \in \{-1,1\}" />， 我们的目标是一个线性 scoring function（评价函数） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000090.jpg" alt="f(x) = w^T x + b" /> ，其中模型参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000479.jpg" alt="w \in \mathbf{R}^m" /> ，截距 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000494.jpg" alt="b \in \mathbf{R}" />。为了做预测， 我们只需要看 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000495.jpg" alt="f(x)" /> 的符号。找到模型参数的一般选择是通过最小化由以下式子给出的正则化训练误差</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000094.jpg" alt="E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> 衡量模型(mis)拟合程度的损失函数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000120.jpg" alt="R" /> 是惩罚模型复杂度的正则化项（也叫作惩罚）; <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000172.jpg" alt="\alpha &gt; 0" /> 是一个非负超平面。</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> 的不同选择产生不同的分类器，例如</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">Hinge: (soft-margin) Support Vector Machines.</li>
<li class="toctree-l">Hinge: (软-间隔) 支持向量机。</li>
<li class="toctree-l">Log:   Logistic Regression.</li>
<li class="toctree-l">Log:   Logistic 回归。</li>
<li class="toctree-l">Least-Squares: Ridge Regression.</li>
<li class="toctree-l">Least-Squares: 岭回归。</li>
<li class="toctree-l">Epsilon-Insensitive: (soft-margin) Support Vector Regression.</li>
<li class="toctree-l">Epsilon-Insensitive: (软-间隔) 支持向量回归。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">所有上述损失函数可以看作是错误分类误差的上限（0 - 1损失），如下图所示。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_loss_functions.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_loss_functions_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000647.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">比较流行的正则化项 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000120.jpg" alt="R" /> 包括：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">L2 norm: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000362.jpg" alt="R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2" />,</li>
<li class="toctree-l">L1 norm: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000042.jpg" alt="R(w) := \sum_{i=1}^{n} |w_i|" />, which leads to sparse
solutions（）.</li>
<li class="toctree-l">Elastic Net: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000760.jpg" alt="R(w) := \frac{\rho}{2} \sum_{i=1}^{n} w_i^2 + (1-\rho) \sum_{i=1}^{n} |w_i|" />, a convex combination of L2 and L1, where <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" /> is given by <code class="docutils"><span class="calibre4">1</span> <span class="calibre4">-</span> <span class="calibre4">l1_ratio</span></code>.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">下图显示当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000016.jpg" alt="R(w) = 1" /> 时参数空间中不同正则项的轮廓。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_sgd_penalties.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sgd_penalties_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000663.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-357">
<h3 class="sigil_not_in_toc1">1.5.6.1. SGD</h3>
<p class="calibre2">随机梯度下降法是一种无约束优化问题的优化方法。与（批量）梯度下降法相反，SGD 通过一次只考虑单个训练样本来近似 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000302.jpg" alt="E(w,b)" /> 真实的梯度。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 类实现了一个 first-order SGD learning routine （一阶 SGD 学习程序）。 算法在训练样本上遍历，并且对每个样本根据由以下式子给出的更新规则来更新模型参数。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000080.jpg" alt="w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial L(w^T x_i + b, y_i)}{\partial w})" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000371.jpg" alt="\eta" /> 是在参数空间中控制步长的 learning rate （学习速率）。
intercept（截距） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000303.jpg" alt="b" /> 的更新类似但不需要正则化。</p>
<p class="calibre10">学习率 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000371.jpg" alt="\eta" /> 可以恒定或者逐渐减小。对于分类来说， 默认的学习率设定方案 （<code class="docutils"><span class="calibre4">learning_rate='optimal'</span></code>）由下式给出。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000413.jpg" alt="\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000180.jpg" alt="t" /> 是时间步长（总共有 <cite class="calibre13">n_samples * n_iter</cite> 时间步长）， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000018.jpg" alt="t_0" /> 是由 Léon Bottou 提出的启发式算法决定的，比如预期的初始更新可以设定为权重的期望大小（假设训练样本的范数近似1）。在 <code class="docutils"><span class="calibre4">BaseSGD</span></code> 中的 <code class="docutils"><span class="calibre4">_init_t</span></code> 中可以找到确切的定义。</p>
<p class="calibre10">对于回归来说，默认的学习率是反向缩放 (<code class="docutils"><span class="calibre4">learning_rate='invscaling'</span></code>)，由下式给出</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000541.jpg" alt="\eta^{(t)} = \frac{eta_0}{t^{power\_t}}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000179.jpg" alt="eta_0" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000680.jpg" alt="power\_t" /> 是用户通过 <code class="docutils"><span class="calibre4">eta0</span></code> 和 <code class="docutils"><span class="calibre4">power_t</span></code> 分别选择的超参数。</p>
<p class="calibre10">使用固定的学习速率则设置 <code class="docutils"><span class="calibre4">learning_rate='constant'</span></code> ，或者设置 <code class="docutils"><span class="calibre4">eta0</span></code> 来指定学习速率。</p>
<p class="calibre10">模型参数可以通过成员 <code class="docutils"><span class="calibre4">coef_</span></code> 和 <code class="docutils"><span class="calibre4">intercept_</span></code> 来获得：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">成员 <code class="docutils"><span class="calibre4">coef_</span></code> holds the weights（控制权重） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /></li>
<li class="toctree-l">成员 <code class="docutils"><span class="calibre4">intercept_</span></code> holds <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000303.jpg" alt="b" /></li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10">参考文献：</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377">“Solving large scale linear prediction problems using stochastic
gradient descent algorithms”</a>
T. Zhang - In Proceedings of ICML ‘04.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696">“Regularization and variable selection via the elastic net”</a>
H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,
67 (2), 301-320.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://arxiv.org/pdf/1107.2490v2.pdf">“Towards Optimal One Pass Large Scale Learning with
Averaged Stochastic Gradient Descent”</a>
Xu, Wei</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-358">
<h2 class="sigil_not_in_toc">1.5.7. 实现细节</h2>
<p class="calibre2">SGD 的实现受到了 Léon Bottou <a class="calibre3 pcalibre" href="http://leon.bottou.org/projects/sgd">Stochastic Gradient SVM</a>  的影响。类似于 SvmSGD，权重向量表达为一个标量和一个向量的内积，这样保证在使用L2正则项时可以高效更新权重。
在 sparse feature vectors （稀疏特征向量）的情况下， intercept （截距）是以更小的学习率（乘以 0.01）更新的，这导致了它的更新更加频繁。训练样本按顺序选取并且每处理一个样本就要降低学习速率。我们采用了 Shalev-Shwartz 等人2007年提出的的学习速率设定方案。
对于多类分类，我们使用了 “one versus all” 方法。
我们在 L1 正则化（和 Elastic Net ）中使用 Tsuruoka 等人2009年提出的 truncated gradient algorithm （截断梯度算法）。代码是用 Cython 编写的。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://leon.bottou.org/slides/largescale/lstut.pdf">“The Tradeoffs of Large Scale Machine Learning”</a> L. Bottou - Website, 2011.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513">“Pegasos: Primal estimated sub-gradient solver for svm”</a>
S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.aclweb.org/anthology/P/P09/P09-1054.pdf">“Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty”</a>
Y. Tsuruoka, J. Tsujii, S. Ananiadou -  In Proceedings of the AFNLP/ACL ‘09.</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-147">
<span id="calibre_link-359" class="calibre4"></span><h1 class="calibre5">1.6. 最近邻</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/caopeirui" class="calibre3 pcalibre">@Veyron C</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/pan8664716" class="calibre3 pcalibre">@舞空</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/wangyangting" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" /> 
    </div>
<p class="calibre10"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="docutils"><span class="calibre4">sklearn.neighbors</span></code></a> 提供了 neighbors-based (基于邻居的) 无监督学习以及监督学习方法的功能。
无监督的最近邻是许多其它学习方法的基础，尤其是 manifold learning (流行学习) 和 spectral clustering (谱聚类)。
受监督的 neighbors-based (基于邻居的) 学习分为两种： <a class="calibre3 pcalibre" href="#calibre_link-148">classification</a> （分类）针对的是具有离散标签的数据，<a class="calibre3 pcalibre" href="#calibre_link-149">regression</a> （回归）针对的是具有连续标签的数据。</p>
<p class="calibre10">最近邻方法的原理是从训练样本中找到与新点在距离上最近的预定数量的几个点，并从这些点中预测标签。
这些点的数量可以是用户自定义的常量（K-最近邻学习），
页可以根据不同的点的局部密度（基于半径的最近邻学习）。距离通常可以通过任何方式来度量：
standard Euclidean distance（标准欧式距离）是最常见的选择。Neighbors-based（基于邻居的）方法被称为 <em class="calibre13">非泛化</em> 机器学习方法，
因为它们只是简单地”记住”了其所有的训练数据（可能转换为一个快速索引结构，如 <a class="calibre3 pcalibre" href="#calibre_link-150"><span class="calibre4">Ball Tree</span></a> 或 <a class="calibre3 pcalibre" href="#calibre_link-151"><span class="calibre4">KD Tree</span></a>）。</p>
<p class="calibre10">尽管它很简单，但最近邻算法已经成功地适用于很多的分类和回归问题，例如手写数字或卫星图像的场景。
作为一个 non-parametric（非参数化）方法，它经常成功地应用于决策边界非常不规则的情景下。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="docutils"><span class="calibre4">sklearn.neighbors</span></code></a> 可以处理 Numpy 数组或 <cite class="calibre13">scipy.sparse</cite> 矩阵作为其输入。
对于密集矩阵，大多数可能距离的矩阵都是支持的。对于稀疏矩阵，支持搜索任意的 Minkowski 度量。</p>
<p class="calibre10">许多学习方法都是依赖最近邻作为核心。
一个例子是 <a class="calibre3 pcalibre" href="density.html#kernel-density"><span class="calibre4">核密度估计</span></a> ,
在 <a class="calibre3 pcalibre" href="density.html#density-estimation"><span class="calibre4">密度估计</span></a> 章节中有更深入的讨论。</p>
<div class="toctree-wrapper" id="calibre_link-360">
<span id="calibre_link-361" class="calibre4"></span><h2 class="sigil_not_in_toc">1.6.1. 无监督最近邻</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="docutils"><span class="calibre4">NearestNeighbors</span></code></a> （最近邻）实现了 unsupervised nearest neighbors learning（无监督的最近邻学习）。
它为三种不同的最近邻算法提供统一的接口：<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a>, 还有基于 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a>
的 brute-force 算法。选择算法时可通过关键字 <code class="docutils"><span class="calibre4">'algorithm'</span></code> 来控制，
并必须是 <code class="docutils"><span class="calibre4">['auto',</span> <span class="calibre4">'ball_tree',</span> <span class="calibre4">'kd_tree',</span> <span class="calibre4">'brute']</span></code> 其中的一个。当默认值设置为 <code class="docutils"><span class="calibre4">'auto'</span></code>
时，算法会尝试从训练数据中确定最佳方法。有关上述每个选项的优缺点，参见 <a href="#calibre_link-152" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-362">`Nearest Neighbor Algorithms`_</span></a> 。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">关于最近邻算法，如果邻居 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000643.jpg" alt="k+1" /> 和邻居 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 具有相同的距离，但具有不同的标签，
结果将取决于训练数据的顺序。</p>
</div>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-363">
<h3 class="sigil_not_in_toc1">1.6.1.1. 找到最近邻</h3>
<p class="calibre2">为了完成找到两组数据集中最近邻点的简单任务, 可以使用 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="docutils"><span class="calibre4">sklearn.neighbors</span></code></a> 中的无监督算法:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors</span> <span class="calibre4">import</span> <span class="calibre4">NearestNeighbors</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">nbrs</span> <span class="calibre4">=</span> <span class="calibre4">NearestNeighbors</span><span class="calibre4">(</span><span class="calibre4">n_neighbors</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">algorithm</span><span class="calibre4">=</span><span class="calibre4">'ball_tree'</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">distances</span><span class="calibre4">,</span> <span class="calibre4">indices</span> <span class="calibre4">=</span> <span class="calibre4">nbrs</span><span class="calibre4">.</span><span class="calibre4">kneighbors</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">indices</span>                                           
<span class="calibre4">array([[0, 1],</span>
<span class="calibre4">       [1, 0],</span>
<span class="calibre4">       [2, 1],</span>
<span class="calibre4">       [3, 4],</span>
<span class="calibre4">       [4, 3],</span>
<span class="calibre4">       [5, 4]]...)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">distances</span>
<span class="calibre4">array([[ 0.        ,  1.        ],</span>
<span class="calibre4">       [ 0.        ,  1.        ],</span>
<span class="calibre4">       [ 0.        ,  1.41421356],</span>
<span class="calibre4">       [ 0.        ,  1.        ],</span>
<span class="calibre4">       [ 0.        ,  1.        ],</span>
<span class="calibre4">       [ 0.        ,  1.41421356]])</span>
</pre>
</div>
</div>
<p class="calibre10">因为查询集匹配训练集，每个点的最近邻点是其自身，距离为0。</p>
<p class="calibre10">还可以有效地生成一个稀疏图来标识相连点之间的连接情况：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">nbrs</span><span class="calibre4">.</span><span class="calibre4">kneighbors_graph</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[ 1.,  1.,  0.,  0.,  0.,  0.],</span>
<span class="calibre4">       [ 1.,  1.,  0.,  0.,  0.,  0.],</span>
<span class="calibre4">       [ 0.,  1.,  1.,  0.,  0.,  0.],</span>
<span class="calibre4">       [ 0.,  0.,  0.,  1.,  1.,  0.],</span>
<span class="calibre4">       [ 0.,  0.,  0.,  1.,  1.,  0.],</span>
<span class="calibre4">       [ 0.,  0.,  0.,  0.,  1.,  1.]])</span>
</pre>
</div>
</div>
<p class="calibre10">我们的数据集是结构化的，因此附近索引顺序的点就在参数空间附近，从而生成了近似 K-nearest neighbors（K-近邻）的块对角矩阵。
这种稀疏图在各种情况下都很有用，它利用点之间的空间关系进行无监督学习：特别地可参见 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="docutils"><span class="calibre4">sklearn.manifold.Isomap</span></code></a>,
<a class="calibre3 pcalibre" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="docutils"><span class="calibre4">sklearn.manifold.LocallyLinearEmbedding</span></code></a>, 和 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="docutils"><span class="calibre4">sklearn.cluster.SpectralClustering</span></code></a>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-364">
<h3 class="sigil_not_in_toc1">1.6.1.2. KDTree 和 BallTree 类</h3>
<p class="calibre2">我们可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 或 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 其中一个类来找最近邻。
这是上文使用过的 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="docutils"><span class="calibre4">NearestNeighbors</span></code></a> 类所包含的功能。
<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 具有相同的接口；
我们将在这里展示使用 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 的例子：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors</span> <span class="calibre4">import</span> <span class="calibre4">KDTree</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kdt</span> <span class="calibre4">=</span> <span class="calibre4">KDTree</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">leaf_size</span><span class="calibre4">=</span><span class="calibre4">30</span><span class="calibre4">,</span> <span class="calibre4">metric</span><span class="calibre4">=</span><span class="calibre4">'euclidean'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kdt</span><span class="calibre4">.</span><span class="calibre4">query</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">k</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">return_distance</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>          
<span class="calibre4">array([[0, 1],</span>
<span class="calibre4">       [1, 0],</span>
<span class="calibre4">       [2, 1],</span>
<span class="calibre4">       [3, 4],</span>
<span class="calibre4">       [4, 3],</span>
<span class="calibre4">       [5, 4]]...)</span>
</pre>
</div>
</div>
<p class="calibre10">对于近邻搜索中选项的更多信息，包括各种度量距离的查询策略的说明等，请参阅 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 类文档。
关于可用度量距离的列表，请参阅 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="docutils"><span class="calibre4">DistanceMetric</span></code></a> 类。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-148">
<span id="calibre_link-365" class="calibre4"></span><h2 class="sigil_not_in_toc">1.6.2. 最近邻分类</h2>
<p class="calibre2">最近邻分类属于基于实例的学习或非泛化学习：它不会去构造一个泛化的内部模型，而是简单地存储训练数据的实例。
分类是由每个点的最近邻的简单多数投票中计算得到的：一个查询点的数据类型是由它最近邻点中最具代表性的数据类型来决定的。</p>
<dl class="calibre10">
<dt class="calibre18">scikit-learn 实现了两种不同的最近邻分类器：<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="docutils"><span class="calibre4">KNeighborsClassifier</span></code></a> 基于每个查询点的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个最近邻实现，</dt>
<dd class="calibre19">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 是用户指定的整数值。<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="docutils"><span class="calibre4">RadiusNeighborsClassifier</span></code></a> 基于每个查询点的固定半径 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 内的邻居数量实现，
其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 是用户指定的浮点数值。</dd>
<dt class="calibre18"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> -邻居分类是 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="docutils"><span class="calibre4">KNeighborsClassifier</span></code></a> 下的两种技术中比较常用的一种。<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 值的最佳选择是高度数据依赖的：</dt>
<dd class="calibre19"><p class="first">通常较大的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 是会抑制噪声的影响，但是使得分类界限不明显。</p>
<p class="last">如果数据是不均匀采样的，那么 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="docutils"><span class="calibre4">RadiusNeighborsClassifier</span></code></a> 中的基于半径的近邻分类可能是更好的选择。</p>
</dd>
<dt class="calibre18">用户指定一个固定半径 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" />，使得稀疏邻居中的点使用较少的最近邻来分类。</dt>
<dd class="calibre19"><p class="first">对于高维参数空间，这个方法会由于所谓的 “维度灾难” 而变得不那么有效。</p>
<p class="last">基本的最近邻分类使用统一的权重：分配给查询点的值是从最近邻的简单多数投票中计算出来的。
在某些环境下，最好对邻居进行加权，使得近邻更有利于拟合。可以通过 <code class="docutils"><span class="calibre4">weights</span></code> 关键字来实现。</p>
</dd>
</dl>
<p class="calibre10">默认值 <code class="docutils"><span class="calibre4">weights</span> <span class="calibre4">=</span> <span class="calibre4">'uniform'</span></code> 为每个近邻分配统一的权重。而 <code class="docutils"><span class="calibre4">weights</span> <span class="calibre4">=</span> <span class="calibre4">'distance'</span></code> 分配权重与查询点的距离成反比。
或者，用户可以自定义一个距离函数用来计算权重。</p>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1">../auto_examples/neighbors/plot_classification.html</td>
</tr>
<tr class="row-odd"><th class="head">scale:</th>
<td class="label1">50</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1">../auto_examples/neighbors/plot_classification.html</td>
</tr>
<tr class="row-odd"><th class="head">scale:</th>
<td class="label1">50</td>
</tr>
</tbody>
</table>
<p class="calibre10">
<strong class="calibre14"><img alt="classification_1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000426.jpg" class="math" /> <img alt="classification_2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000731.jpg" class="math" /></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="calibre4">Nearest Neighbors Classification</span></a>: 使用最近邻进行分类的示例。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-149">
<span id="calibre_link-366" class="calibre4"></span><h2 class="sigil_not_in_toc">1.6.3. 最近邻回归</h2>
<p class="calibre2">最近邻回归是用在数据标签为连续变量，而不是离散变量的情况下。分配给查询点的标签是由它的最近邻标签的均值计算而来的。</p>
<p class="calibre10">scikit-learn 实现了两种不同的最近邻回归：<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code class="docutils"><span class="calibre4">KNeighborsRegressor</span></code></a> 基于每个查询点的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个最近邻实现，
其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 是用户指定的整数值。<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code class="docutils"><span class="calibre4">RadiusNeighborsRegressor</span></code></a> 基于每个查询点的固定半径 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 内的邻居数量实现，
其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 是用户指定的浮点数值。</p>
<p class="calibre10">基本的最近邻回归使用统一的权重：即，本地领域内的每个邻居点对查询
点的分类贡献相当。
在某些环境下，对节点加权可能是有利的，使得附近点对于回归所作出的贡献多于远处点。
这可以通过 <code class="docutils"><span class="calibre4">weights</span></code> 关键字来实现。默认值 <code class="docutils"><span class="calibre4">weights</span> <span class="calibre4">=</span> <span class="calibre4">'uniform'</span></code> 为所有点分配同等权重。
而 <code class="docutils"><span class="calibre4">weights</span> <span class="calibre4">=</span> <span class="calibre4">'distance'</span></code> 分配的权重与查询点距离呈反比。
或者，用户可以自定义一个距离函数用来计算权重。</p>
<div class="toctree-wrapper">
<img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000685.jpg" class="math" />
</div>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1"><p class="calibre10">../auto_examples/neighbors/plot_regression.html
:align: center</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">scale:</th>
<td class="label1">75</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="calibre10">使用多输出的最近邻进行回归分析 <a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="calibre4">Face completion with a multi-output estimators</span></a>。</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre10">在这个示例中，输入 X 是脸上半部分像素，输出 Y 是脸下半部分像素。</p>
<div class="toctree-wrapper">

</div>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1"><p class="calibre10">../auto_examples/plot_multioutput_face_completion.html
:scale: 75</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">align:</th>
<td class="label1">center</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
</td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"><span class="calibre4">Nearest Neighbors regression</span></a>: 使用最近邻进行回归的示例。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="calibre4">Face completion with a multi-output estimators</span></a>: 使用最近邻进行多输出回归的示例。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-367">
<h2 class="sigil_not_in_toc">1.6.4. 最近邻算法</h2>
<div class="toctree-wrapper" id="calibre_link-368">
<span id="calibre_link-369" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.6.4.1. 暴力计算</h3>
<p class="calibre2">最近邻的快速计算是机器学习中一个活跃的研究领域。最简单的近邻搜索涉及数据集中所有成对点之间距离的暴力计算：
对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 维度中的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 个样本来说, 这个方法的复杂度是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000358.jpg" alt="O[D N^2]" />。
对于小数据样本，高效的暴力近邻搜索是非常有竞争力的。
然而，随着样本数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 的增长，暴力方法很快变得不行了。在 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="docutils"><span class="calibre4">sklearn.neighbors</span></code></a> 类中，
暴力近邻搜索通过关键字 <code class="docutils"><span class="calibre4">algorithm</span> <span class="calibre4">=</span> <span class="calibre4">'brute'</span></code> 来指定，并通过 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a> 中的例程来进行计算。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-370">
<span id="calibre_link-151" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.6.4.2. K-D 树</h3>
<p class="calibre2">为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说，
这些结构试图通过有效地编码样本的 aggregate distance (聚合距离) 信息来减少所需的距离计算量。
基本思想是，若 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 点距离 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000783.jpg" alt="B" /> 点非常远，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000783.jpg" alt="B" /> 点距离 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 点非常近，
可知 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 点与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 点很遥远，<em class="calibre13">不需要明确计算它们的距离</em>。
通过这样的方式，近邻搜索的计算成本可以降低为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000711.jpg" alt="O[D N \log(N)]" /> 或更低。
这是对于暴力搜索在大样本数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 中表现的显著改善。</p>
<p class="calibre10">利用这种聚合信息的早期方法是 <em class="calibre13">KD tree</em> 数据结构（* K-dimensional tree* 的简写）,
它将二维 <em class="calibre13">Quad-trees</em> 和三维 <a href="#calibre_link-153" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-371">*</span></a>Oct-trees 推广到任意数量的维度.
KD 树是一个二叉树结构，它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。
KD 树的构造非常快：因为只能沿数据轴执行分区, 无需计算 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" />-dimensional 距离。
一旦构建完成, 查询点的最近邻距离计算复杂度仅为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000611.jpg" alt="O[\log(N)]" /> 。
虽然 KD 树的方法对于低维度 (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000705.jpg" alt="D &lt; 20" />) 近邻搜索非常快, 当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 增长到很大时,
效率变低: 这就是所谓的 “维度灾难” 的一种体现。
在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code class="docutils"><span class="calibre4">algorithm</span> <span class="calibre4">=</span> <span class="calibre4">'kd_tree'</span></code> 来指定,
并且使用类 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 来计算。</p>
<div class="toctree-wrapper">
<p class="calibre10">References:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://dl.acm.org/citation.cfm?doid=361002.361007">“Multidimensional binary search trees used for associative searching”</a>,
Bentley, J.L., Communications of the ACM (1975)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-372">
<span id="calibre_link-150" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.6.4.3. Ball 树</h3>
<p class="calibre2">为了解决 KD 树在高维上效率低下的问题, 开发了 <em class="calibre13">ball 树</em> 数据结构.
其中 KD 树沿笛卡尔轴分割数据, ball 树在沿着一系列的 hyper-spheres 来分割数据.
通过这种方法构建的树要比 KD 树消耗更多的时间,
但是这种数据结构对于高结构化的数据是非常有效的, 即使在高纬度上也是一样.</p>
<dl class="calibre10">
<dt class="calibre18">ball 树将数据递归地划分为由质心 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 和半径 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 定义的节点,</dt>
<dd class="calibre19">使得节点中的每个点位于由 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 定义的 hyper-sphere 内.
通过使用 <em class="calibre13">triangle inequality（三角不等式）</em> 减少近邻搜索的候选点数:</dd>
</dl>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000085.jpg" alt="|x+y| \leq |x| + |y|" class="math" /></p>
</div>
<p class="calibre10">通过这种设置, 测试点和质心之间的单一距离计算足以确定距节点内所有点的距离的下限和上限.
由于 ball 树节点的球形几何, 它可以在高维度上执行 <em class="calibre13">KD-tree</em>, 尽管实际的性能高度依赖于训练数据的结构.
在 scikit-learn 中, 基于 ball 树的近邻搜索可以使用关键字 <code class="docutils"><span class="calibre4">algorithm</span> <span class="calibre4">=</span> <span class="calibre4">'ball_tree'</span></code> 来指定,
并且使用类 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">sklearn.neighbors.BallTree</span></code></a> 来计算.
或者, 用户可以直接使用 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 类.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209">“Five balltree construction algorithms”</a>,
Omohundro, S.M., International Computer Science Institute
Technical Report (1989)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-373">
<h3 class="sigil_not_in_toc1">1.6.4.4. 最近邻算法的选择</h3>
<p class="calibre2">对于给定数据集的最优算法是一个复杂的选择, 并且取决于多个因素:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first">样本数量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> (i.e. <code class="docutils"><span class="calibre4">n_samples</span></code>) 和维度 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> (例如. <code class="docutils"><span class="calibre4">n_features</span></code>).</p>
<ul class="calibre7">
<li class="toctree-l"><p class="first"><em class="calibre13">Brute force</em> 查询时间以 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000876.jpg" alt="O[D N]" /> 增长</p>
</li>
<li class="toctree-l"><p class="first"><em class="calibre13">Ball tree</em> 查询时间大约以 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000077.jpg" alt="O[D \log(N)]" /> 增长</p>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18"><em class="calibre13">KD tree</em> 的查询时间 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 的变化是很难精确描述的.</dt>
<dd class="calibre19"><p class="toctree-wrapper">对于较小的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> (小于20) 的成本大约是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000665.jpg" alt="O[D\log(N)]" />, 并且 KD 树更加有效.</p>
</dd>
</dl>
<p class="calibre10">对于较大的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 成本的增加接近 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000830.jpg" alt="O[DN]" />,
由于树结构引起的开销会导致查询效率比暴力还要低.</p>
</li>
</ul>
<dl class="calibre10">
<dt class="calibre18">对于小数据集 (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 小于30), <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000547.jpg" alt="\log(N)" /> 相当于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" />, 暴力算法比基于树的算法更加有效.</dt>
<dd class="calibre19"><p class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="docutils"><span class="calibre4">KDTree</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 通过提供一个 <em class="calibre13">leaf size</em> 参数来解决这个问题:</p>
</dd>
</dl>
<p class="calibre10">这控制了查询切换到暴力计算样本数量. 使得两种算法的效率都能接近于对较小的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 的暴力计算的效率.</p>
</li>
<li class="toctree-l"><p class="first">数据结构: 数据的 <em class="calibre13">intrinsic dimensionality</em> (本征维数) 和/或数据的 <em class="calibre13">sparsity</em> (稀疏度).
本征维数是指数据所在的流形的维数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000040.jpg" alt="d \le D" />, 在参数空间可以是线性或非线性的.
稀疏度指的是数据填充参数空间的程度(这与“稀疏”矩阵中使用的概念不同,
数据矩阵可能没有零项, 但是从这个意义上来讲,它的 <strong class="calibre14">structure</strong> 仍然是 “稀疏” 的)。</p>
<ul class="calibre7">
<li class="toctree-l"><em class="calibre13">Brute force</em> (暴力查询)时间不受数据结构的影响。</li>
<li class="toctree-l"><em class="calibre13">Ball tree</em> 和 <em class="calibre13">KD tree</em> 的数据结构对查询时间影响很大.
一般地, 小维度的 sparser (稀疏) 数据会使查询更快.
因为 KD 树的内部表现形式是与参数轴对齐的,
对于任意的结构化数据它通常不会表现的像 ball tree 那样好.</li>
</ul>
<p class="calibre10">在机器学习中往往使用的数据集是非常结构化的, 而且非常适合基于树结构的查询。</p>
</li>
<li class="toctree-l"><p class="first">请求 query point（查询点）的近邻数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre7">
<li class="toctree-l"><em class="calibre13">Brute force</em> 查询时间几乎不受 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 值的影响.</li>
<li class="toctree-l"><em class="calibre13">Ball tree</em> 和 <em class="calibre13">KD tree</em> 的查询时间会随着 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的增加而变慢.
这是由于两个影响: 首先, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的值越大在参数空间中搜索的部分就越大.
其次, 使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000825.jpg" alt="k &gt; 1" /> 进行树的遍历时, 需要对内部结果进行排序.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 与  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 相比变大时, 在基于树的查询中修剪树枝的能力是减弱的. 在这种情况下, 暴力查询会更加有效.</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre7">
<li class="toctree-l">query points（查询点）数.  ball tree 和 KD Tree 都需要一个构建阶段.
在许多查询中分摊时，这种结构的成本可以忽略不计。
如果只执行少量的查询, 可是构建成本却占总成本的很大一部分.
如果仅需查询很少的点, 暴力方法会比基于树的方法更好.</li>
</ul>
<p class="calibre10">一般地, <code class="docutils"><span class="calibre4">algorithm</span> <span class="calibre4">=</span> <span class="calibre4">'auto'</span></code> 选择 <code class="docutils"><span class="calibre4">'kd_tree'</span></code> 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000649.jpg" alt="k &lt; N/2" /></p>
</div>
</blockquote>
</li>
</ul>
<p class="calibre10">并且 <code class="docutils"><span class="calibre4">'effective_metric_'</span></code> 在 <code class="docutils"><span class="calibre4">'kd_tree'</span></code> 的列表 <code class="docutils"><span class="calibre4">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils"><span class="calibre4">'ball_tree'</span></code> 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000649.jpg" alt="k &lt; N/2" /> 并且
<code class="docutils"><span class="calibre4">'effective_metric_'</span></code> 在 <code class="docutils"><span class="calibre4">'ball_tree'</span></code> 的列表 <code class="docutils"><span class="calibre4">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils"><span class="calibre4">'brute'</span></code> 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000649.jpg" alt="k &lt; N/2" /> 并且
<code class="docutils"><span class="calibre4">'effective_metric_'</span></code> 不在 <code class="docutils"><span class="calibre4">'kd_tree'</span></code> 或 <code class="docutils"><span class="calibre4">'ball_tree'</span></code> 的列表 <code class="docutils"><span class="calibre4">'VALID_METRICS'</span></code> 中.
它选择 <code class="docutils"><span class="calibre4">'brute'</span></code> 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000605.jpg" alt="k &gt;= N/2" />.</p>
<p class="calibre10">这种选择基于以下假设: 查询点的数量与训练点的数量至少相同, 并且 <code class="docutils"><span class="calibre4">leaf_size</span></code> 接近其默认值 <code class="docutils"><span class="calibre4">30</span></code>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-374">
<h3 class="sigil_not_in_toc1">1.6.4.5. <code class="docutils2"><span class="calibre4">leaf_size</span></code> 的影响</h3>
<p class="calibre2">如上所述, 对于小样本暴力搜索是比基于数的搜索更有效的方法.
这一事实在 ball 树和 KD 树中被解释为在叶节点内部切换到蛮力搜索.
该开关的级别可以使用参数 <code class="docutils"><span class="calibre4">leaf_size</span></code> 来指定.
这个参数选择有很多的效果:</p>
<dl class="calibre10">
<dt class="calibre18"><strong class="calibre14">构造时间</strong></dt>
<dd class="calibre19">更大的 <code class="docutils"><span class="calibre4">leaf_size</span></code> 会导致更快的树构建时间, 因为需要创建更少的节点.</dd>
<dt class="calibre18"><strong class="calibre14">查询时间</strong></dt>
<dd class="calibre19">一个大或小的 <code class="docutils"><span class="calibre4">leaf_size</span></code> 可能会导致次优查询成本.
当 <code class="docutils"><span class="calibre4">leaf_size</span></code> 接近 1 时, 遍历节点所涉及的开销大大减慢了查询时间.
当 <code class="docutils"><span class="calibre4">leaf_size</span></code>, 接近训练集的大小，查询变得本质上是暴力的.
这些之间的一个很好的妥协是 <code class="docutils"><span class="calibre4">leaf_size</span> <span class="calibre4">=</span> <span class="calibre4">30</span></code>, 这是该参数的默认值.</dd>
</dl>
<p class="calibre10"><strong class="calibre14">内存</strong>
随着leaf_size的增加，存储树结构所需的内存减少。
对于存储每个节点的D维质心的ball tree，这点至关重要。
针对 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">BallTree</span></code></a> 所需的存储空间近似于 <code class="docutils"><span class="calibre4">1</span> <span class="calibre4">/</span> <span class="calibre4">leaf_size</span></code> 乘以训练集的大小.</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">leaf_size</span></code> 不被 brute force queries（暴力查询）所引用.</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-375">
<span id="calibre_link-376" class="calibre4"></span><h2 class="sigil_not_in_toc">1.6.5. 最近质心分类</h2>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="docutils"><span class="calibre4">NearestCentroid</span></code></a> 分类器是一个简单的算法, 通过其成员的质心来表示每个类。
实际上, 这使得它类似于 <code class="docutils"><span class="calibre4">sklearn.KMeans</span></code> 算法的标签更新阶段.
它也没有参数选择, 使其成为良好的基准分类器.
然而，它确实受到非凸类的影响，而且当类有显著不同的方差时，假设所有维度的方差都是相等的。
对于没有做出这个假设的更复杂的方法, 请参阅线性判别分析 (<a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="docutils"><span class="calibre4">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span></code></a>)
和二次判别分析 (<a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="docutils"><span class="calibre4">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</span></code></a>).
默认的 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="docutils"><span class="calibre4">NearestCentroid</span></code></a> 用法示例如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors.nearest_centroid</span> <span class="calibre4">import</span> <span class="calibre4">NearestCentroid</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">NearestCentroid</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">NearestCentroid(metric='euclidean', shrink_threshold=None)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">0.8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">]]))</span>
<span class="calibre4">[1]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-377">
<h3 class="sigil_not_in_toc1">1.6.5.1. 最近缩小质心</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="docutils"><span class="calibre4">NearestCentroid</span></code></a> 分类器有一个 <code class="docutils"><span class="calibre4">shrink_threshold</span></code> 参数,
它实现了 nearest shrunken centroid 分类器.
实际上, 每个质心的每个特征的值除以该特征的类中的方差.
然后通过 <code class="docutils"><span class="calibre4">shrink_threshold</span></code> 来减小特征值.
最值得注意的是, 如果特定特征值过0, 则将其设置为0.
实际上，每个质心的特征值，通过该特征类除以方差，再减去shrink_threshold得到。
这很有用, 例如, 去除噪声特征.</p>
<p class="calibre10">在以下例子中, 使用一个较小的 shrink 阀值将模型的准确度从 0.81 提高到 0.82.</p>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1">../auto_examples/neighbors/plot_nearest_centroid.html</td>
</tr>
<tr class="row-odd"><th class="head">scale:</th>
<td class="label1">50</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">target:</th>
<td class="label1">../auto_examples/neighbors/plot_nearest_centroid.html</td>
</tr>
<tr class="row-odd"><th class="head">scale:</th>
<td class="label1">50</td>
</tr>
</tbody>
</table>
<p class="calibre10">
<strong class="calibre14"><img alt="nearest_centroid_1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000590.jpg" class="math" /> <img alt="nearest_centroid_2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000275.jpg" class="math" /></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="calibre4">Nearest Centroid Classification</span></a>: 一个分类的例子, 它使用了不同 shrink 阀值的最近质心.</li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-154">
<span id="calibre_link-378" class="calibre4"></span><h1 class="calibre5">1.7. 高斯过程</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@glassy</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Trembleguy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@AI追寻者</a><br class="calibre9" />
    </div>
<p class="calibre10"><strong class="calibre14">高斯过程 (GP)</strong> 是一种常用的监督学习方法，旨在解决*回归问题*和*概率分类问题*。</p>
<p class="calibre10">高斯过程模型的优点如下：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><p class="first">预测内插了观察结果（至少对于正则核）。</p>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">预测结果是概率形式的（高斯形式的）。这样的话，</dt>
<dd class="calibre19"><p class="toctree-wrapper">人们可以计算得到经验置信区间并且据此来判断是否需要修改（在线拟合，自适应）</p>
</dd>
</dl>
<p class="calibre10">在一些区域的预测值。</p>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">通用性: 可以指定不同的:ref:<cite class="calibre13">内核(kernels)&lt;gp_kernels&gt;</cite>。</dt>
<dd class="calibre19"><p class="toctree-wrapper">虽然该函数提供了常用的内核，但是也可以指定自定义内核。</p>
</dd>
</dl>
</li>
</ul>
</div>
</blockquote>
<p class="calibre10">高斯过程模型的缺点包括：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">它们不稀疏，例如，模型通常使用整个样本/特征信息来进行预测。</li>
<li class="toctree-l">高维空间模型会失效，高维也就是指特征的数量超过几十个。</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-379">
<span id="calibre_link-380" class="calibre4"></span><h2 class="sigil_not_in_toc">1.7.1. 高斯过程回归（GPR）</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor" title="sklearn.gaussian_process.GaussianProcessRegressor"><code class="docutils"><span class="calibre4">GaussianProcessRegressor</span></code></a> 类实现了回归情况下的高斯过程(GP)模型。
为此，需要实现指定GP的先验。当参数 <code class="docutils"><span class="calibre4">normalize_y=False</span></code> 时，先验的均值
通常假定为常数或者零; 当 <code class="docutils"><span class="calibre4">normalize_y=True</span></code> 时，先验均值通常为训练数
据的均值。而先验的方差通过传递 <a class="calibre3 pcalibre" href="#calibre_link-155"><span class="calibre4">内核(kernel)</span></a> 对象来指定。通过
最大化基于传递 <code class="docutils"><span class="calibre4">optimizer</span></code> 的对数边缘似然估计(LML)，内核的超参可以在
GaussianProcessRegressor 类执行拟合过程中被优化。由于 LML 可能会存在多个
局部最优解，因此优化过程可以通过指定 <code class="docutils"><span class="calibre4">n_restarts_optimizer</span></code> 参数进行
多次重复。通过设置内核的超参初始值来进行第一次优化的运行。后续的运行
过程中超参值都是从合理范围值中随机选取的。如果需要保持初始化超参值，
那么需要把优化器设置为 <cite class="calibre13">None</cite> 。</p>
<p class="calibre10">目标变量中的噪声级别通过参数 <code class="docutils"><span class="calibre4">alpha</span></code> 来传递并指定，要么全局是常数要么是一个数据点。
请注意，适度的噪声水平也可以有助于处理拟合期间的数字问题，因为它被有效地实现为吉洪诺夫正则化(Tikhonov regularization)，
即通过将其添加到核心矩阵的对角线。明确指定噪声水平的替代方法是将 WhiteKernel 组件包含在内核中，
这可以从数据中估计全局噪声水平（见下面的示例）。</p>
<p class="calibre10">算法实现是基于 <a class="calibre3 pcalibre" href="#calibre_link-156" id="calibre_link-157">[RW2006]</a> 中的算法 2.1 。除了标准 scikit learn 估计器的 API 之外，
GaussianProcessRegressor 的作用还包括：</p>
<ul class="calibre6">
<li class="toctree-l">允许预测，无需事先拟合（基于GP先验）</li>
<li class="toctree-l">提供了一种额外的方法 <code class="docutils"><span class="calibre4">sample_y(X)</span></code> , 其评估
在给定输入处从 GPR （先验或后验）绘制的样本</li>
<li class="toctree-l">公开了一种方法 <code class="docutils"><span class="calibre4">log_marginal_likelihood(theta)</span></code> ,
可以在外部使用其他方式选择超参数，例如通过马尔科夫链蒙特卡罗链(Markov chain Monte Carlo)。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-381">
<h2 class="sigil_not_in_toc">1.7.2. GPR 示例</h2>
<div class="toctree-wrapper" id="calibre_link-382">
<h3 class="sigil_not_in_toc1">1.7.2.1. 具有噪声级的 GPR 估计</h3>
<p class="calibre2">该示例说明具有包含 WhiteKernel 的和核(sum-kernel)的 GPR 可以估计数据的噪声水平。
对数边缘似然（LML）景观的图示表明存在 LML 的两个局部最大值。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_noisy.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_noisy_0001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000329.jpg" class="math" /></a>
</div>
<p class="calibre10">第一个对应于具有高噪声电平和大长度尺度的模型，其解释数据中噪声的所有变化。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_noisy.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_noisy_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000648.jpg" class="math" /></a>
</div>
<p class="calibre10">第二个具有较小的噪声水平和较短的长度尺度，这解释了无噪声功能关系的大部分变化。
第二种模式有较高的可能性; 然而，根据超参数的初始值，基于梯度的优化也可能会收敛到高噪声解。
因此，对于不同的初始化，重复优化多次是很重要的。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_noisy.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_noisy_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000308.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-383">
<h3 class="sigil_not_in_toc1">1.7.2.2. GPR 和内核岭回归(Kernel Ridge Regression)的比较</h3>
<p class="calibre2">内核脊回归（KRR）和 GPR 通过内部使用 “kernel trick(内核技巧)” 来学习目标函数。
KRR学习由相应内核引起的空间中的线性函数，该空间对应于原始空间中的非线性函数。
基于平均误差损失与脊正弦化，选择内核空间中的线性函数。
GPR使用内核来定义先验分布在目标函数上的协方差，并使用观察到的训练数据来定义似然函数。
基于贝叶斯定理，定义了目标函数上的（高斯）后验分布，其平均值用于预测。</p>
<p class="calibre10">一个主要区别是，GPR 可以基于边际似然函数上的梯度上升选择内核的超参数，
而KRR需要在交叉验证的损失函数（均方误差损失）上执行网格搜索。
另一个区别是，GPR 学习目标函数的生成概率模型，因此可以提供有意义的置信区间和后验样本以及预测值，
而KRR仅提供预测。</p>
<p class="calibre10">下图说明了人造数据集上的两种方法，其中包括正弦目标函数和强噪声。
该图比较了基于 ExpSineSquared 内核的 KRR 和 GPR 的学习模型，适用于学习周期函数。
内核的超参数控制内核的平滑度（length_scale）和周期性（周期性）。
此外，数据的噪声水平由 GPR 通过内核中的另外的 WhiteKernel 组件和 KRR 的正则化参数 α 明确地学习。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_compare_gpr_krr.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_compare_gpr_krr_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000290.jpg" class="math" /></a>
</div>
<p class="calibre10">该图显示，两种方法都可以学习合理的目标函数模型。
GPR将函数的周期正确地识别为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000860.jpg" alt="2*\pi" /> （6.28），而 KRR 选择倍增的周期为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000215.jpg" alt="4*\pi" /> 。
此外，GPR 为 KRR 不可用的预测提供了合理的置信区间。
两种方法之间的主要区别是拟合和预测所需的时间：
原则上KRR的拟合速度较快，超参数优化的网格搜索与超参数（ “curse of dimensionality(维度诅咒)” ）呈指数级关系。
GPR中的参数的基于梯度的优化不受此指数缩放的影响，因此在具有三维超参数空间的该示例上相当快。
预测的时间是相似的; 然而，生成 GPR 预测分布的方差需要的时间比生成平均值要长。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-384">
<h3 class="sigil_not_in_toc1">1.7.2.3. Mauna Loa CO2 数据中的 GRR</h3>
<p class="calibre2">该示例基于 [RW2006] 的第 5.4.3 节。
它演示了使用梯度上升的对数边缘似然性的复杂内核工程和超参数优化的示例。
数据包括在 1958 年至 1997 年间夏威夷 Mauna Loa 天文台收集的每月平均大气二氧
化碳浓度（以百万分之几（ppmv）计）。目的是将二氧化碳浓度建模为时间t的函数。</p>
<p class="calibre10">内核由几个术语组成，负责说明信号的不同属性：</p>
<ul class="calibre6">
<li class="toctree-l">一个长期的，顺利的上升趋势是由一个 RBF 内核来解释的。
具有较大长度尺寸的RBF内核将使该分量平滑;
没有强制这种趋势正在上升，这给 GP 带来了这个选择。
具体的长度尺度和振幅是自由的超参数。</li>
<li class="toctree-l">季节性因素，由定期的 ExpSineSquared 内核解释，固定周期为1年。
该周期分量的长度尺度控制其平滑度是一个自由参数。
为了使准确周期性的衰减，采用带有RBF内核的产品。
该RBF组件的长度尺寸控制衰减时间，并且是另一个自由参数。</li>
<li class="toctree-l">较小的中期不规则性将由 RationalQuadratic 内核组件来解释，
RationalQuadratic 内核组件的长度尺度和 alpha 参数决定长度尺度的扩散性。
根据 [RW2006] ，这些不规则性可以更好地由 RationalQuadratic 来解释，
而不是 RBF 内核组件，这可能是因为它可以容纳几个长度尺度。</li>
<li class="toctree-l">“noise(噪声)” 一词，由一个 RBF 内核贡献组成，它将解释相关的噪声分量，</li>
</ul>
<blockquote class="calibre15">
<div class="toctree-wrapper">如局部天气现象以及 WhiteKernel 对白噪声的贡献。
相对幅度和RBF的长度尺度是进一步的自由参数。</div>
</blockquote>
<p class="calibre10">在减去目标平均值后最大化对数边际似然率产生下列内核，其中LML为-83.214:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">34.4</span><span class="calibre4">**</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">RBF</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">41.8</span><span class="calibre4">)</span>
<span class="calibre4">+</span> <span class="calibre4">3.27</span><span class="calibre4">**</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">RBF</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">180</span><span class="calibre4">)</span> <span class="calibre4">*</span> <span class="calibre4">ExpSineSquared</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">1.44</span><span class="calibre4">,</span>
                                                   <span class="calibre4">periodicity</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">+</span> <span class="calibre4">0.446</span><span class="calibre4">**</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">RationalQuadratic</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">17.7</span><span class="calibre4">,</span> <span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">0.957</span><span class="calibre4">)</span>
<span class="calibre4">+</span> <span class="calibre4">0.197</span><span class="calibre4">**</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">RBF</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">0.138</span><span class="calibre4">)</span> <span class="calibre4">+</span> <span class="calibre4">WhiteKernel</span><span class="calibre4">(</span><span class="calibre4">noise_level</span><span class="calibre4">=</span><span class="calibre4">0.0336</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">因此，大多数目标信号（34.4ppm）由长期上升趋势（长度为41.8年）解释。
周期分量的振幅为3.27ppm，衰减时间为180年，长度为1.44。
长时间的衰变时间表明我们在当地非常接近周期性的季节性成分。
相关噪声的幅度为0.197ppm，长度为0.138年，白噪声贡献为0.197ppm。
因此，整体噪声水平非常小，表明该模型可以很好地解释数据。
该图还显示，该模型直到2015年左右才能做出置信度比较高的预测</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_co2.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_co2_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000554.jpg" class="math" /></a>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-385">
<span id="calibre_link-386" class="calibre4"></span><h2 class="sigil_not_in_toc">1.7.3. 高斯过程分类（GPC）</h2>
<p class="calibre2">所述 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code class="docutils"><span class="calibre4">GaussianProcessClassifier</span></code></a> 器实现了用于分类目的的高斯过程（GP），当测试的预测采用类概率的形式，更能够用于概率分类。
GaussianProcessClassifier 在隐函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000557.jpg" alt="f" /> 之前设置GP先验，然后通过链接函数进行压缩以获得概率分类。
隐函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000557.jpg" alt="f" /> 因此就是所谓的干扰函数(nuisance function)，其值不能被观测到，并且自身不具有相关性。
其目的是允许模型的表达形式更加简便，并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000557.jpg" alt="f" /> 在预测过程中被去除（整合）。
GaussianProcessClassifier 实现了逻辑链接函数，
对于该逻辑，积分不能在分析上计算，但在二进制情况下很容易近似。</p>
<p class="calibre10">与回归设置相反，即使设置了高斯过程先验，隐函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000557.jpg" alt="f" /> 的后验也不符合高斯分布，
因为高斯似然不适用于离散类标签。相反，使用的是与逻辑链接函数（logit）对应的非高斯似然。
GaussianProcessClassifier 通过拉普拉斯近似(Laplace approximation)来估计非高斯后验分布。
更多详细信息，请参见 [RW2006] 的第 3 章。</p>
<p class="calibre10">GP先验平均值假定为零。先验的协方差是通过传递 <a class="calibre3 pcalibre" href="#calibre_link-155"><span class="calibre4">内核(kernel)</span></a> 对象来指定的。
在通过最大化基于传递的对数边缘似然（LML）的 GaussianProcessRegressor 拟合期间，
优化内核的超参数 <code class="docutils"><span class="calibre4">optimizer</span></code> 。由于LML可能具有多个局部最优值，
所以优化器可以通过指定重复启动 <code class="docutils"><span class="calibre4">n_restarts_optimizer</span></code> 。
第一次运行始终从内核的初始超参数值开始执行;
从已经从允许值的范围中随机选择超参数值来进行后续运行。
如果初始超参数需要保持固定，<cite class="calibre13">None</cite> 可以传递作为优化器。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code class="docutils"><span class="calibre4">GaussianProcessClassifier</span></code></a> 通过执行基于OvR(one-versus-rest)或
OvO(one-versus-one )策略的训练和预测来支持多类分类。
在OvR(one-versus-rest)策略中，每个类都配有一个二进制高斯过程分类器，该类别被训练为将该类与其余类分开。
在 “one_vs_one” 中，对于每对类拟合一个二进制高斯过程分类器，这被训练为分离这两个类。
这些二进制预测因子的预测被组合成多类预测。更多详细信息，请参阅 <a class="calibre3 pcalibre" href="multiclass.html#multiclass"><span class="calibre4">多类别分类</span></a> 。</p>
<p class="calibre10">在高斯过程分类的情况下，”one_vs_one” 策略可能在计算上更廉价，
因为它必须解决涉及整个训练集的每一个子集的许多问题，
而不是整个数据集的较少的问题。由于高斯过程分类与数据集的大小相互立方，这可能要快得多。
但是，请注意，”one_vs_one” 不支持预测概率估计，而只是简单的预测。
此外，请注意， <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code class="docutils"><span class="calibre4">GaussianProcessClassifier</span></code></a> 在内部还没有实现真正的多类 Laplace 近似，
但如上所述，在解决内部二进制分类任务的基础上，它们使用OvR或OvO的组合方法。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-387">
<h2 class="sigil_not_in_toc">1.7.4. GPC 示例</h2>
<div class="toctree-wrapper" id="calibre_link-388">
<h3 class="sigil_not_in_toc1">1.7.4.1. GPC 概率预测</h3>
<p class="calibre2">该示例说明了对于具有不同选项的超参数的RBF内核的GPC预测概率。
第一幅图显示GPC具有任意选择的超参数的预测概率，以及对应于最大LML（对数边缘似然）对应的超参数。</p>
<p class="calibre10">虽然通过优化LML选择的超参数具有相当大的LML，但是根据测试数据的对数损失，它们的表现更差。
该图显示，这是因为它们在阶级边界（这是好的）表现出类概率的急剧变化，
但预测概率接近0.5远离类边界（这是坏的）这种不良影响是由于GPC内部使用了拉普拉斯逼近。</p>
<p class="calibre10">第二幅图显示了内核超参数的不同选择的LML（对数边缘似然），突出了在第一幅图中使用的通过黑点（训练集）选择的两个超参数。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpc.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpc_0001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000655.jpg" class="math" /></a>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpc.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpc_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000452.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-389">
<h3 class="sigil_not_in_toc1">1.7.4.2. GPC 在 XOR 数据集上的举例说明</h3>
<p class="calibre2">此示例说明了在XOR数据上的GPC。各向同性的核（ <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code class="docutils"><span class="calibre4">RBF</span></code></a> ）和非固定的核（ <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code class="docutils"><span class="calibre4">DotProduct</span></code></a> ）对比固定性。
在这个特定的数据集上， <cite class="calibre13">DotProduct</cite> 内核获得了更好的结果，因为类边界是线性的，与坐标轴重合。
然而，实际上，诸如 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code class="docutils"><span class="calibre4">RBF</span></code></a> 这样的固定内核经常获得更好结果。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpc_xor.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpc_xor_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000887.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-390">
<h3 class="sigil_not_in_toc1">1.7.4.3. iris 数据集上的高斯过程分类（GPC）</h3>
<p class="calibre2">该示例说明了用于虹膜数据集的二维版本上各向同性和各向异性RBF核的GPC的预测概率。
这说明了GPC对多类分类的适用性。
各向异性RBF内核通过为两个特征维度分配不同的长度尺度来获得稍高的LML（对数边缘似然）。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpc_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpc_iris_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000775.jpg" class="math" /></a>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-155">
<span id="calibre_link-391" class="calibre4"></span><h2 class="sigil_not_in_toc">1.7.5. 高斯过程内核</h2>
<p class="calibre2">内核（也可以叫做GPs上下文中的”协方差函数”）
是决定高斯过程（GP）先验和后验形状的关键组成部分。
它们通过定义两个数据点的“相似性”，并结合相似的
数据点应该具有相似的目标值的假设，对所学习的函数进行编码。
内核可以分为两类：固定内核，只取决于两个数据点的距离，
不依赖于它们的绝对值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000740.jpg" alt="k(x_i, x_j)= k(d(x_i, x_j))" />
，因此它们对于输入空间中的转换是不变的；非固定的内核，取
决于数据点的具体值。固定内核可以进一步细分为各向同性和各向
异性内核，其中各向同性内核不会在输入空间中旋转。想要了解
更多细节，请参看 <a class="calibre3 pcalibre" href="#calibre_link-156" id="calibre_link-158">[RW2006]</a> 的第四章。</p>
<div class="toctree-wrapper" id="calibre_link-392">
<h3 class="sigil_not_in_toc1">1.7.5.1. 高斯过程内核 API</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code class="docutils"><span class="calibre4">Kernel</span></code></a> 主要是用来计算数据点之间的高斯过程协方差。
为此，内核中 <code class="docutils"><span class="calibre4">__call__</span></code> 方法会被调用。该方法可以用于计算
2d阵列X中所有数据点对的“自动协方差”，或二维阵列X的数据点
与二维阵列Y中的数据点的所有组合的“互协方差”。以下论断对于
所有内核k（除了 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel" title="sklearn.gaussian_process.kernels.WhiteKernel"><code class="docutils"><span class="calibre4">WhiteKernel</span></code></a>）都是成立的：<code class="docutils"><span class="calibre4">k(X)</span> <span class="calibre4">==</span> <span class="calibre4">K(X,</span> <span class="calibre4">Y=X)</span></code>。
如果仅仅是自协方差的对角线元素被使用，那么内核的方法 <code class="docutils"><span class="calibre4">diag()</span></code> 将会被调用，
该方法比等价的调用 <code class="docutils"><span class="calibre4">__call__</span></code>: <code class="docutils"><span class="calibre4">np.diag(k(X,</span> <span class="calibre4">X))</span> <span class="calibre4">==</span> <span class="calibre4">k.diag(X)</span></code>
具有更高的计算效率。</p>
<p class="calibre10">内核通过超参数向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000293.jpg" alt="\theta" /> 进行参数化。这些超参数可以
控制例如内核的长度或周期性（见下文）。通过设置 <code class="docutils"><span class="calibre4">__call__</span></code>
方法的参数 <code class="docutils"><span class="calibre4">eval_gradient=True</span></code> ，所有的内核支持计算解析
内核自协方差对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000293.jpg" alt="\theta" /> 的解析梯度。该梯度被用来在
高斯过程中（不论是回归型还是分类型的）计算LML（对数边缘似然）函数
的梯度，进而被用来通过梯度下降的方法极大化LML（对数边缘似然）函数
从而确定 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000293.jpg" alt="\theta" /> 的值。对于每个超参数，当对内核的实例
进行赋值时，初始值和边界值需要被指定。通过内核对象属性 <code class="docutils"><span class="calibre4">theta</span></code> ，
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000293.jpg" alt="\theta" /> 的当前值可以被获取或者设置。更重要的是，
超参的边界值可以被内核属性 <code class="docutils"><span class="calibre4">bounds</span></code> 获取。需要注意的是，
以上两种属性值(theta和bounds)都会返回内部使用值的日志转换值，
这是因为这两种属性值通常更适合基于梯度的优化。每个超参数的
规范 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Hyperparameter.html#sklearn.gaussian_process.kernels.Hyperparameter" title="sklearn.gaussian_process.kernels.Hyperparameter"><code class="docutils"><span class="calibre4">Hyperparameter</span></code></a> 以实例形式被存储在相应内核中。
请注意使用了以”x”命名的超参的内核必然具有self.x和self.x_bounds这两种属性。</p>
<p class="calibre10">所有内核的抽象基类为 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code class="docutils"><span class="calibre4">Kernel</span></code></a> 。Kernel 基类实现了
一个相似的接口 <code class="docutils"><span class="calibre4">Estimator</span></code> ，提供了方法 <code class="docutils"><span class="calibre4">get_params()</span></code> ,
<code class="docutils"><span class="calibre4">set_params()</span></code> 以及 <code class="docutils"><span class="calibre4">clone()</span></code> 。这也允许通过诸如
<code class="docutils"><span class="calibre4">Pipeline</span></code> 或者 <code class="docutils"><span class="calibre4">GridSearch</span></code> 之类的元估计来设置内核值。
需要注意的是，由于内核的嵌套结构（通过内核操作符，如下所见），
内核参数的名称可能会变得相对复杂些。通常来说，对于二元内核操作，
参数的左运算元以 <code class="docutils"><span class="calibre4">k1__</span></code> 为前缀，而右运算元以 <code class="docutils"><span class="calibre4">k2__</span></code> 为前缀。
一个额外的便利方法是 <code class="docutils"><span class="calibre4">clone_with_theta(theta)</span></code>，
该方法返回克隆版本的内核，但是设置超参数为 <code class="docutils"><span class="calibre4">theta</span></code>。
示例如下：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.gaussian_process.kernels</span> <span class="calibre4">import</span> <span class="calibre4">ConstantKernel</span><span class="calibre4">,</span> <span class="calibre4">RBF</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">ConstantKernel</span><span class="calibre4">(</span><span class="calibre4">constant_value</span><span class="calibre4">=</span><span class="calibre4">1.0</span><span class="calibre4">,</span> <span class="calibre4">constant_value_bounds</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">10.0</span><span class="calibre4">))</span> <span class="calibre4">*</span> <span class="calibre4">RBF</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">length_scale_bounds</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">10.0</span><span class="calibre4">))</span> <span class="calibre4">+</span> <span class="calibre4">RBF</span><span class="calibre4">(</span><span class="calibre4">length_scale</span><span class="calibre4">=</span><span class="calibre4">2.0</span><span class="calibre4">,</span> <span class="calibre4">length_scale_bounds</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">10.0</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">hyperparameter</span> <span class="calibre4">in</span> <span class="calibre4">kernel</span><span class="calibre4">.</span><span class="calibre4">hyperparameters</span><span class="calibre4">:</span> <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">hyperparameter</span><span class="calibre4">)</span>
<span class="calibre4">Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)</span>
<span class="calibre4">Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)</span>
<span class="calibre4">Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">params</span> <span class="calibre4">=</span> <span class="calibre4">kernel</span><span class="calibre4">.</span><span class="calibre4">get_params</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">key</span> <span class="calibre4">in</span> <span class="calibre4">sorted</span><span class="calibre4">(</span><span class="calibre4">params</span><span class="calibre4">):</span> <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> : </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">key</span><span class="calibre4">,</span> <span class="calibre4">params</span><span class="calibre4">[</span><span class="calibre4">key</span><span class="calibre4">]))</span>
<span class="calibre4">k1 : 1**2 * RBF(length_scale=0.5)</span>
<span class="calibre4">k1__k1 : 1**2</span>
<span class="calibre4">k1__k1__constant_value : 1.0</span>
<span class="calibre4">k1__k1__constant_value_bounds : (0.0, 10.0)</span>
<span class="calibre4">k1__k2 : RBF(length_scale=0.5)</span>
<span class="calibre4">k1__k2__length_scale : 0.5</span>
<span class="calibre4">k1__k2__length_scale_bounds : (0.0, 10.0)</span>
<span class="calibre4">k2 : RBF(length_scale=2)</span>
<span class="calibre4">k2__length_scale : 2.0</span>
<span class="calibre4">k2__length_scale_bounds : (0.0, 10.0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">.</span><span class="calibre4">theta</span><span class="calibre4">)</span>  <span class="calibre4"># Note: log-transformed</span>
<span class="calibre4">[ 0.         -0.69314718  0.69314718]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">.</span><span class="calibre4">bounds</span><span class="calibre4">)</span>  <span class="calibre4"># Note: log-transformed</span>
<span class="calibre4">[[       -inf  2.30258509]</span>
<span class="calibre4"> [       -inf  2.30258509]</span>
<span class="calibre4"> [       -inf  2.30258509]]</span>
</pre>
</div>
</div>
<p class="calibre10">所有的高斯过程内核操作都可以通过 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a> 来进行互操作，反之亦然。
<a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Kernel.html#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code class="docutils"><span class="calibre4">Kernel</span></code></a> 的子类实例可以通过 <code class="docutils"><span class="calibre4">metric</span></code> 参数传给 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a> 中的</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><code class="docutils"><span class="calibre4">pairwise_kernels</span></code> 。更重要的是，超参数的梯度不是分析的，而是数字，所有这些内核只支持</div>
</blockquote>
<p class="calibre10">各向同性距离。该参数 <code class="docutils"><span class="calibre4">gamma</span></code> 被认为是一个超参数，可以进行优化。其他内核参数在初始化时直接设置，
并保持固定。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-393">
<h3 class="sigil_not_in_toc1">1.7.5.2. 基础内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.ConstantKernel.html#sklearn.gaussian_process.kernels.ConstantKernel" title="sklearn.gaussian_process.kernels.ConstantKernel"><code class="docutils"><span class="calibre4">ConstantKernel</span></code></a> 内核类可以被用作 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product" title="sklearn.gaussian_process.kernels.Product"><code class="docutils"><span class="calibre4">Product</span></code></a> 内核类的一部分，
在它可以对其他因子（内核）进行度量的场景下或者作为更改高斯过程均值的</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum" title="sklearn.gaussian_process.kernels.Sum"><code class="docutils"><span class="calibre4">Sum</span></code></a> 类的一部分。这取决于参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000285.jpg" alt="constant\_value" /> 的设置。该方法定义为：</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000778.jpg" alt="k(x_i, x_j) = constant\_value \;\forall\; x_1, x_2" class="math" /></p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.WhiteKernel.html#sklearn.gaussian_process.kernels.WhiteKernel" title="sklearn.gaussian_process.kernels.WhiteKernel"><code class="docutils"><span class="calibre4">WhiteKernel</span></code></a> 内核类的主要应用实例在于当解释信号的噪声部分时
可以作为内核集合的一部分。通过调节参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000196.jpg" alt="noise\_level" />，
该类可以用来估计噪声级别。具体如下所示：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000123.jpg" alt="k(x_i, x_j) = noise\_level \text{ if } x_i == x_j \text{ else } 0" class="math" /></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-394">
<h3 class="sigil_not_in_toc1">1.7.5.3. 内核操作</h3>
<p class="calibre2">内核操作是把1~2个基内核与新内核进行合并。内核类 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Sum.html#sklearn.gaussian_process.kernels.Sum" title="sklearn.gaussian_process.kernels.Sum"><code class="docutils"><span class="calibre4">Sum</span></code></a> 通过
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000191.jpg" alt="k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)" /> 相加来合并 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000149.jpg" alt="k1" />
和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000606.jpg" alt="k2" /> 内核。内核类 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Product.html#sklearn.gaussian_process.kernels.Product" title="sklearn.gaussian_process.kernels.Product"><code class="docutils"><span class="calibre4">Product</span></code></a> 通过
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000738.jpg" alt="k_{product}(X, Y) = k1(X, Y) * k2(X, Y)" /> 把 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000149.jpg" alt="k1" />
和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000606.jpg" alt="k2" /> 内核进行合并。内核类 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Exponentiation.html#sklearn.gaussian_process.kernels.Exponentiation" title="sklearn.gaussian_process.kernels.Exponentiation"><code class="docutils"><span class="calibre4">Exponentiation</span></code></a> 通过
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000198.jpg" alt="k_{exp}(X, Y) = k(X, Y)^\text{exponent}" /> 把基内核与
常量参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000181.jpg" alt="exponent" /> 进行合并。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-395">
<h3 class="sigil_not_in_toc1">1.7.5.4. 径向基函数内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code class="docutils"><span class="calibre4">RBF</span></code></a> 内核是一个固定内核，它也被称为“平方指数”内核。它通过定长的参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000315.jpg" alt="l&gt;0" />
来对内核进行参数化。该参数既可以是标量（内核的各向同性变体）或者与输入 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> （内核的各向异性变体）
具有相同数量的维度的向量。该内核可以被定义为：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000746.jpg" alt="k(x_i, x_j) = \text{exp}\left(-\frac{1}{2} d(x_i / l, x_j / l)^2\right)" class="math" /></p>
</div>
<p class="calibre10">这个内核是无限可微的，这意味着这个内核作为协方差函数的 GP 具有所有阶数的均方差导数，
因此非常平滑。由RBF内核产生的GP的先验和后验示意图如下所示：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_prior_posterior.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_prior_posterior_0001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000267.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-396">
<h3 class="sigil_not_in_toc1">1.7.5.5. Matérn 内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.Matern.html#sklearn.gaussian_process.kernels.Matern" title="sklearn.gaussian_process.kernels.Matern"><code class="docutils"><span class="calibre4">Matern</span></code></a> 内核是一个固定内核，是 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code class="docutils"><span class="calibre4">RBF</span></code></a> 内核的泛化。它有一个额外的参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" />，
该参数控制结果函数的平滑程度。它由定长参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000315.jpg" alt="l&gt;0" /> 来实现参数化。该参数既可以是标量
（内核的各向同性变体）或者与输入 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> （内核的各向异性变体）具有相同数量的维度的向量。
该内核可以被定义为：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000311.jpg" alt="k(x_i, x_j) = \sigma^2\frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\gamma\sqrt{2\nu} d(x_i / l, x_j / l)\Bigg)^\nu K_\nu\Bigg(\gamma\sqrt{2\nu} d(x_i / l, x_j / l)\Bigg)," class="math" /></p>
</div>
<p class="calibre10">因为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000238.jpg" alt="\nu\rightarrow\infty" /> ，Matérn 内核收敛到 RBF 内核。
当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000320.jpg" alt="\nu = 1/2" /> 时，Matérn 内核变得与绝对指数内核相同时，即</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000025.jpg" alt="k(x_i, x_j) = \sigma^2 \exp \Bigg(-\gamma d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{1}{2}" class="math" /></p>
</div>
<p class="calibre10">特别的，当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000872.jpg" alt="\nu = 3/2" /> 时：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000630.jpg" alt="k(x_i, x_j) = \sigma^2 \Bigg(1 + \gamma \sqrt{3} d(x_i / l, x_j / l)\Bigg) \exp \Bigg(-\gamma \sqrt{3}d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{3}{2}" class="math" /></p>
</div>
<p class="calibre10">和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000599.jpg" alt="\nu = 5/2" /> :</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000323.jpg" alt="k(x_i, x_j) = \sigma^2 \Bigg(1 + \gamma \sqrt{5}d(x_i / l, x_j / l) +\frac{5}{3} \gamma^2d(x_i / l, x_j / l)^2 \Bigg) \exp \Bigg(-\gamma \sqrt{5}d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{5}{2}" class="math" /></p>
</div>
<p class="calibre10">是学习函数的常用选择，并且不是无限可微的（由 RBF 内核假定）
但是至少具有一阶( <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000872.jpg" alt="\nu = 3/2" /> )或者二阶( <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000599.jpg" alt="\nu = 5/2" /> )可微性。</p>
<p class="calibre10">通过 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" /> 灵活控制学习函数的平滑性可以更加适应真正的底层函数关联属性。
通过 Matérn 内核产生的高斯过程的先验和后验如下图所示：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_prior_posterior.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_prior_posterior_0041.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000506.jpg" class="math" /></a>
</div>
<p class="calibre10">想要更进一步地了解不同类型的Matérn内核请参阅 <a class="calibre3 pcalibre" href="#calibre_link-156" id="calibre_link-159">[RW2006]</a> , pp84。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-397">
<h3 class="sigil_not_in_toc1">1.7.5.6. 有理二次内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic" title="sklearn.gaussian_process.kernels.RationalQuadratic"><code class="docutils"><span class="calibre4">RationalQuadratic</span></code></a> 内核可以被看做不同特征尺度下的 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code class="docutils"><span class="calibre4">RBF</span></code></a> 内核的规模混合（一个无穷和）
它通过长度尺度参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000315.jpg" alt="l&gt;0" /> 和比例混合参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000199.jpg" alt="\alpha&gt;0" /> 进行参数化。
此时仅支持 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000896.jpg" alt="l" /> 标量的各向同性变量。内核公式如下：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000272.jpg" alt="k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}" class="math" /></p>
</div>
<p class="calibre10">从 RBF 内核中产生的高斯过程的先验和后验如下图所示：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_prior_posterior.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_prior_posterior_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000268.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-398">
<h3 class="sigil_not_in_toc1">1.7.5.7. 正弦平方内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared" title="sklearn.gaussian_process.kernels.ExpSineSquared"><code class="docutils"><span class="calibre4">ExpSineSquared</span></code></a> 内核可以对周期性函数进行建模。它由定长参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000315.jpg" alt="l&gt;0" />
以及周期参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000572.jpg" alt="p&gt;0" /> 来实现参数化。此时仅支持 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000896.jpg" alt="l" /> 标量的各向同性变量。内核公式如下：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000772.jpg" alt="k(x_i, x_j) = \text{exp}\left(-2 \left(\text{sin}(\pi / p * d(x_i, x_j)) / l\right)^2\right)" class="math" /></p>
</div>
<p class="calibre10">从ExpSineSquared内核中产生的高斯过程的先验和后验如下图所示：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_prior_posterior.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_prior_posterior_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000812.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-399">
<h3 class="sigil_not_in_toc1">1.7.5.8. 点乘内核</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code class="docutils"><span class="calibre4">DotProduct</span></code></a> 内核是非固定内核，它可以通过在线性回归的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000446.jpg" alt="x_d (d = 1, . . . , D)" /> 的相关系数上加上
服从于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000207.jpg" alt="N(0, 1)" /> 的先验以及在线性回归的偏置上加上服从于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000875.jpg" alt="N(0, \sigma_0^2)" /> 的先验来获得。
该 <a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code class="docutils"><span class="calibre4">DotProduct</span></code></a> 内核对于原点坐标的旋转是不变的，因此不是转换。它通过设置参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000820.jpg" alt="\sigma_0^2" /> 来进行参数化。
当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000666.jpg" alt="\sigma_0^2 = 0" /> 时，该内核叫做同质线性内核；否则该内核是非同质的。内核公式如下：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000509.jpg" alt="k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j" class="math" /></p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.kernels.DotProduct.html#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code class="docutils"><span class="calibre4">DotProduct</span></code></a> 内核通常和指数分布相结合。实例如下图所示：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/gaussian_process/plot_gpr_prior_posterior.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gpr_prior_posterior_0031.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000698.jpg" class="math" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-400">
<h3 class="sigil_not_in_toc1">1.7.5.9. 参考文献</h3>
<table class="docutils1" frame="void" id="calibre_link-156" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[RW2006]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-157">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-158">2</a>, <a class="calibre3 pcalibre" href="#calibre_link-159">3</a>)</em> Carl Eduard Rasmussen and Christopher K.I. Williams, “Gaussian Processes for Machine Learning”, MIT Press 2006, Link to an official complete PDF version of the book <a class="calibre3 pcalibre" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">here</a> .</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-401">
<h2 class="sigil_not_in_toc">1.7.6. 传统高斯过程</h2>
<p class="calibre2">在本节中，描述了版本 0.16.1 及之前 scikit 中高斯过程的实现，
请注意，此实现已被弃用，将在版本 0.18 中删除。</p>
<div class="toctree-wrapper" id="calibre_link-402">
<h3 class="sigil_not_in_toc1">1.7.6.1. 回归实例介绍</h3>
<p class="calibre2">假定我们要替代这个函数：<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000707.jpg" alt="g(x) = x \sin(x)" /> 。
为了做到这一点，该功能被评估到一个实验设计上。然后，
我们定义一个回归和相关模型可能被其他参数指定的高斯模型，
并且要求模型能够拟合数据。拟合过程由于受到实例化过程中
参数数目的影响可能依赖于参数的最大似然估计或者直接使用给定的参数。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">gaussian_process</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">f</span><span class="calibre4">(</span><span class="calibre4">x</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">return</span> <span class="calibre4">x</span> <span class="calibre4">*</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">sin</span><span class="calibre4">(</span><span class="calibre4">x</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">atleast_2d</span><span class="calibre4">([</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">3.</span><span class="calibre4">,</span> <span class="calibre4">5.</span><span class="calibre4">,</span> <span class="calibre4">6.</span><span class="calibre4">,</span> <span class="calibre4">7.</span><span class="calibre4">,</span> <span class="calibre4">8.</span><span class="calibre4">])</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">f</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">ravel</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">x</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">atleast_2d</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">linspace</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">1000</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gp</span> <span class="calibre4">=</span> <span class="calibre4">gaussian_process</span><span class="calibre4">.</span><span class="calibre4">GaussianProcess</span><span class="calibre4">(</span><span class="calibre4">theta0</span><span class="calibre4">=</span><span class="calibre4">1e-2</span><span class="calibre4">,</span> <span class="calibre4">thetaL</span><span class="calibre4">=</span><span class="calibre4">1e-4</span><span class="calibre4">,</span> <span class="calibre4">thetaU</span><span class="calibre4">=</span><span class="calibre4">1e-1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gp</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">GaussianProcess(beta0=None, corr=&lt;function squared_exponential at 0x...&gt;,</span>
<span class="calibre4">        normalize=True, nugget=array(2.22...-15),</span>
<span class="calibre4">        optimizer='fmin_cobyla', random_start=1, random_state=...</span>
<span class="calibre4">        regr=&lt;function constant at 0x...&gt;, storage_mode='full',</span>
<span class="calibre4">        theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),</span>
<span class="calibre4">        thetaU=array([[ 0.1]]), verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">sigma2_pred</span> <span class="calibre4">=</span> <span class="calibre4">gp</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">x</span><span class="calibre4">,</span> <span class="calibre4">eval_MSE</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-403">
<h3 class="sigil_not_in_toc1">1.7.6.2. 噪声数据拟合</h3>
<p class="calibre2">当含噪声的数据被用来做拟合时，对于每个数据点，高斯过程模型可以指定噪声的方差。
<a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcess.html#sklearn.gaussian_process.GaussianProcess" title="sklearn.gaussian_process.GaussianProcess"><code class="docutils"><span class="calibre4">GaussianProcess</span></code></a> 包含一个被添加到训练数据得到的自相关矩阵对角线中的
参数 <code class="docutils"><span class="calibre4">nugget</span></code> 。通常来说这是一种类型的吉洪诺夫正则化方法。
在平方指数相关函数的特殊情况下，该归一化等效于指定输入中的小数方差。也就是：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000650.jpg" alt="\mathrm{nugget}_i = \left[\frac{\sigma_i}{y_i}\right]^2" class="math" /></p>
</div>
<p class="calibre10">使用 <code class="docutils"><span class="calibre4">nugget</span></code> 以及 <code class="docutils"><span class="calibre4">corr</span></code> 正确设置，高斯过程可以更好地用于从噪声数据恢复给定的向量函数。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-404">
<h3 class="sigil_not_in_toc1">1.7.6.3. 数学形式</h3>
<div class="toctree-wrapper" id="calibre_link-405">
<h4 class="sigil_not_in_toc1">1.7.6.3.1. 初始假设</h4>
<p class="calibre2">假设需要对电脑实验的结果进行建模，例如使用一个数学函数：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000822.jpg" alt="g: &amp; \mathbb{R}^{n_{\rm features}} \rightarrow \mathbb{R} \\    &amp; X \mapsto y = g(X)" class="math" /></p>
</div>
<p class="calibre10">同时假设这个函数是 <em class="calibre13">一个</em> 有关于 <em class="calibre13">一个</em> 高斯过程 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000435.jpg" alt="G" /> 的条件采用方法，
那么从这个假设出发，GPML 通常可以表示为如下形式：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000730.jpg" alt="G(X) = f(X)^T \beta + Z(X)" class="math" /></p>
</div>
<p class="calibre10">其中， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000639.jpg" alt="f(X)^T \beta" /> 是一个线性回归模型，并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000453.jpg" alt="Z(X)" /> 是一个
以零为均值，协方差函数完全平稳的高斯过程：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000470.jpg" alt="C(X, X&apos;) = \sigma^2 R(|X - X&apos;|)" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000477.jpg" alt="\sigma^2" /> 表示其方差， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000120.jpg" alt="R" /> 表示仅仅基于样本之间的绝对相关距离的相关函数，可能是特征（这是平稳性假设）</p>
<p class="calibre10">从这些基本的公式中可以注意到GPML仅仅是基本的线性二乘回归问题的扩展。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000600.jpg" alt="g(X) \approx f(X)^T \beta" class="math" /></p>
</div>
<p class="calibre10">除此之外，我们另外假定由相关函数指定的样本之间的一些空间相干性（相关性）。
事实上，普通最小二乘法假设当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000493.jpg" alt="X = X&apos;" /> 时，相关性模型 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000287.jpg" alt="R(|X - X&apos;|)" />
是 1；否则为 0 ，相关性模型为 <em class="calibre13">狄拉克</em> 相关模型&ndash;有时在克里金文献中被称为 <em class="calibre13">熔核</em> 相关模型</p>
</div>
<div class="toctree-wrapper" id="calibre_link-406">
<h4 class="sigil_not_in_toc1">1.7.6.3.2. 最佳线性无偏预测（BLUP）</h4>
<p class="calibre2">我们现在推导出基于观测结果的 <em class="calibre13">最佳线性无偏预测</em> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000346.jpg" alt="g" /></p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000674.jpg" alt="\hat{G}(X) = G(X | y_1 = g(X_1), ...,                             y_{n_{\rm samples}} = g(X_{n_{\rm samples}}))" class="math" /></p>
</div>
<p class="calibre10">它可以由 <em class="calibre13">给定的属性</em> 加以派生：</p>
<ul class="calibre6">
<li class="toctree-l">它是线性的(观测结果的线性组合)</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000079.jpg" alt="\hat{G}(X) \equiv a(X)^T y" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">它是无偏的</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000610.jpg" alt="\mathbb{E}[G(X) - \hat{G}(X)] = 0" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">它是最好的（在均方误差的意义上）</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000279.jpg" alt="\hat{G}(X)^* = \arg \min\limits_{\hat{G}(X)} \;                                         \mathbb{E}[(G(X) - \hat{G}(X))^2]" class="math" /></p>
</div>
<p class="calibre10">因此最优的带权向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000101.jpg" alt="a(X)" /> 是以下等式约束优化问题的解</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000369.jpg" alt="a(X)^* = \arg \min\limits_{a(X)} &amp; \; \mathbb{E}[(G(X) - a(X)^T y)^2] \\                    {\rm s. t.} &amp; \; \mathbb{E}[G(X) - a(X)^T y] = 0" class="math" /></p>
</div>
<p class="calibre10">以拉格朗日形式重写这个受约束的优化问题，并进一步寻求要满足的一阶最优条件，
从而得到一个以闭合形式表达式为终止形式的预测器 - 参见参考文献中完整的证明。</p>
<p class="calibre10">最后，BLUP 为高斯随机变量，其中均值为：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000585.jpg" alt="\mu_{\hat{Y}}(X) = f(X)^T\,\hat{\beta} + r(X)^T\,\gamma" class="math" /></p>
</div>
<p class="calibre10">方差为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000575.jpg" alt="\sigma_{\hat{Y}}^2(X) = \sigma_{Y}^2\, ( 1 - r(X)^T\,R^{-1}\,r(X) + u(X)^T\,(F^T\,R^{-1}\,F)^{-1}\,u(X) )" class="math" /></p>
</div>
<p class="calibre10">其中:</p>
<ul class="calibre6">
<li class="toctree-l">根据自相关函数以及内置参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000293.jpg" alt="\theta" /> 所定义的相关性矩阵为：</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000497.jpg" alt="R_{i\,j} = R(|X_i - X_j|, \theta), \; i,\,j = 1, ..., m" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">在进行预测的点与 DOE 中的点之间的互相关的向量:</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000124.jpg" alt="r_i = R(|X - X_i|, \theta), \; i = 1, ..., m" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">回归矩阵 (例如范德蒙矩阵 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000557.jpg" alt="f" /> 以多项式为基):</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000121.jpg" alt="F_{i\,j} = f_i(X_j), \; i = 1, ..., p, \, j = 1, ..., m" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">广义最小二乘回归权重:</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000742.jpg" alt="\hat{\beta} =(F^T\,R^{-1}\,F)^{-1}\,F^T\,R^{-1}\,Y" class="math" /></p>
</div>
<ul class="calibre6">
<li class="toctree-l">和向量:</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000463.jpg" alt="\gamma &amp; = R^{-1}(Y - F\,\hat{\beta}) \\ u(X) &amp; = F^T\,R^{-1}\,r(X) - f(X)" class="math" /></p>
</div>
<p class="calibre10">需要重点注意的是，高斯过程预测器的概率输出是完全可分析的并且依赖于基本的线性代数操作。
更准确地说，预测结果的均值是两个简单线性组合（点积）的和，方差需要两个矩阵反转操作，但关联
矩阵只能使用 Cholesky 分解算法分解一次。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-407">
<h4 class="sigil_not_in_toc1">1.7.6.3.3. 经验最佳线性无偏估计（EBLUP）</h4>
<p class="calibre2">到现在为止，自相关和回归模型都已假定给出。然而，在实践中，它们从来都是未知的
因此需要为这些模型 <a class="calibre3 pcalibre" href="#calibre_link-160"><span class="calibre4">关联模型</span></a> 做出（积极的）经验选择。</p>
<p class="calibre10">根据这些选择，可以来估计 BLUP 中涉及到的遗留未知参数。为此，需要使用一系列
被提供的观测值同时结合一系列推断技术。目前使用的方法是基于 DACE’s Matlab 工
具包的*最大似然估计* - 参见 DACE 手册中的完全推导公式。最大似然估计的问题在
自相关参数中是一个全局优化的问题。这种全局优化通过 scipy.optimize 中的
fmin_cobyla 优化函数加以实现。然而，在各向异性的情况下，我们提供了
Welch 的分量优化算法的实现 - 参见参考。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-160">
<span id="calibre_link-408" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.7.6.4. 关联模型</h3>
<p class="calibre2">由于几乎相等的假设，常用的关联模型和一些有名的SVM内核相匹配。它们必须要满足 Mercer 条件
并且需要保持固定形式。然而，请注意，相关模型的选择应该与观察到来的原始实验的已知特性一致。
例如：</p>
<ul class="calibre6">
<li class="toctree-l">如果原始实验被认为是无限可微（平滑），则应使用 <em class="calibre13">平方指数关联模型</em> 。</li>
<li class="toctree-l">如果不是无限可微的, 那么需要使用 <em class="calibre13">指数关联模型</em>.</li>
<li class="toctree-l">还要注意，存在一个将衍生度作为输入的相关模型：这是 Matern 相关模型，但这里并没有实现（ TODO ）.</li>
</ul>
<p class="calibre10">关于选择合适的关联模型更详细的讨论，请参阅 Rasmussen＆Williams 的文献。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-409">
<span id="calibre_link-410" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.7.6.5. 回归模型</h3>
<p class="calibre2">常用的线性回归模型包括零阶（常数）、一阶和二阶多项式。
但是可以以 Python 函数的形式指定它自己的特性，它将特征X
作为输入，并返回一个包含函数集值的向量。唯一加以限制地是，
函数的个数不能超过有效观测值的数目，因此基本的回归问题不被*确定*。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-411">
<h3 class="sigil_not_in_toc1">1.7.6.6. 实现细节</h3>
<p class="calibre2">模型通过 DACE 的 Matlab 工具包来实现。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Lab/lab_maps/krigging/DACE-krigingsoft/dace/dace.pdf">DACE, A Matlab Kriging Toolbox</a> S Lophaven, HB Nielsen, J
Sondergaard 2002,</li>
<li class="toctree-l">W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell, and M.D.
Morris (1992). Screening, predicting, and computer experiments.
Technometrics, 34(1) 15&ndash;25.</li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-162">
<span id="calibre_link-412" class="calibre4"></span><h1 class="calibre5">1.8. 交叉分解</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@peels</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Counting stars</a><br class="calibre9" />    
    </div>
<p class="calibre10">交叉分解模块主要包含两个算法族: 偏最小二乘法（PLS）和典型相关分析（CCA）。</p>
<p class="calibre10">这些算法族具有发现两个多元数据集之间的线性关系的用途： <code class="docutils"><span class="calibre4">fit</span></code> method （拟合方法）的参数 <code class="docutils"><span class="calibre4">X</span></code> 和 <code class="docutils"><span class="calibre4">Y</span></code> 都是 2 维数组。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cross_decomposition/plot_compare_cross_decomposition.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_compare_cross_decomposition_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000514.jpg" class="calibre29" /></a>
</div>
<p class="calibre10">交叉分解算法能够找到两个矩阵 (X 和 Y) 的基础关系。它们是对在两个空间的
协方差结构进行建模的隐变量方法。它们将尝试在X空间中找到多维方向，该方向能
够解释Y空间中最大多维方差方向。PLS回归特别适用于当预测变量矩阵具有比观测值
更多的变量以及当X值存在多重共线性时。相比之下，在这些情况下，标准回归将失败。</p>
<p class="calibre10">包含在此模块中的类有：<a class="calibre3 pcalibre" href="generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression" title="sklearn.cross_decomposition.PLSRegression"><code class="docutils"><span class="calibre4">PLSRegression</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.cross_decomposition.PLSCanonical.html#sklearn.cross_decomposition.PLSCanonical" title="sklearn.cross_decomposition.PLSCanonical"><code class="docutils"><span class="calibre4">PLSCanonical</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.cross_decomposition.CCA.html#sklearn.cross_decomposition.CCA" title="sklearn.cross_decomposition.CCA"><code class="docutils"><span class="calibre4">CCA</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.cross_decomposition.PLSSVD.html#sklearn.cross_decomposition.PLSSVD" title="sklearn.cross_decomposition.PLSSVD"><code class="docutils"><span class="calibre4">PLSSVD</span></code></a></p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cross_decomposition/plot_compare_cross_decomposition.html#sphx-glr-auto-examples-cross-decomposition-plot-compare-cross-decomposition-py"><span class="calibre4">Compare cross decomposition methods</span></a></li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-163">
<span id="calibre_link-413" class="calibre4"></span><h1 class="calibre5">1.9. 朴素贝叶斯</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Kyrie</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@TWITCH</a><br class="calibre9" />  
    </div>
<p class="calibre10">朴素贝叶斯方法是基于贝叶斯定理的一组有监督学习算法，即“简单”地假设每对特征之间相互独立。
给定一个类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 和一个从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000793.jpg" alt="x_1" /> 到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000459.jpg" alt="x_n" /> 的相关的特征向量，
贝叶斯定理阐述了以下关系:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000598.jpg" alt="P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}                                  {P(x_1, \dots, x_n)}" class="math" /></p>
</div>
<p class="calibre10">使用简单(naive)的假设-每对特征之间都相互独立:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000154.jpg" alt="P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y) ," class="math" /></p>
</div>
<p class="calibre10">对于所有的 :math: <cite class="calibre13">i</cite> ，这个关系式可以简化为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000357.jpg" alt="P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}                                  {P(x_1, \dots, x_n)}" class="math" /></p>
</div>
<p class="calibre10">由于在给定的输入中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000386.jpg" alt="P(x_1, \dots, x_n)" /> 是一个常量，我们使用下面的分类规则:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000326.jpg" alt="P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)  \Downarrow  \hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y)," class="math" /></p>
</div>
<p class="calibre10">我们可以使用最大后验概率(Maximum A Posteriori, MAP) 来估计 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000729.jpg" alt="P(y)" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000555.jpg" alt="P(x_i \mid y)" /> ; 前者是训练集中类别 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 的相对频率。</p>
<p class="calibre10">各种各样的的朴素贝叶斯分类器的差异大部分来自于处理 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000555.jpg" alt="P(x_i \mid y)" /> 分布时的所做的假设不同。</p>
<p class="calibre10">尽管其假设过于简单，在很多实际情况下，朴素贝叶斯工作得很好，特别是文档分类和垃圾邮件过滤。这些工作都要求
一个小的训练集来估计必需参数。(至于为什么朴素贝叶斯表现得好的理论原因和它适用于哪些类型的数据，请参见下面的参考。)</p>
<p class="calibre10">相比于其他更复杂的方法，朴素贝叶斯学习器和分类器非常快。
分类条件分布的解耦意味着可以独立单独地把每个特征视为一维分布来估计。这样反过来有助于缓解维度灾难带来的问题。</p>
<p class="calibre10">另一方面，尽管朴素贝叶斯被认为是一种相当不错的分类器，但却不是好的估计器(estimator)，所以不能太过于重视从 <code class="docutils"><span class="calibre4">predict_proba</span></code> 输出的概率。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">H. Zhang (2004). <a class="calibre3 pcalibre" href="http://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf">The optimality of Naive Bayes.</a>
Proc. FLAIRS.</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-414">
<span id="calibre_link-415" class="calibre4"></span><h2 class="sigil_not_in_toc">1.9.1. 高斯朴素贝叶斯</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="docutils"><span class="calibre4">GaussianNB</span></code></a> 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000517.jpg" alt="P(x_i \mid y) &amp;= \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)" class="math" /></p>
</div>
<p class="calibre10">参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000001.jpg" alt="\sigma_y" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000375.jpg" alt="\mu_y" /> 使用最大似然法估计。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.naive_bayes</span> <span class="calibre4">import</span> <span class="calibre4">GaussianNB</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gnb</span> <span class="calibre4">=</span> <span class="calibre4">GaussianNB</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">gnb</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Number of mislabeled points out of a total </span><span class="calibre4">%d</span><span class="calibre4"> points : </span><span class="calibre4">%d</span><span class="calibre4">"</span>
<span class="calibre4">... </span>      <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">],(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span> <span class="calibre4">!=</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">sum</span><span class="calibre4">()))</span>
<span class="calibre4">Number of mislabeled points out of a total 150 points : 6</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-416">
<span id="calibre_link-417" class="calibre4"></span><h2 class="sigil_not_in_toc">1.9.2. 多项分布朴素贝叶斯</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="docutils"><span class="calibre4">MultinomialNB</span></code></a> 实现了服从多项分布数据的朴素贝叶斯算法，也是用于文本分类(这个领域中数据往往以词向量表示，尽管在实践中 tf-idf 向量在预测时表现良好)的两大经典朴素贝叶斯算法之一。
分布参数由每类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000058.jpg" alt="\theta_y = (\theta_{y1},\ldots,\theta_{yn})" /> 向量决定， 式中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 是特征的数量(对于文本分类，是词汇量的大小) <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000774.jpg" alt="\theta_{yi}" /> 是样本中属于类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 中特征 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 概率 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000555.jpg" alt="P(x_i \mid y)" /> 。</p>
<p class="calibre10">参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000166.jpg" alt="\theta_y" /> 使用平滑过的最大似然估计法来估计，即相对频率计数:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000816.jpg" alt="\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}" class="math" /></p>
</div>
<dl class="calibre10">
<dt class="calibre18">式中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000440.jpg" alt="N_{yi} = \sum_{x \in T} x_i" /> 是 训练集 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000900.jpg" alt="T" /> 中 特征 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 在类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 中出现的次数，</dt>
<dd class="calibre19"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000263.jpg" alt="N_{y} = \sum_{i=1}^{|T|} N_{yi}" />  是类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 中出现所有特征的计数总和。</dd>
</dl>
<p class="calibre10">先验平滑因子 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000658.jpg" alt="\alpha \ge 0" /> 应用于在学习样本中没有出现的特征，以防在将来的计算中出现0概率输出。
把  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000465.jpg" alt="\alpha = 1" /> 被称为拉普拉斯平滑(Lapalce smoothing)，而 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000669.jpg" alt="\alpha &lt; 1" /> 被称为利德斯通(Lidstone smoothing)。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"></div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-418">
<span id="calibre_link-419" class="calibre4"></span><h2 class="sigil_not_in_toc">1.9.3. 伯努利朴素贝叶斯</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB" title="sklearn.naive_bayes.BernoulliNB"><code class="docutils"><span class="calibre4">BernoulliNB</span></code></a> 实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征
都假设是一个二元 (Bernoulli, boolean) 变量。
因此，这类算法要求样本以二元值特征向量表示；如果样本含有其他类型的数据， 一个 <code class="docutils"><span class="calibre4">BernoulliNB</span></code> 实例会将其二值化(取决于 <code class="docutils"><span class="calibre4">binarize</span></code> 参数)。</p>
<p class="calibre10">伯努利朴素贝叶斯的决策规则基于</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000325.jpg" alt="P(x_i \mid y) = P(i \mid y) x_i + (1 - P(i \mid y)) (1 - x_i)" class="math" /></p>
</div>
<p class="calibre10">与多项分布朴素贝叶斯的规则不同
伯努利朴素贝叶斯明确地惩罚类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 中没有出现作为预测因子的特征 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> ，而多项分布分布朴素贝叶斯只是简单地忽略没出现的特征。</p>
<p class="calibre10">在文本分类的例子中，词频向量(word occurrence vectors)(而非词数向量(word count vectors))可能用于训练和用于这个分类器。 <code class="docutils"><span class="calibre4">BernoulliNB</span></code> 可能在一些数据集上可能表现得更好，特别是那些更短的文档。
如果时间允许，建议对两个模型都进行评估。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.</li>
<li class="toctree-l">A. McCallum and K. Nigam (1998).
<a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529">A comparison of event models for Naive Bayes text classification.</a>
Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.</li>
<li class="toctree-l">V. Metsis, I. Androutsopoulos and G. Paliouras (2006).
<a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542">Spam filtering with Naive Bayes &ndash; Which Naive Bayes?</a>
3rd Conf. on Email and Anti-Spam (CEAS).</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-420">
<h2 class="sigil_not_in_toc">1.9.4. 堆外朴素贝叶斯模型拟合</h2>
<p class="calibre2">朴素贝叶斯模型可以解决整个训练集不能导入内存的大规模分类问题。
为了解决这个问题，
<a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="docutils"><span class="calibre4">MultinomialNB</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB" title="sklearn.naive_bayes.BernoulliNB"><code class="docutils"><span class="calibre4">BernoulliNB</span></code></a>, 和 <a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="docutils"><span class="calibre4">GaussianNB</span></code></a> 实现了 <code class="docutils"><span class="calibre4">partial_fit</span></code> 方法，可以动态的增加数据，使用方法与其他分类器的一样，使用示例见 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="calibre4">Out-of-core classification of text documents</span></a> 。所有的朴素贝叶斯分类器都支持样本权重。</p>
<p class="calibre10">与 <code class="docutils"><span class="calibre4">fit</span></code> 方法不同，首次调用 <code class="docutils"><span class="calibre4">partial_fit</span></code> 方法需要传递一个所有期望的类标签的列表。</p>
<p class="calibre10">对于 scikit-learn 中可用方案的概览，另见 <a class="calibre3 pcalibre" href="scaling_strategies.html#scaling-strategies"><span class="calibre4">out-of-core learning</span></a> 文档。</p>
<p class="calibre10">所有朴素贝叶斯模型调用 <code class="docutils"><span class="calibre4">partial_fit</span></code> 都会引入一些计算开销。推荐让数据快越大越好，其大小与 RAM 中可用内存大小相同。</p>
</div>
</div>


<div class="calibre" id="calibre_link-164">
<span id="calibre_link-421" class="calibre4"></span><h1 class="calibre5">1.10. 决策树</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@文谊</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@皮卡乒的皮卡乓</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@I Remember</a><br class="calibre9" />
    </div>
<p class="calibre10"><strong class="calibre14">Decision Trees (DTs)</strong>  是一种用来 <a class="calibre3 pcalibre" href="#calibre_link-165"><span class="calibre4">classification</span></a> 和 <a class="calibre3 pcalibre" href="#calibre_link-166"><span class="calibre4">regression</span></a> 的无参监督学习方法。其目的是创建一种模型从数据特征中学习简单的决策规则来预测一个目标变量的值。</p>
<p class="calibre10">例如，在下面的图片中，决策树通过if-then-else的决策规则来学习数据从而估测数一个正弦图像。决策树越深入，决策规则就越复杂并且对数据的拟合越好。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000503.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">决策树的优势:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">便于理解和解释。树的结构可以可视化出来。</li>
</ul>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">训练需要的数据少。其他机器学习模型通常需要数据规范化，比如构建虚拟变量和移除缺失值,不过请注意，这种模型不支持缺失值。</li>
</ul>
</div>
</blockquote>
<ul class="calibre6">
<li class="toctree-l">由于训练决策树的数据点的数量导致了决策树的使用开销呈指数分布(训练树模型的时间复杂度是参与训练数据点的对数值)。</li>
<li class="toctree-l">能够处理数值型数据和分类数据。其他的技术通常只能用来专门分析某一种变量类型的数据集。详情请参阅算法。</li>
<li class="toctree-l">能够处理多路输出的问题。</li>
<li class="toctree-l">使用白盒模型。如果某种给定的情况在该模型中是可以观察的，那么就可以轻易的通过布尔逻辑来解释这种情况。相比之下，在黑盒模型中的结果就是很难说明清        楚地。</li>
<li class="toctree-l">可以通过数值统计测试来验证该模型。这对事解释验证该模型的可靠性成为可能。</li>
<li class="toctree-l">即使该模型假设的结果与真实模型所提供的数据有些违反，其表现依旧良好。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">决策树的缺点包括:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">决策树模型容易产生一个过于复杂的模型,这样的模型对数据的泛化性能会很差。这就是所谓的过拟合.一些策略像剪枝、设置叶节点所需的最小样本数或设置数的最大深度是避免出现      该问题最为有效地方法。</li>
<li class="toctree-l">决策树可能是不稳定的，因为数据中的微小变化可能会导致完全不同的树生成。这个问题可以通过决策树的集成来得到缓解</li>
<li class="toctree-l">在多方面性能最优和简单化概念的要求下，学习一棵最优决策树通常是一个NP难问题。因此，实际的决策树学习算法是基于启发式算法，例如在每个节点进     行局部最优决策的贪心算法。这样的算法不能保证返回全局最优决策树。这个问题可以通过集成学习来训练多棵决策树来缓解,这多棵决策树一般通过对特征和样本有放回的随机采样来生成。</li>
<li class="toctree-l">有些概念很难被决策树学习到,因为决策树很难清楚的表述这些概念。例如XOR，奇偶或者复用器的问题。</li>
<li class="toctree-l">如果某些类在问题中占主导地位会使得创建的决策树有偏差。因此，我们建议在拟合前先对数据集进行平衡。</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-165">
<span id="calibre_link-422" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.1. 分类</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">DecisionTreeClassifier</span></code></a> 是能够在数据集上执行多分类的类,与其他分类器一样，<a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">DecisionTreeClassifier</span></code></a> 采用输入两个数组：数组X，用 <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_features]</span></code> 的方式来存放训练样本。整数值数组Y，用 <code class="docutils"><span class="calibre4">[n_samples]</span></code> 来保存训练样本的类标签:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">tree</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">tree</span><span class="calibre4">.</span><span class="calibre4">DecisionTreeClassifier</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">执行通过之后，可以使用该模型来预测样本类别:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([1])</span>
</pre>
</div>
</div>
<p class="calibre10">另外，也可以预测每个类的概率，这个概率是叶中相同类的训练样本的分数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict_proba</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([[ 0.,  1.]])</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">DecisionTreeClassifier</span></code></a> 既能用于二分类（其中标签为[-1,1]）也能用于多分类（其中标签为[0,…,k-1]）。使用Lris数据集，我们可以构造一个决策树，如下所示:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">tree</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">tree</span><span class="calibre4">.</span><span class="calibre4">DecisionTreeClassifier</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">经过训练，我们可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" title="sklearn.tree.export_graphviz"><code class="docutils"><span class="calibre4">export_graphviz</span></code></a> 导出器以 <a class="calibre3 pcalibre" href="http://www.graphviz.org/">Graphviz</a> 格式导出决策树.
如果你是用 <a class="calibre3 pcalibre" href="http://conda.io">conda</a> 来管理包，那么安装 graphviz 二进制文件和 python 包可以用以下指令安装</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">conda install python-graphviz</div>
</blockquote>
<p class="calibre10">或者，可以从 graphviz 项目主页下载 graphviz 的二进制文件，并从 pypi 安装 Python 包装器，并安装 <cite class="calibre13">pip install graphviz</cite> .以下是在整个 iris 数据集上训练的上述树的 graphviz 导出示例; 其结果被保存在 <cite class="calibre13">iris.pdf</cite> 中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>   &gt;&gt;&gt; import graphviz # doctest: +SKIP
   &gt;&gt;&gt; dot_data = tree.export_graphviz(clf, out_file=None) # doctest: +SKIP
   &gt;&gt;&gt; graph = graphviz.Source(dot_data) # doctest: +SKIP
   &gt;&gt;&gt; graph.render("iris") # doctest: +SKIP

:func:`export_graphviz` 出导出还支持各种美化，包括通过他们的类着色节点（或回归值），如果需要，使用显式变量和类名。Jupyter notebook也可以自动找出相同的模块::

   &gt;&gt;&gt; dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP
                            feature_names=iris.feature_names,  # doctest: +SKIP
                            class_names=iris.target_names,  # doctest: +SKIP
                            filled=True, rounded=True,  # doctest: +SKIP
                            special_characters=True)  # doctest: +SKIP
   &gt;&gt;&gt; graph = graphviz.Source(dot_data)  # doctest: +SKIP
   &gt;&gt;&gt; graph # doctest: +SKIP
</pre>
</div>
</div>
<div class="toctree-wrapper">
<img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/iris.svg" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000301.jpg" class="math" /></div>
<p class="calibre10">执行通过之后，可以使用该模型预测样品类别:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">:])</span>
<span class="calibre4">array([0])</span>
</pre>
</div>
</div>
<p class="calibre10">或者，可以根据决策树叶子树里训练样本中的相同类的分数，使得类预测成为可能:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict_proba</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">:])</span>
<span class="calibre4">array([[ 1.,  0.,  0.]])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/tree/plot_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_0013.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000826.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py"><span class="calibre4">Plot the decision surface of a decision tree on the iris dataset</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-166">
<span id="calibre_link-423" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.2. 回归</h2>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000503.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">决策树通过使用 <a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="docutils"><span class="calibre4">DecisionTreeRegressor</span></code></a> 类也可以用来解决回归问题。如在分类设置中，拟合方法将数组X和数组y作为参数，只有在这种情况下，y数组预期才是浮点值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">tree</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2.5</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">tree</span><span class="calibre4">.</span><span class="calibre4">DecisionTreeRegressor</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">array([ 0.5])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py"><span class="calibre4">Decision Tree Regression</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-424">
<span id="calibre_link-425" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.3. 多值输出问题</h2>
<p class="calibre2">一个多值输出问题是一个类似当 Y 是大小为当 Y 是大小为 <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_outputs]</span></code> 的2d数组时，有多个输出值需要预测的监督学习问题。</p>
<p class="calibre10">当输出值之间没有关联时，一个很简单的处理该类型的方法是建立一个n独立模型，即每个模型对应一个输出，然后使用这些模型来独立地预测n个输出中的每一个。然而，由于可能与相同输入相关的输出值本身是相关的，所以通常更好的方法是构建能够同时预测所有n个输出的单个模型。首先，因为仅仅是建立了一个模型所以训练时间会更短。第二，最终模型的泛化性能也会有所提升。对于决策树，这一策略可以很容易地用于多输出问题。 这需要以下更改：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">在叶中存储n个输出值，而不是一个;</li>
<li class="toctree-l">通过计算所有n个输出的平均减少量来作为分裂标准.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">该模块通过在 <code class="docutils"><span class="calibre4">DecisionTreeClassifier</span> <span class="calibre4">`和</span> <span class="calibre4">:class:`DecisionTreeRegressor</span></code> 中实现该策略来支持多输出问题。如果决策树与大小为 <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_outputs]</span></code> 的输出数组Y向匹配，则得到的估计器将:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>* ``predict`` 是输出n_output的值

* 在 ``predict_proba`` 上输出 n_output 数组列表
</pre>
</div>
</div>
<p class="calibre10">用多输出决策树进行回归分析 <a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py"><span class="calibre4">Multi-output Decision Tree Regression</span></a> 。 在该示例中，输入X是单个实数值，并且输出Y是X的正弦和余弦。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression_multioutput.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_tree_regression_multioutput_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000552.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">使用多输出树进行分类，在 <a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="calibre4">Face completion with a multi-output estimators</span></a> 中进行了演示。 在该示例中，输入X是面的上半部分的像素，并且输出Y是这些面的下半部分的像素。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html"></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py"><span class="calibre4">Multi-output Decision Tree Regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="calibre4">Face completion with a multi-output estimators</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">M. Dumont et al,  <a class="calibre3 pcalibre" href="http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf">Fast multi-class image annotation with random subwindows
and multiple output randomized trees</a>, International Conference on
Computer Vision Theory and Applications 2009</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-426">
<span id="calibre_link-427" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.4. 复杂度分析</h2>
<p class="calibre2">总体来说，用来构建平衡二叉树的运行时间为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000041.jpg" alt="O(n_{samples}n_{features}\log(n_{samples}))" /> 查询时间为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000327.jpg" alt="O(\log(n_{samples}))" /> 。尽管树的构造算法尝试生成平衡树，但它们并不总能保持平衡。假设子树能大概保持平衡，每个节点的成本包括通过 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000142.jpg" alt="O(n_{features})" /> 时间复杂度来搜索找到提供熵减小最大的特征。每个节点的花费为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000536.jpg" alt="O(n_{features}n_{samples}\log(n_{samples}))" /> ，从而使得整个决策树的构造成本为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000467.jpg" alt="O(n_{features}n_{samples}^{2}\log(n_{samples}))" /> 。</p>
<p class="calibre10">Scikit-learn提供了更多有效的方法来创建决策树。初始实现（如上所述）将重新计算沿着给定特征的每个新分割点的类标签直方图（用于分类）或平均值（用于回归）。与分类所有的样本特征，然后再次训练时运行标签计数，可将每个节点的复杂度降低为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000524.jpg" alt="O(n_{features}\log(n_{samples}))" /> ，则总的成本花费为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000536.jpg" alt="O(n_{features}n_{samples}\log(n_{samples}))" /> 。这是一种对所有基于树的算法的改进选项。默认情况下，对于梯度提升模型该算法是打开的，一般来说它会让训练速度更快。但对于所有其他算法默认是关闭的，当训练深度很深的树时往往会减慢训练速度。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-428">
<h2 class="sigil_not_in_toc">1.10.5. 实际使用技巧</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li>
<li class="toctree-l">考虑事先进行降维( <a class="calibre3 pcalibre" href="decomposition.html#pca"><span class="calibre4">PCA</span></a> , <a class="calibre3 pcalibre" href="decomposition.html#ica"><span class="calibre4">ICA</span></a> ，使您的树更好地找到具有分辨性的特征。</li>
<li class="toctree-l">通过 <code class="docutils"><span class="calibre4">export</span></code> 功能可以可视化您的决策树。使用 <code class="docutils"><span class="calibre4">max_depth=3</span></code> 作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li>
<li class="toctree-l">请记住，填充树的样本数量会增加树的每个附加级别。使用 <code class="docutils"><span class="calibre4">max_depth</span></code> 来控制输的大小防止过拟合。</li>
<li class="toctree-l">通过使用 <code class="docutils"><span class="calibre4">min_samples_split</span></code> 和 <code class="docutils"><span class="calibre4">min_samples_leaf</span></code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。所以尝试 <code class="docutils"><span class="calibre4">min_samples_leaf=5</span></code> 作为初始值。如果样本的变化量很大，可以使用浮点数作为这两个参数中的百分比。两者之间的主要区别在于 <code class="docutils"><span class="calibre4">min_samples_leaf</span></code> 保证叶结点中最少的采样数，而 <code class="docutils"><span class="calibre4">min_samples_split</span></code> 可以创建任意小的叶子，尽管在文献中 <code class="docutils"><span class="calibre4">min_samples_split</span></code> 更常见。</li>
<li class="toctree-l">在训练之前平衡您的数据集，以防止决策树偏向于主导类.可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code class="docutils"><span class="calibre4">sample_weight</span></code>) 的和归一化为相同的值。还要注意的是，基于权重的预修剪标准 (<code class="docutils"><span class="calibre4">min_weight_fraction_leaf</span></code>) 对于显性类别的偏倚偏小，而不是不了解样本权重的标准，如 <code class="docutils"><span class="calibre4">min_samples_leaf</span></code> 。</li>
</ul>
</div>
</blockquote>
<ul class="calibre6">
<li class="toctree-l">如果样本被加权，则使用基于权重的预修剪标准 <code class="docutils"><span class="calibre4">min_weight_fraction_leaf</span></code> 来优化树结构将更容易，这确保叶节点包含样本权重的总和的至少一部分。</li>
<li class="toctree-l">所有的决策树内部使用 <code class="docutils"><span class="calibre4">np.float32</span></code> 数组 ，如果训练数据不是这种格式，将会复制数据集。</li>
<li class="toctree-l">如果输入的矩阵X为稀疏矩阵，建议您在调用fit之前将矩阵X转换为稀疏的``csc_matrix`` ,在调用predict之前将 <code class="docutils"><span class="calibre4">csr_matrix</span></code> 稀疏。当特征在大多数样本中具有零值时，与密集矩阵相比，稀疏矩阵输入的训练时间可以快几个数量级。</li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-429">
<span id="calibre_link-430" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.6. 决策树算法: ID3, C4.5, C5.0 和 CART</h2>
<p class="calibre2">所有种类的决策树算法有哪些以及它们之间的区别？scikit-learn 中实现何种算法呢？</p>
<p class="calibre10">ID3（Iterative Dichotomiser 3）由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。</p>
<p class="calibre10">C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。</p>
<p class="calibre10">C5.0 是 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。</p>
<p class="calibre10">CART（Classification and Regression Trees （分类和回归树））与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。</p>
<p class="calibre10">scikit-learn 使用 CART 算法的优化版本。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-431">
<span id="calibre_link-432" class="calibre4"></span><h2 class="sigil_not_in_toc">1.10.7. 数学表达</h2>
<p class="calibre2">给定训练向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000020.jpg" alt="x_i \in R^n" />, i=1,…, l 和标签向量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000891.jpg" alt="y \in R^l" />。决策树递归地分割空间，例如将有相同标签的样本归为一组。</p>
<p class="calibre10">将 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 节点上的数据用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000498.jpg" alt="Q" /> 来表示。每一个候选组 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000801.jpg" alt="\theta = (j, t_m)" /> 包含一个特征 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000457.jpg" alt="j" /> 和阈值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000063.jpg" alt="t_m" /> 将,数据分成 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000163.jpg" alt="Q_{left}(\theta)" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000540.jpg" alt="Q_{right}(\theta)" /> 子集。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000481.jpg" alt="Q_{left}(\theta) = {(x, y) | x_j &lt;= t_m}  Q_{right}(\theta) = Q \setminus Q_{left}(\theta)" class="math" /></p>
</div>
<p class="calibre10">使用不纯度函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000515.jpg" alt="H()" /> 计算 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 处的不纯度,其选择取决于正在解决的任务（分类或回归）</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000867.jpg" alt="G(Q, \theta) = \frac{n_{left}}{N_m} H(Q_{left}(\theta)) + \frac{n_{right}}{N_m} H(Q_{right}(\theta))" class="math" /></p>
</div>
<p class="calibre10">选择使不纯度最小化的参数</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000737.jpg" alt="\theta^* = \operatorname{argmin}_\theta  G(Q, \theta)" class="math" /></p>
</div>
<p class="calibre10">重新计算子集 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000901.jpg" alt="Q_{left}(\theta^*)" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000363.jpg" alt="Q_{right}(\theta^*)" /> ，直到达到最大允许深度，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000671.jpg" alt="N_m &lt; \min_{samples}" /> 或 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000782.jpg" alt="N_m = 1" />。</p>
<div class="toctree-wrapper" id="calibre_link-433">
<h3 class="sigil_not_in_toc1">1.10.7.1. 分类标准</h3>
<p class="calibre2">对于节点 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> ，表示具有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000340.jpg" alt="N_m" /> 个观测值的区域 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000331.jpg" alt="R_m" /> ，如果分类结果采用值是 0,1,…,K-1 的值，让</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000839.jpg" alt="p_{mk} = 1/ N_m \sum_{x_i \in R_m} I(y_i = k)" class="math" /></p>
</div>
<p class="calibre10">是节点 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 中k类观测的比例通常用来处理杂质的方法是Gini</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000881.jpg" alt="H(X_m) = \sum_k p_{mk} (1 - p_{mk})" class="math" /></p>
</div>
<p class="calibre10">Cross-Entropy （交叉熵）</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000005.jpg" alt="H(X_m) = - \sum_k p_{mk} \log(p_{mk})" class="math" /></p>
</div>
<p class="calibre10">和 Misclassification （错误分类）</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000373.jpg" alt="H(X_m) = 1 - \max(p_{mk})" class="math" /></p>
</div>
<p class="calibre10">在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000491.jpg" alt="X_m" /> 训练 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 节点上的数据时。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-434">
<h3 class="sigil_not_in_toc1">1.10.7.2. 回归标准</h3>
<p class="calibre2">如果目标是连续性的值，那么对于节点 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> ,表示具有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000340.jpg" alt="N_m" /> 个观测值的区域 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000331.jpg" alt="R_m" /> ，对于以后的分裂节点的位置的决定常用的最小化标准是均方差和平均绝对误差，前者使用终端节点处的平均值来最小化L2误差，后者使用终端节点处的中值来最小化 L1 误差。</p>
<p class="calibre10">Mean Squared Error （均方误差）:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000626.jpg" alt="c_m = \frac{1}{N_m} \sum_{i \in N_m} y_i  H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - c_m)^2" class="math" /></p>
</div>
<p class="calibre10">Mean Absolute Error（平均绝对误差）:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000416.jpg" alt="\bar{y_m} = \frac{1}{N_m} \sum_{i \in N_m} y_i  H(X_m) = \frac{1}{N_m} \sum_{i \in N_m} |y_i - \bar{y_m}|" class="math" /></p>
</div>
<p class="calibre10">在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000491.jpg" alt="X_m" /> 训练 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 节点上的数据时。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Predictive_analytics">https://en.wikipedia.org/wiki/Predictive_analytics</a></li>
<li class="toctree-l">L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and
Regression Trees. Wadsworth, Belmont, CA, 1984.</li>
<li class="toctree-l">J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</li>
<li class="toctree-l">T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning, Springer, 2009.</li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-176">
<span id="calibre_link-435" class="calibre4"></span><h1 class="calibre5">1.11. 集成方法</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/zehuichen123" class="calibre3 pcalibre">@zehuichen123</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/JanzenLiu" class="calibre3 pcalibre">@JanzenLiu</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@\S^R^Y/</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@StupidStalker</a><br class="calibre9" />
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@文谊</a><br class="calibre9" />
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@t9UhoI</a><br class="calibre9" />
    </div>
<p class="calibre10"><code class="docutils"><span class="calibre4">注意，在本文中</span> <span class="calibre4">bagging</span> <span class="calibre4">和</span> <span class="calibre4">boosting</span> <span class="calibre4">为了更好的保留原文意图，不进行翻译</span></code>
<code class="docutils"><span class="calibre4">estimator-&gt;估计器</span>&nbsp; <span class="calibre4">base</span> <span class="calibre4">estimator-&gt;基估计器</span></code></p>
<p class="calibre10"><strong class="calibre14">集成方法</strong> 的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力/鲁棒性。</p>
<p class="calibre10">集成方法通常分为两种:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first"><strong class="calibre14">平均方法</strong>，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。</p>
<p class="calibre10"><strong class="calibre14">示例:</strong> <a class="calibre3 pcalibre" href="#calibre_link-177"><span class="calibre4">Bagging 方法</span></a> , <a class="calibre3 pcalibre" href="#calibre_link-178"><span class="calibre4">随机森林</span></a> , …</p>
</li>
<li class="toctree-l"><p class="first">相比之下，在 <strong class="calibre14">boosting 方法</strong> 中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差。这种方法主要目的是为了结合多个弱模型，使集成的模型更加强大。</p>
<p class="calibre10"><strong class="calibre14">示例:</strong> <a class="calibre3 pcalibre" href="#calibre_link-179"><span class="calibre4">AdaBoost</span></a> , <a class="calibre3 pcalibre" href="#calibre_link-180"><span class="calibre4">梯度提升树</span></a> , …</p>
</li>
</ul>
<div class="toctree-wrapper" id="calibre_link-436">
<span id="calibre_link-177" class="calibre4"></span><h2 class="sigil_not_in_toc">1.11.1. Bagging meta-estimator（Bagging 元估计器）</h2>
<p class="calibre2">在集成算法中，bagging 方法会在原始训练集的随机子集上构建一类黑盒估计器的多个实例，然后把这些估计器的预测结果结合起来形成最终的预测结果。
该方法通过在构建模型的过程中引入随机性，来减少基估计器的方差(例如，决策树)。
在多数情况下，bagging 方法提供了一种非常简单的方式来对单一模型进行改进，而无需修改背后的算法。
因为 bagging 方法可以减小过拟合，所以通常在强分类器和复杂模型上使用时表现的很好（例如，完全决策树，fully developed decision trees），相比之下 boosting 方法则在弱模型上表现更好（例如，浅层决策树，shallow decision trees）。</p>
<p class="calibre10">bagging 方法有很多种，其主要区别在于随机抽取训练子集的方法不同：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">如果抽取的数据集的随机子集是样例的随机子集，我们叫做粘贴 (Pasting) <a class="calibre3 pcalibre" href="#calibre_link-181" id="calibre_link-185">[B1999]</a> 。</li>
<li class="toctree-l">如果样例抽取是有放回的，我们称为 Bagging <a class="calibre3 pcalibre" href="#calibre_link-182" id="calibre_link-186">[B1996]</a> 。</li>
<li class="toctree-l">如果抽取的数据集的随机子集是特征的随机子集，我们叫做随机子空间 (Random Subspaces) <a class="calibre3 pcalibre" href="#calibre_link-183" id="calibre_link-187">[H1998]</a> 。</li>
<li class="toctree-l">最后，如果基估计器构建在对于样本和特征抽取的子集之上时，我们叫做随机补丁 (Random Patches) <a class="calibre3 pcalibre" href="#calibre_link-184" id="calibre_link-188">[LG2012]</a> 。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">在 scikit-learn 中，bagging 方法使用统一的 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier" title="sklearn.ensemble.BaggingClassifier"><code class="docutils"><span class="calibre4">BaggingClassifier</span></code></a> 元估计器（或者 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor" title="sklearn.ensemble.BaggingRegressor"><code class="docutils"><span class="calibre4">BaggingRegressor</span></code></a> ），输入的参数和随机子集抽取策略由用户指定。<code class="docutils"><span class="calibre4">max_samples</span></code> 和 <code class="docutils"><span class="calibre4">max_features</span></code> 控制着子集的大小（对于样例和特征）， <code class="docutils"><span class="calibre4">bootstrap</span></code> 和 <code class="docutils"><span class="calibre4">bootstrap_features</span></code> 控制着样例和特征的抽取是有放回还是无放回的。
当使用样本子集时，通过设置 <code class="docutils"><span class="calibre4">oob_score=True</span></code> ，可以使用袋外(out-of-bag)样本来评估泛化精度。下面的代码片段说明了如何构造一个 <code class="docutils"><span class="calibre4">KNeighborsClassifier</span></code> 估计器的 bagging 集成实例，每一个基估计器都建立在 50% 的样本随机子集和 50% 的特征随机子集上。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">BaggingClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors</span> <span class="calibre4">import</span> <span class="calibre4">KNeighborsClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">bagging</span> <span class="calibre4">=</span> <span class="calibre4">BaggingClassifier</span><span class="calibre4">(</span><span class="calibre4">KNeighborsClassifier</span><span class="calibre4">(),</span>
<span class="calibre4">... </span>                            <span class="calibre4">max_samples</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">max_features</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="calibre4">Single estimator versus bagging: bias-variance decomposition</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<table class="docutils1" frame="void" id="calibre_link-181" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-185">[B1999]</a></td>
<td class="label1">L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-182" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-186">[B1996]</a></td>
<td class="label1">L. Breiman, “Bagging predictors”, Machine Learning, 24(2),
123-140, 1996.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-183" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-187">[H1998]</a></td>
<td class="label1">T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-184" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-188">[LG2012]</a></td>
<td class="label1">G. Louppe and P. Geurts, “Ensembles on Random Patches”,
Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-178">
<span id="calibre_link-437" class="calibre4"></span><h2 class="sigil_not_in_toc">1.11.2. 由随机树组成的森林</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="docutils"><span class="calibre4">sklearn.ensemble</span></code></a> 模块包含两个基于 <a class="calibre3 pcalibre" href="tree.html#tree"><span class="calibre4">随机决策树</span></a> 的平均算法： RandomForest 算法和 Extra-Trees 算法。
这两种算法都是专门为树而设计的扰动和组合技术（perturb-and-combine techniques） <a class="calibre3 pcalibre" href="#calibre_link-189" id="calibre_link-192">[B1998]</a> 。
这种技术通过在分类器构造过程中引入随机性来创建一组不同的分类器。集成分类器的预测结果就是单个分类器预测结果的平均值。</p>
<p class="calibre10">与其他分类器一样，森林分类器必须拟合（fit）两个数组：
保存训练样本的数组（或稀疏或稠密的）X，大小为 <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_features]</span></code>，和
保存训练样本目标值（类标签）的数组 Y，大小为 <code class="docutils"><span class="calibre4">[n_samples]</span></code>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">RandomForestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">同 <a class="calibre3 pcalibre" href="tree.html#tree"><span class="calibre4">决策树</span></a> 一样，随机森林算法（forests of trees）也能用来解决 <a class="calibre3 pcalibre" href="tree.html#tree-multioutput"><span class="calibre4">多输出问题</span></a> （如果 Y 的大小是 <code class="docutils"><span class="calibre4">[n_samples, n_outputs])</span></code> ）。</p>
<div class="toctree-wrapper" id="calibre_link-438">
<h3 class="sigil_not_in_toc1">1.11.2.1. 随机森林</h3>
<p class="calibre2">在随机森林中（参见 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">ExtraTreesClassifier</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="docutils"><span class="calibre4">ExtraTreesRegressor</span></code></a> 类），
集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的（例如，自助采样法-bootstrap sample，这里采用西瓜书中的译法）。
另外，在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点。
由于这种随机性，森林的偏差通常会有略微的增大（相对于单个非随机树的偏差），但是由于取了平均，其方差也会减小，通常能够补偿偏差的增加，从而产生一个总体上更好的模型。</p>
<p class="calibre10">与原始文献 <a class="calibre3 pcalibre" href="#calibre_link-190" id="calibre_link-191">[B2001]</a> 不同的是，scikit-learn 的实现是取每个分类器预测概率的平均，而不是让每个分类器对类别进行投票。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-439">
<h3 class="sigil_not_in_toc1">1.11.2.2. 极限随机树</h3>
<p class="calibre2">在极限随机树中（参见 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">ExtraTreesClassifier</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="docutils"><span class="calibre4">ExtraTreesRegressor</span></code></a> 类)，
计算分割点方法中的随机性进一步增强。
在随机森林中，使用的特征是候选特征的随机子集；不同于寻找最具有区分度的阈值，
这里的阈值是针对每个候选特征随机生成的，并且选择这些随机生成的阈值中的最佳者作为分割规则。
这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_blobs</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">RandomForestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">ExtraTreesClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.tree</span> <span class="calibre4">import</span> <span class="calibre4">DecisionTreeClassifier</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_blobs</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">10000</span><span class="calibre4">,</span> <span class="calibre4">n_features</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">centers</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">DecisionTreeClassifier</span><span class="calibre4">(</span><span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">,</span> <span class="calibre4">min_samples_split</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">()</span>                             
<span class="calibre4">0.97...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">min_samples_split</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">()</span>                             
<span class="calibre4">0.999...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">ExtraTreesClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">min_samples_split</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">()</span> <span class="calibre4">&gt;</span> <span class="calibre4">0.999</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_forest_iris_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000316.jpg" class="calibre27" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-440">
<h3 class="sigil_not_in_toc1">1.11.2.3. 参数</h3>
<p class="calibre2">使用这些方法时要调整的参数主要是 <code class="docutils"><span class="calibre4">n_estimators</span></code> 和 <code class="docutils"><span class="calibre4">max_features</span></code>。
前者（n_estimators）是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。
此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。
后者（max_features）是分割节点时考虑的特征的随机子集的大小。
这个值越低，方差减小得越多，但是偏差的增大也越多。
根据经验，回归问题中使用 <code class="docutils"><span class="calibre4">max_features</span> <span class="calibre4">=</span> <span class="calibre4">n_features</span></code> ，
分类问题使用 <code class="docutils"><span class="calibre4">max_features</span> <span class="calibre4">=</span> <span class="calibre4">sqrt（n_features</span></code>
（其中 <code class="docutils"><span class="calibre4">n_features</span></code> 是特征的个数）是比较好的默认值。
<code class="docutils"><span class="calibre4">max_depth</span> <span class="calibre4">=</span> <span class="calibre4">None</span></code> 和 <code class="docutils"><span class="calibre4">min_samples_split</span> <span class="calibre4">=</span> <span class="calibre4">2</span></code> 结合通常会有不错的效果（即生成完全的树）。
请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。
另外，请注意，在随机森林中，默认使用自助采样法（<code class="docutils"><span class="calibre4">bootstrap</span> <span class="calibre4">=</span> <span class="calibre4">True</span></code>），
然而 extra-trees 的默认策略是使用整个数据集（<code class="docutils"><span class="calibre4">bootstrap</span> <span class="calibre4">=</span> <span class="calibre4">False</span></code>）。
当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 <code class="docutils"><span class="calibre4">oob_score</span> <span class="calibre4">=</span> <span class="calibre4">True</span></code> 即可实现。</p>
<div class="toctree-wrapper">
<p class="calibre10">提示:</p>
<p class="calibre10">默认参数下模型复杂度是：<code class="docutils"><span class="calibre4">O(M*N*log(N))</span></code> ，
其中 <code class="docutils"><span class="calibre4">M</span></code> 是树的数目， <code class="docutils"><span class="calibre4">N</span></code> 是样本数。
可以通过设置以下参数来降低模型复杂度： <code class="docutils"><span class="calibre4">min_samples_split</span></code> , <code class="docutils"><span class="calibre4">min_samples_leaf</span></code> , <code class="docutils"><span class="calibre4">max_leaf_nodes`` 和 ``max_depth</span></code> 。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-441">
<h3 class="sigil_not_in_toc1">1.11.2.4. 并行化</h3>
<p class="calibre2">最后，这个模块还支持树的并行构建和预测结果的并行计算，这可以通过 <code class="docutils"><span class="calibre4">n_jobs</span></code> 参数实现。
如果设置 <code class="docutils"><span class="calibre4">n_jobs</span> <span class="calibre4">=</span> <span class="calibre4">k</span></code> ，则计算被划分为 <code class="docutils"><span class="calibre4">k</span></code> 个作业，并运行在机器的 <code class="docutils"><span class="calibre4">k</span></code> 个核上。
如果设置 <code class="docutils"><span class="calibre4">n_jobs</span> <span class="calibre4">=</span> <span class="calibre4">-1</span></code> ，则使用机器的所有核。
注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code class="docutils"><span class="calibre4">k</span></code> 个作业不会快 <code class="docutils"><span class="calibre4">k</span></code> 倍）。
当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py"><span class="calibre4">Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="calibre4">Pixel importances with a parallel forest of trees</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="calibre4">Face completion with a multi-output estimators</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<table class="docutils1" frame="void" id="calibre_link-190" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-191">[B2001]</a></td>
<td class="label1"><ol class="arabic" start="12">
<li class="toctree-l">Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-189" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-192">[B1998]</a></td>
<td class="label1"><ol class="arabic" start="12">
<li class="toctree-l">Breiman, “Arcing Classifiers”, Annals of Statistics 1998.</li>
</ol>
</td>
</tr>
</tbody>
</table>
<ul class="calibre6">
<li class="toctree-l">P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-206">
<span id="calibre_link-442" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.11.2.5. 特征重要性评估</h3>
<p class="calibre2">特征对目标变量预测的相对重要性可以通过（树中的决策节点的）特征使用的相对顺序（即深度）来进行评估。
决策树顶部使用的特征对更大一部分输入样本的最终预测决策做出贡献；因此，可以使用接受每个特征对最终预测的贡献的样本比例来评估该 <strong class="calibre14">特征的相对重要性</strong> 。</p>
<p class="calibre10">通过对多个随机树中的 <strong class="calibre14">预期贡献率</strong> （expected activity rates） <strong class="calibre14">取平均</strong>，可以减少这种估计的 <strong class="calibre14">方差</strong> ，并将其用于特征选择。</p>
<p class="calibre10">下面的例子展示了一个面部识别任务中每个像素的相对重要性，其中重要性由颜色（的深浅）来表示，使用的模型是 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">ExtraTreesClassifier</span></code></a> 。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances_faces.html"></a>
</div>
<p class="calibre10">实际上，对于训练完成的模型这些估计值存储在 <code class="docutils"><span class="calibre4">feature_importances_</span></code> 属性中。
这是一个大小为 <code class="docutils"><span class="calibre4">(n_features,)</span></code> 的数组，其每个元素值为正，并且总和为 1.0。一个元素的值越高，其对应的特征对预测函数的贡献越大。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="calibre4">Pixel importances with a parallel forest of trees</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="calibre4">Feature importances with forests of trees</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-443">
<span id="calibre_link-444" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.11.2.6. 完全随机树嵌入</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="docutils"><span class="calibre4">RandomTreesEmbedding</span></code></a> 实现了一个无监督的数据转换。
通过由完全随机树构成的森林，<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="docutils"><span class="calibre4">RandomTreesEmbedding</span></code></a> 使用数据最终归属的叶子节点的索引值（编号）对数据进行编码。
该索引以 one-of-K 方式编码，最终形成一个高维的稀疏二进制编码。 这种编码可以被非常高效地计算出来，并且可以作为其他学习任务的基础。
编码的大小和稀疏度可以通过选择树的数量和每棵树的最大深度来确定。对于集成中的每棵树的每个节点包含一个实例（校对者注：这里真的没搞懂）。
编码的大小（维度）最多为 <code class="docutils"><span class="calibre4">n_estimators</span> <span class="calibre4">*</span> <span class="calibre4">2</span> <span class="calibre4">**</span> <span class="calibre4">max_depth</span></code> ，即森林中的叶子节点的最大数。</p>
<p class="calibre10">由于相邻数据点更可能位于树的同一叶子中，该变换可以作为一种隐式地非参数密度估计。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="calibre4">Hashing feature transformation using Totally Random Trees</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="calibre4">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a> 比较了手写体数字的非线性降维技术。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="calibre4">Feature transformations with ensembles of trees</span></a> 比较了基于树的有监督和无监督特征变换.</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">See also</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="manifold.html#manifold"><span class="calibre4">流形学习</span></a> 方法也可以用于特征空间的非线性表示, 以及降维.</p>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-179">
<span id="calibre_link-445" class="calibre4"></span><h2 class="sigil_not_in_toc">1.11.3. AdaBoost</h2>
<p class="calibre2">模型 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="docutils"><span class="calibre4">sklearn.ensemble</span></code></a> 包含了流行的提升算法 AdaBoost, 这个算法是由 Freund and Schapire 在 1995 年提出来的 <a class="calibre3 pcalibre" href="#calibre_link-193" id="calibre_link-197">[FS1995]</a>.</p>
<p class="calibre10">AdaBoost 的核心思想是用反复修改的数据（校对者注：主要是修正数据的权重）来训练一系列的弱学习器(一个弱学习器模型仅仅比随机猜测好一点,
比如一个简单的决策树),由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合,
得到我们最终的预测结果。在每一次所谓的提升（boosting）迭代中，数据的修改由应用于每一个训练样本的（新）
的权重 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000538.jpg" alt="w_1" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000769.jpg" alt="w_2" />, …, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000661.jpg" alt="w_N" /> 组成（校对者注：即修改每一个训练样本应用于新一轮学习器的权重）。
初始化时,将所有弱学习器的权重都设置为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000811.jpg" alt="w_i = 1/N" /> ,因此第一次迭代仅仅是通过原始数据训练出一个弱学习器。在接下来的
连续迭代中,样本的权重逐个地被修改,学习算法也因此要重新应用这些已经修改的权重。在给定的一个迭代中,
那些在上一轮迭代中被预测为错误结果的样本的权重将会被增加，而那些被预测为正确结果的样本的权
重将会被降低。随着迭代次数的增加，那些难以预测的样例的影响将会越来越大，每一个随后的弱学习器都将
会被强迫更加关注那些在之前被错误预测的样例 <a class="calibre3 pcalibre" href="#calibre_link-194" id="calibre_link-200">[HTF]</a>.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000450.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">AdaBoost 既可以用在分类问题也可以用在回归问题中:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">对于 multi-class 分类， <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="docutils"><span class="calibre4">AdaBoostClassifier</span></code></a> 实现了 AdaBoost-SAMME 和 AdaBoost-SAMME.R <a class="calibre3 pcalibre" href="#calibre_link-195" id="calibre_link-198">[ZZRH2009]</a>.</li>
<li class="toctree-l">对于回归， <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor" title="sklearn.ensemble.AdaBoostRegressor"><code class="docutils"><span class="calibre4">AdaBoostRegressor</span></code></a> 实现了 AdaBoost.R2 <a class="calibre3 pcalibre" href="#calibre_link-196" id="calibre_link-199">[D1997]</a>.</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-446">
<h3 class="sigil_not_in_toc1">1.11.3.1. 使用方法</h3>
<p class="calibre2">下面的例子展示了如何训练一个包含 100 个弱学习器的 AdaBoost 分类器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">AdaBoostClassifier</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">AdaBoostClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">()</span>                             
<span class="calibre4">0.9...</span>
</pre>
</div>
</div>
<p class="calibre10">弱学习器的数量由参数 <code class="docutils"><span class="calibre4">n_estimators</span></code> 来控制。 <code class="docutils"><span class="calibre4">learning_rate</span></code> 参数用来控制每个弱学习器对
最终的结果的贡献程度（校对者注：其实应该就是控制每个弱学习器的权重修改速率，这里不太记得了，不确定）。
弱学习器默认使用决策树。不同的弱学习器可以通过参数 <code class="docutils"><span class="calibre4">base_estimator</span></code> 来指定。
获取一个好的预测结果主要需要调整的参数是 <code class="docutils"><span class="calibre4">n_estimators</span></code> 和 <code class="docutils"><span class="calibre4">base_estimator</span></code> 的复杂度
(例如:对于弱学习器为决策树的情况，树的深度 <code class="docutils"><span class="calibre4">max_depth</span></code> 或叶子节点的最小样本数 <code class="docutils"><span class="calibre4">min_samples_leaf</span></code>
等都是控制树的复杂度的参数)</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py"><span class="calibre4">Discrete versus Real AdaBoost</span></a> 使用 AdaBoost-SAMME 和 AdaBoost-SAMME.R 比较 decision stump， decision tree（决策树）和 boosted decision stump（增强决策树）的分类错误。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py"><span class="calibre4">Multi-class AdaBoosted Decision Trees</span></a> 展示了 AdaBoost-SAMME 和 AdaBoost-SAMME.R 在 multi-class （多类）问题上的性能。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"><span class="calibre4">Two-class AdaBoost</span></a> 展示了使用 AdaBoost-SAMME 的非线性可分两类问题的决策边界和决策函数值。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"><span class="calibre4">Decision Tree Regression with AdaBoost</span></a> 使用 AdaBoost.R2 算法证明了回归。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-193" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-197">[FS1995]</a></td>
<td class="label1">Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of
On-Line Learning and an Application to Boosting”, 1997.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-195" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-198">[ZZRH2009]</a></td>
<td class="label1">J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”,
2009.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-196" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-199">[D1997]</a></td>
<td class="label1"><ol class="arabic" start="8">
<li class="toctree-l">Drucker. “Improving Regressors using Boosting Techniques”, 1997.</li>
</ol>
</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-194" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-200">[HTF]</a></td>
<td class="label1">T. Hastie, R. Tibshirani and J. Friedman, “Elements of
Statistical Learning Ed. 2”, Springer, 2009.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-447">
<span id="calibre_link-180" class="calibre4"></span><h2 class="sigil_not_in_toc">1.11.4. Gradient Tree Boosting（梯度树提升）</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Tree Boosting</a>
或梯度提升回归树（GBRT）是对于任意的可微损失函数的提升算法的泛化。 GBRT 是一个准确高效的现有程序，
它既能用于分类问题也可以用于回归问题。梯度树提升模型被应用到各种领域，包括网页搜索排名和生态领域。</p>
<p class="calibre10">GBRT 的优点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">对混合型数据的自然处理（异构特征）</li>
<li class="toctree-l">强大的预测能力</li>
<li class="toctree-l">在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)</li>
</ul>
</div>
</blockquote>
<p class="calibre10">GBRT 的缺点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBRT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">模块 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="docutils"><span class="calibre4">sklearn.ensemble</span></code></a> 通过梯度提升树提供了分类和回归的方法.</p>
<div class="toctree-wrapper" id="calibre_link-448">
<h3 class="sigil_not_in_toc1">1.11.4.1. 分类</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="docutils"><span class="calibre4">GradientBoostingClassifier</span></code></a> 既支持二分类又支持多分类问题。
下面的例子展示了如何训练一个包含 100 个决策树弱学习器的梯度提升分类器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_hastie_10_2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">GradientBoostingClassifier</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_hastie_10_2</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[:</span><span class="calibre4">2000</span><span class="calibre4">],</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">2000</span><span class="calibre4">:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">y</span><span class="calibre4">[:</span><span class="calibre4">2000</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">2000</span><span class="calibre4">:]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">GradientBoostingClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">learning_rate</span><span class="calibre4">=</span><span class="calibre4">1.0</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span>                 
<span class="calibre4">0.913...</span>
</pre>
</div>
</div>
<p class="calibre10">弱学习器(例如:回归树)的数量由参数 <code class="docutils"><span class="calibre4">n_estimators</span></code> 来控制；每个树的大小可以通过由参数 <code class="docutils"><span class="calibre4">max_depth</span></code> 设置树的深度，或者由参数 <code class="docutils"><span class="calibre4">max_leaf_nodes</span></code> 设置叶子节点数目来控制。 <code class="docutils"><span class="calibre4">learning_rate</span></code> 是一个在 (0,1] 之间的超参数，这个参数通过 shrinkage(缩减步长) 来控制过拟合。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">超过两类的分类问题需要在每一次迭代时推导 <code class="docutils"><span class="calibre4">n_classes</span></code> 个回归树。因此，所有的需要推导的树数量等于 <code class="docutils"><span class="calibre4">n_classes</span> <span class="calibre4">*</span> <span class="calibre4">n_estimators</span></code> 。对于拥有大量类别的数据集我们强烈推荐使用 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">RandomForestClassifier</span></code></a> 来代替 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="docutils"><span class="calibre4">GradientBoostingClassifier</span></code></a> 。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-449">
<h3 class="sigil_not_in_toc1">1.11.4.2. 回归</h3>
<p class="calibre2">对于回归问题 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="docutils"><span class="calibre4">GradientBoostingRegressor</span></code></a> 支持一系列 <a class="calibre3 pcalibre" href="#calibre_link-201"><span class="calibre4">different loss functions</span></a> ，这些损失函数可以通过参数 <code class="docutils"><span class="calibre4">loss</span></code> 来指定；对于回归问题默认的损失函数是最小二乘损失函数（ <code class="docutils"><span class="calibre4">'ls'</span></code> ）。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">mean_squared_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_friedman1</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">GradientBoostingRegressor</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_friedman1</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">1200</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">noise</span><span class="calibre4">=</span><span class="calibre4">1.0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[:</span><span class="calibre4">200</span><span class="calibre4">],</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">200</span><span class="calibre4">:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">y</span><span class="calibre4">[:</span><span class="calibre4">200</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">200</span><span class="calibre4">:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">est</span> <span class="calibre4">=</span> <span class="calibre4">GradientBoostingRegressor</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">learning_rate</span><span class="calibre4">=</span><span class="calibre4">0.1</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">loss</span><span class="calibre4">=</span><span class="calibre4">'ls'</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_error</span><span class="calibre4">(</span><span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">))</span>    
<span class="calibre4">5.00...</span>
</pre>
</div>
</div>
<p class="calibre10">下图展示了应用损失函数为最小二乘损失，基学习器个数为 500 的 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="docutils"><span class="calibre4">GradientBoostingRegressor</span></code></a> 来处理 <a class="calibre3 pcalibre" href="generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston" title="sklearn.datasets.load_boston"><code class="docutils"><span class="calibre4">sklearn.datasets.load_boston</span></code></a> 数据集的结果。左图表示每一次迭代的训练误差和测试误差。每一次迭代的训练误差保存在提升树模型的 <code class="docutils"><span class="calibre4">train_score_</span></code> 属性中，每一次迭代的测试误差能够通过
<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict" title="sklearn.ensemble.GradientBoostingRegressor.staged_predict"><code class="docutils"><span class="calibre4">staged_predict</span></code></a> 方法获取，该方法返回一个生成器，用来产生每一
个迭代的预测结果。类似下面这样的图表，可以用于决定最优的树的数量，从而进行提前停止。右图表示每个特征的重要性，它
可以通过 <code class="docutils"><span class="calibre4">feature_importances_</span></code> 属性来获取.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gradient_boosting_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000065.jpg" class="calibre30" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="calibre4">Gradient Boosting regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="calibre4">Gradient Boosting Out-of-Bag estimates</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-450">
<span id="calibre_link-451" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.11.4.3. 训练额外的弱学习器</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="docutils"><span class="calibre4">GradientBoostingRegressor</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="docutils"><span class="calibre4">GradientBoostingClassifier</span></code></a> 都支持设置参数 <code class="docutils"><span class="calibre4">warm_start=True</span></code> ，这样设置允许我们在已经训练的模型上面添加更多的估计器。</div>
</blockquote>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">_</span> <span class="calibre4">=</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">200</span><span class="calibre4">,</span> <span class="calibre4">warm_start</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>  <span class="calibre4"># set warm_start and new nr of trees</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">_</span> <span class="calibre4">=</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span> <span class="calibre4"># fit additional 100 trees to est</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_error</span><span class="calibre4">(</span><span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">))</span>    
<span class="calibre4">3.84...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-452">
<span id="calibre_link-453" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.11.4.4. 控制树的大小</h3>
<p class="calibre2">回归树基学习器的大小定义了可以被梯度提升模型捕捉到的变量（即特征）相互作用（即多个特征共同对预测产生影响）的程度。
通常一棵深度为 <code class="docutils"><span class="calibre4">h</span></code> 的树能捕获到秩为  <code class="docutils"><span class="calibre4">h</span></code> 的相互作用。这里有两种控制单棵回归树大小的方法。</p>
<p class="calibre10">如果你指定 <code class="docutils"><span class="calibre4">max_depth=h</span></code> ，那么将会产生一个深度为 <code class="docutils"><span class="calibre4">h</span></code> 的完全二叉树。这棵树将会有（至多） <code class="docutils"><span class="calibre4">2**h</span></code> 个叶子节点和 <code class="docutils"><span class="calibre4">2**h</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code> 个切分节点。</p>
<p class="calibre10">另外，你能通过参数 <code class="docutils"><span class="calibre4">max_leaf_nodes</span></code> 指定叶子节点的数量来控制树的大小。在这种情况下，树将会使用最优优先搜索来生成，这种搜索方式是通过每次选取对不纯度提升最大的节点来展开。一棵 <code class="docutils"><span class="calibre4">max_leaf_nodes=k</span></code> 的树拥有 <code class="docutils"><span class="calibre4">k</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code> 个切分节点，因此可以模拟秩最高达到 <code class="docutils"><span class="calibre4">max_leaf_nodes</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code> 的相互作用（即 <code class="docutils"><span class="calibre4">max_leaf_nodes</span> <span class="calibre4">-</span> <span class="calibre4">1</span></code> 个特征共同决定预测值）。</p>
<p class="calibre10">我们发现 <code class="docutils"><span class="calibre4">max_leaf_nodes=k</span></code> 可以给出与 <code class="docutils"><span class="calibre4">max_depth=k-1</span></code> 品质相当的结果，但是其训练速度明显更快，同时也会以多一点的训练误差作为代价。参数 <code class="docutils"><span class="calibre4">max_leaf_nodes</span></code> 对应于文章 <a class="calibre3 pcalibre" href="#calibre_link-202" id="calibre_link-211">[F2001]</a> 中梯度提升章节中的变量 <code class="docutils"><span class="calibre4">J</span></code> ，同时与 R 语言的 gbm 包的参数 <code class="docutils"><span class="calibre4">interaction.depth</span></code> 相关，两者间的关系是 <code class="docutils"><span class="calibre4">max_leaf_nodes</span> <span class="calibre4">==</span> <span class="calibre4">interaction.depth</span> <span class="calibre4">+</span> <span class="calibre4">1</span></code> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-454">
<h3 class="sigil_not_in_toc1">1.11.4.5. Mathematical formulation（数学公式）</h3>
<p class="calibre2">GBRT 可以认为是以下形式的可加模型:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000337.jpg" alt="F(x) = \sum_{m=1}^{M} \gamma_m h_m(x)" class="math" /></p>
</div>
</div>
</blockquote>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000252.jpg" alt="h_m(x)" /> 是基本函数,在提升算法场景中它通常被称作 <em class="calibre13">weak learners</em> 。梯度树提升算法（Gradient Tree Boosting）使用固定大小
的 <a class="calibre3 pcalibre" href="tree.html#tree"><span class="calibre4">decision trees</span></a> 作为弱分类器,决策树本身拥有的一些特性使它能够在提升过程中变得有价值,
即处理混合类型数据以及构建具有复杂功能模型的能力.</p>
<p class="calibre10">与其他提升算法类似， GBRT 利用前向分步算法思想构建加法模型:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000361.jpg" alt="F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)" class="math" /></p>
</div>
</div>
</blockquote>
<p class="calibre10">在每一个阶段中，基于当前模型 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000250.jpg" alt="F_{m-1}" /> 和拟合函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000395.jpg" alt="F_{m-1}(x_i)" /> 选择合适的决策树函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000252.jpg" alt="h_m(x)" /> ,从而最小化损失函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> 。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000318.jpg" alt="F_m(x) = F_{m-1}(x) + \arg\min_{h} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) - h(x))" class="math" /></p>
</div>
</div>
</blockquote>
<p class="calibre10">初始模型 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000219.jpg" alt="F_{0}" /> 是问题的具体,对于最小二乘回归,通常选择目标值的平均值.</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">初始化模型也能够通过 <code class="docutils"><span class="calibre4">init</span></code> 参数来指定，但传递的对象需要实现 <code class="docutils"><span class="calibre4">fit</span></code> 和 <code class="docutils"><span class="calibre4">predict</span></code> 函数。</p>
</div>
<p class="calibre10">梯度提升（Gradient Boosting）尝试通过最速下降法以数字方式解决这个最小化问题.最速下降方向是在当前模型 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000250.jpg" alt="F_{m-1}" /> 下的损失函数的负梯度方向，其中模型 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000250.jpg" alt="F_{m-1}" /> 可以计算任何可微损失函数:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000827.jpg" alt="F_m(x) = F_{m-1}(x) + \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i, F_{m-1}(x_i))" class="math" /></p>
</div>
</div>
</blockquote>
<p class="calibre10">其中步长 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000216.jpg" alt="\gamma_m" /> 通过如下方式线性搜索获得:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000341.jpg" alt="\gamma_m = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i) - \gamma \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)})" class="math" /></p>
</div>
</div>
</blockquote>
<p class="calibre10">该算法处理分类和回归问题不同之处在于具体损失函数的使用。</p>
<div class="toctree-wrapper" id="calibre_link-455">
<span id="calibre_link-201" class="calibre4"></span><h4 class="sigil_not_in_toc1">1.11.4.5.1. Loss Functions（损失函数）</h4>
<p class="calibre2">以下是目前支持的损失函数,具体损失函数可以通过参数 <code class="docutils"><span class="calibre4">loss</span></code> 指定:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">回归 (Regression)<ul class="calibre7">
<li class="toctree-l">Least squares ( <code class="docutils"><span class="calibre4">'ls'</span></code> ): 由于其优越的计算性能,该损失函数成为回归算法中的自然选择。
初始模型 （校对者注：即损失函数的初始值，下同） 通过目标值的均值给出。</li>
<li class="toctree-l">Least absolute deviation ( <code class="docutils"><span class="calibre4">'lad'</span></code> ): 回归中具有鲁棒性的损失函数,初始模型通过目
标值的中值给出。</li>
<li class="toctree-l">Huber ( <code class="docutils"><span class="calibre4">'huber'</span></code> ): 回归中另一个具有鲁棒性的损失函数,它是最小二乘和最小绝对偏差两者的结合.
其利用 <code class="docutils"><span class="calibre4">alpha</span></code> 来控制模型对于异常点的敏感度(详细介绍请参考 <a class="calibre3 pcalibre" href="#calibre_link-202" id="calibre_link-212">[F2001]</a>).</li>
<li class="toctree-l">Quantile ( <code class="docutils"><span class="calibre4">'quantile'</span></code> ): 分位数回归损失函数.用 <code class="docutils"><span class="calibre4">0</span> <span class="calibre4">&lt;</span> <span class="calibre4">alpha</span> <span class="calibre4">&lt;</span> <span class="calibre4">1</span></code> 来指定分位数这个损
失函数可以用来产生预测间隔。（详见 <a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"><span class="calibre4">Prediction Intervals for Gradient Boosting Regression</span></a> ）。</li>
</ul>
</li>
<li class="toctree-l">分类 (Classification)<ul class="calibre7">
<li class="toctree-l">Binomial deviance (<code class="docutils"><span class="calibre4">'deviance'</span></code>): 对于二分类问题(提供概率估计)即负的二项 log 似然损失函数。模型以 log 的比值比来初始化。</li>
<li class="toctree-l">Multinomial deviance (<code class="docutils"><span class="calibre4">'deviance'</span></code>): 对于多分类问题的负的多项log似然损失函数具有 <code class="docutils"><span class="calibre4">n_classes</span></code> 个互斥的类。提供概率估计。
初始模型由每个类的先验概率给出.在每一次迭代中 <code class="docutils"><span class="calibre4">n_classes</span></code> 回归树被构建,这使得 GBRT 在处理多类别数据集时相当低效。</li>
<li class="toctree-l">Exponential loss (<code class="docutils"><span class="calibre4">'exponential'</span></code>): 与 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="docutils"><span class="calibre4">AdaBoostClassifier</span></code></a> 具有相同的损失函数。与 <code class="docutils"><span class="calibre4">'deviance'</span></code> 相比，对被错误标记的样本的鲁棒性较差，仅用于在二分类问题。</li>
</ul>
</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-456">
<h3 class="sigil_not_in_toc1">1.11.4.6. Regularization（正则化）</h3>
<div class="toctree-wrapper" id="calibre_link-457">
<span id="calibre_link-458" class="calibre4"></span><h4 class="sigil_not_in_toc1">1.11.4.6.1. 收缩率 (Shrinkage)</h4>
<p class="calibre2"><a class="calibre3 pcalibre" href="#calibre_link-202" id="calibre_link-213">[F2001]</a> 提出一个简单的正则化策略,通过一个因子 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" /> 来衡量每个弱分类器对于最终结果的贡献:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000093.jpg" alt="F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)" class="math" /></p>
</div>
<p class="calibre10">参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" /> 由于它可以控制梯度下降的步长, 因此也叫作 <strong class="calibre14">learning rate</strong> ，它可以通过 <code class="docutils"><span class="calibre4">learning_rate</span></code> 参数来设置.</p>
<p class="calibre10">在训练一定数量的弱分类器时,参数 <code class="docutils"><span class="calibre4">learning_rate</span></code> 和参数 <code class="docutils"><span class="calibre4">n_estimators</span></code> 之间有很强的制约关系。
较小的 <code class="docutils"><span class="calibre4">learning_rate</span></code> 需要大量的弱分类器才能维持训练误差的稳定。经验表明数值较小的 <code class="docutils"><span class="calibre4">learning_rate</span></code>
将会得到更好的测试误差。 <a class="calibre3 pcalibre" href="#calibre_link-203" id="calibre_link-215">[HTF2009]</a> 推荐把 <code class="docutils"><span class="calibre4">learning_rate</span></code> 设置为一个较小的常数
(例如: <code class="docutils"><span class="calibre4">learning_rate</span> <span class="calibre4">&lt;=</span> <span class="calibre4">0.1</span></code> )同时通过提前停止策略来选择合适的 <code class="docutils"><span class="calibre4">n_estimators</span></code> .
有关 <code class="docutils"><span class="calibre4">learning_rate</span></code> 和 <code class="docutils"><span class="calibre4">n_estimators</span></code> 更详细的讨论可以参考 <a class="calibre3 pcalibre" href="#calibre_link-204" id="calibre_link-216">[R2007]</a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-459">
<h4 class="sigil_not_in_toc1">1.11.4.6.2. 子采样 (Subsampling)</h4>
<p class="calibre2"><a class="calibre3 pcalibre" href="#calibre_link-205" id="calibre_link-214">[F1999]</a> 提出了随机梯度提升,这种方法将梯度提升（gradient boosting）和 bootstrap averaging(bagging) 相结合。在每次迭代中,基分类器是通过抽取所有可利用训练集中一小部分的 <code class="docutils"><span class="calibre4">subsample</span></code> 训练得到的子样本采用无放回的方式采样。 <code class="docutils"><span class="calibre4">subsample</span></code> 参数的值一般设置为 0.5 。</p>
<p class="calibre10">下图表明了收缩与否和子采样对于模型拟合好坏的影响。我们可以明显看到指定收缩率比没有收缩拥有更好的表现。而将子采样和收缩率相结合能进一步的提高模型的准确率。相反，使用子采样而不使用收缩的结果十分糟糕。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000723.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">另一个减少方差的策略是特征子采样,这种方法类似于 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">RandomForestClassifier</span></code></a> 中的随机分割。子采样的特征数可以通过参数 <code class="docutils"><span class="calibre4">max_features</span></code> 来控制。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">采用一个较小的 <code class="docutils"><span class="calibre4">max_features</span></code> 值能大大缩减模型的训练时间。</p>
</div>
<p class="calibre10">随机梯度提升允许计算测试偏差的袋外估计值（Out-of-bag），方法是计算那些不在自助采样之内的样本偏差的改进。这个改进保存在属性 <code class="docutils"><span class="calibre4">oob_improvement_</span></code> 中 <code class="docutils"><span class="calibre4">oob_improvement_[i]</span></code> 如果将第 i 步添加到当前预测中，则可以改善 OOB 样本的损失。袋外估计可以使用在模型选择中，例如决定最优迭代次数。 OOB 估计通常都很悲观,因此我们推荐使用交叉验证来代替它，而当交叉验证太耗时时我们就只能使用 OOB 了。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py"><span class="calibre4">Gradient Boosting regularization</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="calibre4">Gradient Boosting Out-of-Bag estimates</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py"><span class="calibre4">OOB Errors for Random Forests</span></a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-460">
<h3 class="sigil_not_in_toc1">1.11.4.7. Interpretation（解释性）</h3>
<p class="calibre2">通过简单地可视化树结构可以很容易地解释单个决策树,然而对于梯度提升模型来说,一般拥有数百棵/种回归树，将每一棵树都可视化来解释整个模型是很困难的。幸运的是，有很多关于总结和解释梯度提升模型的技术。</p>
<div class="toctree-wrapper" id="calibre_link-461">
<h4 class="sigil_not_in_toc1">1.11.4.7.1. Feature importance（特征重要性）</h4>
<p class="calibre2">通常情况下每个特征对于预测目标的影响是不同的.在很多情形下大多数特征和预测结果是无关的。当解释一个模型时，第一个问题通常是：这些重要的特征是什么？他们如何在预测目标方面产生积极的影响的？</p>
<p class="calibre10">单个决策树本质上是通过选择最佳切分点来进行特征选择.这个信息可以用来评定每个特征的重要性。基本思想是：在树的分割点中使用的特征越频繁，特征越重要。 这个特征重要性的概念可以通过简单地平均每棵树的特征重要性来扩展到决策树集合。（详见 <a class="calibre3 pcalibre" href="#calibre_link-206"><span class="calibre4">特征重要性评估</span></a> ）。</p>
<p class="calibre10">对于一个训练好的梯度提升模型，其特征重要性分数可以通过属性 <code class="docutils"><span class="calibre4">feature_importances_</span></code> 查看:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_hastie_10_2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">GradientBoostingClassifier</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_hastie_10_2</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">GradientBoostingClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">learning_rate</span><span class="calibre4">=</span><span class="calibre4">1.0</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">feature_importances_</span>  
<span class="calibre4">array([ 0.11,  0.1 ,  0.11,  ...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="calibre4">Gradient Boosting regression</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-462">
<span id="calibre_link-463" class="calibre4"></span><h4 class="sigil_not_in_toc1">1.11.4.7.2. Partial dependence（部分依赖）</h4>
<p class="calibre2">部分依赖图（PDP）展示了目标响应和一系列目标特征的依赖关系，同时边缘化了其他所有特征值（候选特征）。
直觉上，我们可以将部分依赖解释为作为目标特征函数 <a class="calibre3 pcalibre" href="#calibre_link-207" id="calibre_link-210">[2]</a> 的预期目标响应 <a class="calibre3 pcalibre" href="#calibre_link-208" id="calibre_link-209">[1]</a> 。</p>
<p class="calibre10">由于人类感知能力的限制，目标特征的设置必须小一点(通常是1到2)，因此目标特征通常在最重要的特征中选择。</p>
<p class="calibre10">下图展示了加州住房数据集的四个单向和一个双向部分依赖图:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_partial_dependence.html"></a>
</div>
<p class="calibre10">单向 PDPs 告诉我们目标响应和目标特征的相互影响(例如：线性或者非线性)。上图中的左上图展示了一个地区的中等收入对中等房价的影响。我们可以清楚的看到两者之间是线性相关的。</p>
<p class="calibre10">具有两个目标特征的 PDPs 显示这两个特征之间的相互影响。例如：上图中两个变量的 PDP 展示了房价中位数与房屋年龄和每户平均入住人数之间的依赖关系。我们能清楚的看到这两个特征之间的影响：对于每户入住均值而言,当其值大于 2 时，房价与房屋年龄几乎是相对独立的，而其值小于 2 的时，房价对房屋年龄的依赖性就会很强。</p>
<p class="calibre10">模型 <code class="docutils"><span class="calibre4">partial_dependence</span></code> 提供了一个便捷的函数 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html#sklearn.ensemble.partial_dependence.plot_partial_dependence" title="sklearn.ensemble.partial_dependence.plot_partial_dependence"><code class="docutils"><span class="calibre4">plot_partial_dependence</span></code></a> 来产生单向或双向部分依赖图。在下图的例子中我们展示如何创建一个部分依赖的网格图：特征值介于 <code class="docutils"><span class="calibre4">0</span></code> 和 <code class="docutils"><span class="calibre4">1</span></code> 的两个单向依赖 PDPs 和一个在两个特征间的双向 PDPs:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_hastie_10_2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">GradientBoostingClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble.partial_dependence</span> <span class="calibre4">import</span> <span class="calibre4">plot_partial_dependence</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_hastie_10_2</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">GradientBoostingClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">learning_rate</span><span class="calibre4">=</span><span class="calibre4">1.0</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">features</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">)]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">fig</span><span class="calibre4">,</span> <span class="calibre4">axs</span> <span class="calibre4">=</span> <span class="calibre4">plot_partial_dependence</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">features</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<p class="calibre10">对于多类别的模型，你需要通过 <code class="docutils"><span class="calibre4">label</span></code> 参数设置类别标签来创建 PDPs:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mc_clf</span> <span class="calibre4">=</span> <span class="calibre4">GradientBoostingClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">features</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">fig</span><span class="calibre4">,</span> <span class="calibre4">axs</span> <span class="calibre4">=</span> <span class="calibre4">plot_partial_dependence</span><span class="calibre4">(</span><span class="calibre4">mc_clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">features</span><span class="calibre4">,</span> <span class="calibre4">label</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<p class="calibre10">如果你需要部分依赖函数的原始值而不是图，你可以调用 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.partial_dependence.partial_dependence.html#sklearn.ensemble.partial_dependence.partial_dependence" title="sklearn.ensemble.partial_dependence.partial_dependence"><code class="docutils"><span class="calibre4">partial_dependence</span></code></a> 函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble.partial_dependence</span> <span class="calibre4">import</span> <span class="calibre4">partial_dependence</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pdp</span><span class="calibre4">,</span> <span class="calibre4">axes</span> <span class="calibre4">=</span> <span class="calibre4">partial_dependence</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">X</span><span class="calibre4">=</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pdp</span>  
<span class="calibre4">array([[ 2.46643157,  2.46643157, ...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">axes</span>  
<span class="calibre4">[array([-1.62497054, -1.59201391, ...</span>
</pre>
</div>
</div>
<p class="calibre10">该函数允许通过 <code class="docutils"><span class="calibre4">grid</span></code> 参数指定应该被评估的部分依赖函数的的目标特征值或可以十分便利地通过设置 <code class="docutils"><span class="calibre4">X</span></code> 参数从而在训练数据中自动创建 <code class="docutils"><span class="calibre4">grid</span></code> 。如果 <code class="docutils"><span class="calibre4">X</span></code> 被给出，函数返回的 <code class="docutils"><span class="calibre4">axes</span></code> 为每个目标特征提供轴。</p>
<p class="calibre10">对于 <code class="docutils"><span class="calibre4">grid</span></code> 中的每一个 ‘目标’ 特征值，部分依赖函数需要边缘化一棵树中所有候选特征的可能值的预测。
在决策树中，这个函数可以在不参考训练数据的情况下被高效的评估，对于每一网格点执行加权遍历:
如果切分点包含 ‘目标’ 特征，遍历其相关的左分支或相关的右分支,否则就遍历两个分支。每一个分支将被通过进入该分支的训练样本的占比加权，
最后，部分依赖通过所有访问的叶节点的权重的平均值给出。组合树（tree ensembles）的整体结果，需要对每棵树的结果再次平均得到。</p>
<p class="calibre10">注解 (Footnotes)</p>
<table class="docutils1" frame="void" id="calibre_link-208" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-209">[1]</a></td>
<td class="label1">对于损失函数为deviance的分类问题，其目标响应为 logit(p) 。</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-207" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-210">[2]</a></td>
<td class="label1">更精确的来说，这里指在产生初始化模型后，对于目标响应的期望；部分依赖图并不包括 <code class="docutils"><span class="calibre4">init</span></code> 模型。</td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_partial_dependence.html#sphx-glr-auto-examples-ensemble-plot-partial-dependence-py"><span class="calibre4">Partial Dependence Plots</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<table class="docutils1" frame="void" id="calibre_link-202" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[F2001]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-211">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-212">2</a>, <a class="calibre3 pcalibre" href="#calibre_link-213">3</a>)</em> J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”,
The Annals of Statistics, Vol. 29, No. 5, 2001.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-205" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-214">[F1999]</a></td>
<td class="label1"><ol class="arabic" start="10">
<li class="toctree-l">Friedman, “Stochastic Gradient Boosting”, 1999</li>
</ol>
</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-203" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-215">[HTF2009]</a></td>
<td class="label1"><ol class="arabic" start="20">
<li class="toctree-l">Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.</li>
</ol>
</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-204" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-216">[R2007]</a></td>
<td class="label1"><ol class="arabic" start="7">
<li class="toctree-l">Ridgeway, “Generalized Boosted Models: A guide to the gbm package”, 2007</li>
</ol>
</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-464">
<span id="calibre_link-465" class="calibre4"></span><h2 class="sigil_not_in_toc">1.11.5. Voting Classifier（投票分类器）</h2>
<p class="calibre2"><code class="docutils"><span class="calibre4">VotingClassifier</span></code> （投票分类器）的原理是结合了多个不同的机器学习分类器,并且采用多数表决（majority vote）（硬投票） 或者平均预测概率（软投票）的方式来预测分类标签。
这样的分类器可以用于一组同样表现良好的模型,以便平衡它们各自的弱点。</p>
<div class="toctree-wrapper" id="calibre_link-466">
<h3 class="sigil_not_in_toc1">1.11.5.1. 多数类标签 (又称为 多数/硬投票)</h3>
<p class="calibre2">在多数投票中，对于每个特定样本的预测类别标签是所有单独分类器预测的类别标签中票数占据多数（模式）的类别标签。</p>
<p class="calibre10">例如，如果给定样本的预测是</p>
<ul class="calibre6">
<li class="toctree-l">classifier 1 -&gt; class 1</li>
<li class="toctree-l">classifier 2 -&gt; class 1</li>
<li class="toctree-l">classifier 3 -&gt; class 2</li>
</ul>
<p class="calibre10">类别 1 占据多数,通过 <code class="docutils"><span class="calibre4">voting='hard'</span></code> 参数设置投票分类器为多数表决方式，会得到该样本的预测结果是类别 1 。</p>
<p class="calibre10">在平局的情况下,投票分类器（VotingClassifier）将根据升序排序顺序选择类标签。
例如，场景如下:</p>
<ul class="calibre6">
<li class="toctree-l">classifier 1 -&gt; class 2</li>
<li class="toctree-l">classifier 2 -&gt; class 1</li>
</ul>
<p class="calibre10">这种情况下， class 1 将会被指定为该样本的类标签。</p>
<div class="toctree-wrapper" id="calibre_link-467">
<h4 class="sigil_not_in_toc1">1.11.5.1.1. 用法</h4>
<p class="calibre2">以下示例显示如何训练多数规则分类器：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">LogisticRegression</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.naive_bayes</span> <span class="calibre4">import</span> <span class="calibre4">GaussianNB</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">RandomForestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">VotingClassifier</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:,</span> <span class="calibre4">1</span><span class="calibre4">:</span><span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf1</span> <span class="calibre4">=</span> <span class="calibre4">LogisticRegression</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf3</span> <span class="calibre4">=</span> <span class="calibre4">GaussianNB</span><span class="calibre4">()</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">VotingClassifier</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'lr'</span><span class="calibre4">,</span> <span class="calibre4">clf1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'rf'</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'gnb'</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">)],</span> <span class="calibre4">voting</span><span class="calibre4">=</span><span class="calibre4">'hard'</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">label</span> <span class="calibre4">in</span> <span class="calibre4">zip</span><span class="calibre4">([</span><span class="calibre4">clf1</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">,</span> <span class="calibre4">eclf</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">'Logistic Regression'</span><span class="calibre4">,</span> <span class="calibre4">'Random Forest'</span><span class="calibre4">,</span> <span class="calibre4">'naive Bayes'</span><span class="calibre4">,</span> <span class="calibre4">'Ensemble'</span><span class="calibre4">]):</span>
<span class="calibre4">... </span>    <span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'accuracy'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Accuracy: </span><span class="calibre4">%0.2f</span><span class="calibre4"> (+/- </span><span class="calibre4">%0.2f</span><span class="calibre4">) [</span><span class="calibre4">%s</span><span class="calibre4">]"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">(),</span> <span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">std</span><span class="calibre4">(),</span> <span class="calibre4">label</span><span class="calibre4">))</span>
<span class="calibre4">Accuracy: 0.90 (+/- 0.05) [Logistic Regression]</span>
<span class="calibre4">Accuracy: 0.93 (+/- 0.05) [Random Forest]</span>
<span class="calibre4">Accuracy: 0.91 (+/- 0.04) [naive Bayes]</span>
<span class="calibre4">Accuracy: 0.95 (+/- 0.05) [Ensemble]</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-468">
<h3 class="sigil_not_in_toc1">1.11.5.2. 加权平均概率 （软投票）</h3>
<p class="calibre2">与多数投票（硬投票）相比，软投票将类别标签返回为预测概率之和的 argmax 。</p>
<p class="calibre10">具体的权重可以通过权重参数 <code class="docutils"><span class="calibre4">weights</span></code> 分配给每个分类器。当提供权重参数 <code class="docutils"><span class="calibre4">weights</span></code> 时，收集每个分类器的预测分类概率，
乘以分类器权重并取平均值。然后将具有最高平均概率的类别标签确定为最终类别标签。</p>
<p class="calibre10">为了用一个简单的例子来说明这一点，假设我们有 3 个分类器和一个 3 类分类问题，我们给所有分类器赋予相等的权重：w1 = 1,w2 = 1,w3 = 1 。</p>
<p class="calibre10">样本的加权平均概率计算如下：</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="34%" class="label"></col>
<col width="21%" class="label"></col>
<col width="21%" class="label"></col>
<col width="23%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">分类器</th>
<th class="head">类别 1</th>
<th class="head">类别 2</th>
<th class="head">类别 3</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">分类器 1</td>
<td class="label1">w1 * 0.2</td>
<td class="label1">w1 * 0.5</td>
<td class="label1">w1 * 0.3</td>
</tr>
<tr class="row-odd"><td class="label1">分类器 2</td>
<td class="label1">w2 * 0.6</td>
<td class="label1">w2 * 0.3</td>
<td class="label1">w2 * 0.1</td>
</tr>
<tr class="calibre23"><td class="label1">分类器 3</td>
<td class="label1">w3 * 0.3</td>
<td class="label1">w3 * 0.4</td>
<td class="label1">w3 * 0.3</td>
</tr>
<tr class="row-odd"><td class="label1">加权平均的结果</td>
<td class="label1">0.37</td>
<td class="label1">0.4</td>
<td class="label1">0.23</td>
</tr>
</tbody>
</table>
<p class="calibre10">这里可以看出，预测的类标签是 2，因为它具有最大的平均概率.</p>
<p class="calibre10">下边的示例程序说明了当软投票分类器（soft VotingClassifier）是基于线性支持向量机（linear SVM）、决策树（Decision Tree）、K 近邻（K-nearest）分类器时，决策域可能的变化情况:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.tree</span> <span class="calibre4">import</span> <span class="calibre4">DecisionTreeClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors</span> <span class="calibre4">import</span> <span class="calibre4">KNeighborsClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">itertools</span> <span class="calibre4">import</span> <span class="calibre4">product</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">VotingClassifier</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Loading some example data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:,</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span><span class="calibre4">2</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Training classifiers</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf1</span> <span class="calibre4">=</span> <span class="calibre4">DecisionTreeClassifier</span><span class="calibre4">(</span><span class="calibre4">max_depth</span><span class="calibre4">=</span><span class="calibre4">4</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">KNeighborsClassifier</span><span class="calibre4">(</span><span class="calibre4">n_neighbors</span><span class="calibre4">=</span><span class="calibre4">7</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf3</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">,</span> <span class="calibre4">probability</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">VotingClassifier</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'dt'</span><span class="calibre4">,</span> <span class="calibre4">clf1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'knn'</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'svc'</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">)],</span> <span class="calibre4">voting</span><span class="calibre4">=</span><span class="calibre4">'soft'</span><span class="calibre4">,</span> <span class="calibre4">weights</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span><span class="calibre4">1</span><span class="calibre4">,</span><span class="calibre4">2</span><span class="calibre4">])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf1</span> <span class="calibre4">=</span> <span class="calibre4">clf1</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">clf2</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf3</span> <span class="calibre4">=</span> <span class="calibre4">clf3</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">eclf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span><span class="calibre4">y</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_voting_decision_regions.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_voting_decision_regions_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000033.jpg" class="calibre31" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-469">
<h3 class="sigil_not_in_toc1">1.11.5.3. 投票分类器（VotingClassifier）在网格搜索（GridSearch）应用</h3>
<p class="calibre2">为了调整每个估计器的超参数， <cite class="calibre13">VotingClassifier</cite> 也可以和 <cite class="calibre13">GridSearch</cite> 一起使用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf1</span> <span class="calibre4">=</span> <span class="calibre4">LogisticRegression</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf3</span> <span class="calibre4">=</span> <span class="calibre4">GaussianNB</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">VotingClassifier</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'lr'</span><span class="calibre4">,</span> <span class="calibre4">clf1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'rf'</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'gnb'</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">)],</span> <span class="calibre4">voting</span><span class="calibre4">=</span><span class="calibre4">'soft'</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">params</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'lr__C'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">1.0</span><span class="calibre4">,</span> <span class="calibre4">100.0</span><span class="calibre4">],</span> <span class="calibre4">'rf__n_estimators'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">20</span><span class="calibre4">,</span> <span class="calibre4">200</span><span class="calibre4">],}</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">grid</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">estimator</span><span class="calibre4">=</span><span class="calibre4">eclf</span><span class="calibre4">,</span> <span class="calibre4">param_grid</span><span class="calibre4">=</span><span class="calibre4">params</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">grid</span> <span class="calibre4">=</span> <span class="calibre4">grid</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-470">
<h4 class="sigil_not_in_toc1">1.11.5.3.1. 用法</h4>
<p class="calibre2">为了通过预测的类别概率来预测类别标签(投票分类器中的 scikit-learn estimators 必须支持 <code class="docutils"><span class="calibre4">predict_proba</span></code> 方法):</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">VotingClassifier</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'lr'</span><span class="calibre4">,</span> <span class="calibre4">clf1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'rf'</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'gnb'</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">)],</span> <span class="calibre4">voting</span><span class="calibre4">=</span><span class="calibre4">'soft'</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">可选地，也可以为单个分类器提供权重:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">eclf</span> <span class="calibre4">=</span> <span class="calibre4">VotingClassifier</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'lr'</span><span class="calibre4">,</span> <span class="calibre4">clf1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'rf'</span><span class="calibre4">,</span> <span class="calibre4">clf2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'gnb'</span><span class="calibre4">,</span> <span class="calibre4">clf3</span><span class="calibre4">)],</span> <span class="calibre4">voting</span><span class="calibre4">=</span><span class="calibre4">'soft'</span><span class="calibre4">,</span> <span class="calibre4">weights</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span><span class="calibre4">5</span><span class="calibre4">,</span><span class="calibre4">1</span><span class="calibre4">])</span>
</pre>
</div>
</div>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-217">
<span id="calibre_link-471" class="calibre4"></span><h1 class="calibre5">1.12. 多类和多标签算法</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@溪流-十四号</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@大魔王飞仙</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@v</a><br class="calibre9" />   
    </div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">All classifiers in scikit-learn do multiclass classification
out-of-the-box. You don’t need to use the <a class="calibre3 pcalibre" href="classes.html#module-sklearn.multiclass" title="sklearn.multiclass"><code class="docutils"><span class="calibre4">sklearn.multiclass</span></code></a> module
unless you want to experiment with different multiclass strategies.</p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.multiclass" title="sklearn.multiclass"><code class="docutils"><span class="calibre4">sklearn.multiclass</span></code></a> 模块采用了 <em class="calibre13">元评估器</em> ，通过把``多类`` 和 <code class="docutils"><span class="calibre4">多标签</span></code> 分类问题分解为
二元分类问题去解决。这同样适用于多目标回归问题。</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Multiclass classification</strong> <strong class="calibre14">多类分类</strong> 意味着一个分类任务需要对多于两个类的数据进行分类。比如，对一系列的橘子，</li>
</ul>
<p class="calibre10">苹果或者梨的图片进行分类。多类分类假设每一个样本有且仅有一个标签：一个水果可以被归类为苹果，也可以
是梨，但不能同时被归类为两类。</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Multilabel classification</strong> <strong class="calibre14">多标签分类</strong> 给每一个样本分配一系列标签。这可以被认为是预测不</li>
</ul>
<p class="calibre10">相互排斥的数据点的属性，例如与文档类型相关的主题。一个文本可以归类为任意类别，例如可以同时为政治、金融、
教育相关或者不属于以上任何类别。</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Multioutput regression</strong> <strong class="calibre14">多输出分类</strong> 为每个样本分配一组目标值。这可以认为是预测每一个样本的多个属性，</li>
</ul>
<p class="calibre10">比如说一个具体地点的风的方向和大小。</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Multioutput-multiclass classification</strong> and <strong class="calibre14">multi-task classification</strong> <a href="#calibre_link-218" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-472">**</span></a>多输出-多类分类和</li>
</ul>
<dl class="calibre10">
<dt class="calibre18">多任务分类** 意味着单个的评估器要解决多个联合的分类任务。这是只考虑二分类的 multi-label classification</dt>
<dd class="calibre19"><blockquote class="first1">
<div class="toctree-wrapper">和 multi-class classification 任务的推广。 <em class="calibre13">此类问题输出的格式是一个二维数组或者一个稀疏矩阵。</em></div>
</blockquote>
<p class="calibre10">每个输出变量的标签集合可以是各不相同的。比如说，一个样本可以将“梨”作为一个输出变量的值，这个输出变
量在一个含有“梨”、“苹果”等水果种类的有限集合中取可能的值；将“蓝色”或者“绿色”作为第二个输出变量的值，
这个输出变量在一个含有“绿色”、“红色”、“蓝色”等颜色种类的有限集合中取可能的值…</p>
<p class="last">这意味着任何处理 multi-output multiclass or multi-task classification 任务的分类器，在特殊的
情况下支持 multi-label classification 任务。Multi-task classification 与具有不同模型公式
的 multi-output classification 相似。详细情况请查阅相关的分类器的文档。</p>
</dd>
</dl>
<p class="calibre10">所有的 scikit-learn 分类器都能处理 multiclass classification 任务，
但是 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.multiclass" title="sklearn.multiclass"><code class="docutils"><span class="calibre4">sklearn.multiclass</span></code></a> 提供的元评估器允许改变在处理超过两类数据时的方式，因为这会对分类器的性能产生影响
（无论是在泛化误差或者所需要的计算资源方面）</p>
<p class="calibre10">下面是按照 scikit-learn 策略分组的分类器的总结，如果你使用其中的一个，则不需要此类中的元评估器，除非你想要自定义的多分类方式。</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">固有的多类分类器:</strong><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB" title="sklearn.naive_bayes.BernoulliNB"><code class="docutils"><span class="calibre4">sklearn.naive_bayes.BernoulliNB</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.DecisionTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier" title="sklearn.tree.ExtraTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.ExtraTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.ExtraTreesClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="docutils"><span class="calibre4">sklearn.naive_bayes.GaussianNB</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.KNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="docutils"><span class="calibre4">sklearn.semi_supervised.LabelPropagation</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="docutils"><span class="calibre4">sklearn.semi_supervised.LabelSpreading</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="docutils"><span class="calibre4">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">sklearn.svm.LinearSVC</span></code></a> (setting multi_class=”crammer_singer”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">sklearn.linear_model.LogisticRegression</span></code></a> (setting multi_class=”multinomial”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="docutils"><span class="calibre4">sklearn.linear_model.LogisticRegressionCV</span></code></a> (setting multi_class=”multinomial”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">sklearn.neural_network.MLPClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="docutils"><span class="calibre4">sklearn.neighbors.NearestCentroid</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="docutils"><span class="calibre4">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.RadiusNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code class="docutils"><span class="calibre4">sklearn.linear_model.RidgeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV" title="sklearn.linear_model.RidgeClassifierCV"><code class="docutils"><span class="calibre4">sklearn.linear_model.RidgeClassifierCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><strong class="calibre14">1对1的多类分类器:</strong><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="docutils"><span class="calibre4">sklearn.svm.NuSVC</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">sklearn.svm.SVC</span></code></a>.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code class="docutils"><span class="calibre4">sklearn.gaussian_process.GaussianProcessClassifier</span></code></a> (setting multi_class = “one_vs_one”)</li>
</ul>
</li>
<li class="toctree-l"><strong class="calibre14">1对多的多类分类器:</strong><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.GradientBoostingClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code class="docutils"><span class="calibre4">sklearn.gaussian_process.GaussianProcessClassifier</span></code></a> (setting multi_class = “one_vs_rest”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">sklearn.svm.LinearSVC</span></code></a> (setting multi_class=”ovr”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">sklearn.linear_model.LogisticRegression</span></code></a> (setting multi_class=”ovr”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="docutils"><span class="calibre4">sklearn.linear_model.LogisticRegressionCV</span></code></a> (setting multi_class=”ovr”)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">sklearn.linear_model.SGDClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="docutils"><span class="calibre4">sklearn.linear_model.Perceptron</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code class="docutils"><span class="calibre4">sklearn.linear_model.PassiveAggressiveClassifier</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><strong class="calibre14">支持多标签分类的分类器:</strong><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.DecisionTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier" title="sklearn.tree.ExtraTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.ExtraTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.ExtraTreesClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.KNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">sklearn.neural_network.MLPClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.RadiusNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV" title="sklearn.linear_model.RidgeClassifierCV"><code class="docutils"><span class="calibre4">sklearn.linear_model.RidgeClassifierCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><strong class="calibre14">支持多类-多输出分类的分类器:</strong><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.DecisionTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier" title="sklearn.tree.ExtraTreeClassifier"><code class="docutils"><span class="calibre4">sklearn.tree.ExtraTreeClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.ExtraTreesClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.KNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="docutils"><span class="calibre4">sklearn.neighbors.RadiusNeighborsClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestClassifier</span></code></a></li>
</ul>
</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">At present, no metric in <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="docutils"><span class="calibre4">sklearn.metrics</span></code></a>
supports the multioutput-multiclass classification task.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-473">
<h2 class="sigil_not_in_toc">1.12.1. 多标签分类格式</h2>
<p class="calibre2">在 multilabel learning 中，二元分类任务的合集表示为二进制数组：每一个样本是大小为 (n_samples, n_classes) 的二维数组中的一行二进制值，比如非0元素，表示为对应标签的
子集。 一个数组 <code class="docutils"><span class="calibre4">np.array([[1,</span> <span class="calibre4">0,</span> <span class="calibre4">0],</span> <span class="calibre4">[0,</span> <span class="calibre4">1,</span> <span class="calibre4">1],</span> <span class="calibre4">[0,</span> <span class="calibre4">0,</span> <span class="calibre4">0]])</span></code> 表示第一个样本属于第 0 个标签，第二个样本属于第一个和第二个标签，第三个样本不属于任何标签。</p>
<p class="calibre10">通过一系列的标签来产生多标签数据可能更为直观。 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer" title="sklearn.preprocessing.MultiLabelBinarizer"><code class="docutils"><span class="calibre4">MultiLabelBinarizer</span></code></a> 转换器可以用来在标签接口和格式指示器接口之间进行转换。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">MultiLabelBinarizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">MultiLabelBinarizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">array([[0, 0, 1, 1, 1],</span>
<span class="calibre4">       [0, 0, 1, 0, 0],</span>
<span class="calibre4">       [1, 1, 0, 1, 0],</span>
<span class="calibre4">       [1, 1, 1, 1, 1],</span>
<span class="calibre4">       [1, 1, 1, 0, 0]])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-474">
<span id="calibre_link-475" class="calibre4"></span><h2 class="sigil_not_in_toc">1.12.2. 1对其余</h2>
<p class="calibre2">这个方法也被称为 <strong class="calibre14">1对多</strong>, 在 <a class="calibre3 pcalibre" href="generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier" title="sklearn.multiclass.OneVsRestClassifier"><code class="docutils"><span class="calibre4">OneVsRestClassifier</span></code></a> 模块中执行。 这个方法在于每一个类都将用一个分类器进行拟合。
对于每一个分类器，该类将会和其他所有的类有所区别。除了它的计算效率之外 (只需要 <cite class="calibre13">n_classes</cite> 个分类器), 这种方法的优点是它具有可解释性。
因为每一个类都可以通过有且仅有一个分类器来代表，所以通过检查一个类相关的分类器就可以获得该类的信息。这是最常用的方法，也是一个合理的默认选择。</p>
<div class="toctree-wrapper" id="calibre_link-476">
<h3 class="sigil_not_in_toc1">1.12.2.1. 多类学习</h3>
<p class="calibre2">下面是一个使用 OvR 的一个例子：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multiclass</span> <span class="calibre4">import</span> <span class="calibre4">OneVsRestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">LinearSVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">OneVsRestClassifier</span><span class="calibre4">(</span><span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="calibre4">       1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="calibre4">       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-477">
<h3 class="sigil_not_in_toc1">1.12.2.2. 多标签学习</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier" title="sklearn.multiclass.OneVsRestClassifier"><code class="docutils"><span class="calibre4">OneVsRestClassifier</span></code></a> 1对其余分类器  也支持 multilabel classification.
要使用该功能，给分类器提供一个指示矩阵，比如 [i,j] 表示第i个样本中的第j个标签。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_multilabel.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_multilabel_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000366.jpg" class="calibre32" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_multilabel.html#sphx-glr-auto-examples-plot-multilabel-py"><span class="calibre4">Multilabel classification</span></a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-478">
<span id="calibre_link-479" class="calibre4"></span><h2 class="sigil_not_in_toc">1.12.3. 1对1</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier" title="sklearn.multiclass.OneVsOneClassifier"><code class="docutils"><span class="calibre4">OneVsOneClassifier</span></code></a> 1对1分类器 将会为每一对类别构造出一个分类器，在预测阶段，收到最多投票的类别将会被挑选出来。
当存在结时（两个类具有同样的票数的时候）， 1对1分类器会选择总分类置信度最高的类，其中总分类置信度是由下层的二元分类器
计算出的成对置信等级累加而成。</p>
<p class="calibre10">因为这需要训练出 <code class="docutils"><span class="calibre4">n_classes</span> <span class="calibre4">*</span> <span class="calibre4">(n_classes</span> <span class="calibre4">-</span> <span class="calibre4">1)</span> <span class="calibre4">/</span> <span class="calibre4">2</span></code> 个分类器,
由于复杂度为 O(n_classes^2)，这个方法通常比 one-vs-the-rest 慢。然而，这个方法也有优点，比如说是在没有很好的缩放 <code class="docutils"><span class="calibre4">n_samples</span></code> 数据的核方法中。
这是由于每个单独的学习问题只涉及一小部分数据，而 one-vs-the-rest 将会使用 <code class="docutils"><span class="calibre4">n_classes</span></code> 次完整的数据。</p>
<div class="toctree-wrapper" id="calibre_link-480">
<h3 class="sigil_not_in_toc1">1.12.3.1. 多类别学习</h3>
<p class="calibre2">Below is an example of multiclass learning using OvO:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multiclass</span> <span class="calibre4">import</span> <span class="calibre4">OneVsOneClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">LinearSVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">OneVsOneClassifier</span><span class="calibre4">(</span><span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="calibre4">       1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="calibre4">       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">“Pattern Recognition and Machine Learning. Springer”,
Christopher M. Bishop, page 183, (First Edition)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-481">
<span id="calibre_link-482" class="calibre4"></span><h2 class="sigil_not_in_toc">1.12.4. 误差校正输出代码</h2>
<p class="calibre2">基于Output-code的方法不同于 one-vs-the-rest 和 one-vs-one。使用这些方法，每一个类将会被映射到欧几里得空间，每一个维度上的值只能为0或者1。另一种解释它的方法是，每一个类被表示为二进制
码（一个 由0 和 1 组成的数组）。保存 location （位置）/ 每一个类的编码的矩阵被称为 code book。编码的大小是前面提到的欧几里得空间的纬度。直观上来说，每一个类应该使用一个唯一的编码，同时，好的 code book 应该能够优化分类的精度。
在实现上，我们使用随机产生的 code book，正如在 <a class="calibre3 pcalibre" href="#calibre_link-219" id="calibre_link-220">[3]</a> 提倡的方式，即使更加详尽的方法可能会在未来被加入其中。</p>
<p class="calibre10">在训练时，code book 每一位的二分类器将会被训练。在预测时，分类器将映射到类空间中选中的点的附近。</p>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.multiclass.OutputCodeClassifier.html#sklearn.multiclass.OutputCodeClassifier" title="sklearn.multiclass.OutputCodeClassifier"><code class="docutils"><span class="calibre4">OutputCodeClassifier</span></code></a>, <code class="docutils"><span class="calibre4">code_size</span></code> 属性允许用户设置将会用到的分类器的数量。
它是类别总数的百分比。</p>
<p class="calibre10">在 0 或 1 之中的一个数字会比 one-vs-the-rest 使用更少的分类器。理论上 <code class="docutils"><span class="calibre4">log2(n_classes)</span> <span class="calibre4">/</span> <span class="calibre4">n_classes</span></code> 足以明确表示每个类。然而在实际情况中，这也许会导致不太好的精确度，因为 <code class="docutils"><span class="calibre4">log2(n_classes)</span></code> 小于 n_classes.</p>
<p class="calibre10">比 1 大的数字比 one-vs-the-rest 需要更多的分类器。在这种情况下，一些分类器在理论上会纠正其他分类器的错误，因此命名为 “error-correcting” 。然而在实际上这通常不会发生，因为许多分类器的错误通常意义上来说是相关的。error-correcting output codes 和 bagging 有一个相似的作用效果。</p>
<div class="toctree-wrapper" id="calibre_link-483">
<h3 class="sigil_not_in_toc1">1.12.4.1. 多类别学习</h3>
<p class="calibre2">Below is an example of multiclass learning using Output-Codes:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multiclass</span> <span class="calibre4">import</span> <span class="calibre4">OutputCodeClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">LinearSVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">OutputCodeClassifier</span><span class="calibre4">(</span><span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">),</span>
<span class="calibre4">... </span>                           <span class="calibre4">code_size</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="calibre4">       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,</span>
<span class="calibre4">       1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,</span>
<span class="calibre4">       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,</span>
<span class="calibre4">       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">“Solving multiclass learning problems via error-correcting output codes”,
Dietterich T., Bakiri G.,
Journal of Artificial Intelligence Research 2,
1995.</li>
</ul>
<table class="docutils1" frame="void" id="calibre_link-219" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-220">[3]</a></td>
<td class="label1">“The error coding method and PICTs”,
James G., Hastie T.,
Journal of Computational and Graphical statistics 7,
1998.</td>
</tr>
</tbody>
</table>
<ul class="calibre6">
<li class="toctree-l">“The Elements of Statistical Learning”,
Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
2008.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-484">
<h2 class="sigil_not_in_toc">1.12.5. 多输出回归</h2>
<p class="calibre2">多输出回归支持 <code class="docutils"><span class="calibre4">MultiOutputRegressor</span></code> 可以被添加到任何回归器中。这个策略包括对每个目标拟合一个回归器。因为每一个目标可以被一个回归器精确地表示，通过检查对应的回归器，可以获取关于目标的信息。
因为&nbsp;<code class="docutils"><span class="calibre4">MultiOutputRegressor</span></code> 对于每一个目标可以训练出一个回归器，所以它无法利用目标之间的相关度信息。</p>
<p class="calibre10">以下是 multioutput regression（多输出回归）的示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_regression</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multioutput</span> <span class="calibre4">import</span> <span class="calibre4">MultiOutputRegressor</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">GradientBoostingRegressor</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">make_regression</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">n_targets</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">MultiOutputRegressor</span><span class="calibre4">(</span><span class="calibre4">GradientBoostingRegressor</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[-154.75474165, -147.03498585,  -50.03812219],</span>
<span class="calibre4">       [   7.12165031,    5.12914884,  -81.46081961],</span>
<span class="calibre4">       [-187.8948621 , -100.44373091,   13.88978285],</span>
<span class="calibre4">       [-141.62745778,   95.02891072, -191.48204257],</span>
<span class="calibre4">       [  97.03260883,  165.34867495,  139.52003279],</span>
<span class="calibre4">       [ 123.92529176,   21.25719016,   -7.84253   ],</span>
<span class="calibre4">       [-122.25193977,  -85.16443186, -107.12274212],</span>
<span class="calibre4">       [ -30.170388  ,  -94.80956739,   12.16979946],</span>
<span class="calibre4">       [ 140.72667194,  176.50941682,  -17.50447799],</span>
<span class="calibre4">       [ 149.37967282,  -81.15699552,   -5.72850319]])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-485">
<h2 class="sigil_not_in_toc">1.12.6. 多输出分类</h2>
<p class="calibre2">Multioutput classification 支持能够被添加到任何带有 <code class="docutils"><span class="calibre4">MultiOutputClassifier</span></code> 标志的分类器中. 这种方法为每一个目标训练一个分类器。
这就允许产生多目标变量分类器。这种类的目的是扩展评估器用于评估一系列目标函数 (f1,f2,f3…,fn) ，这些函数在一个单独的预测矩阵上进行训练以此来预测一系列的响应 (y1,y2,y3…,yn)。</p>
<p class="calibre10">下面是多输出分类的一个例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">make_classification</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multioutput</span> <span class="calibre4">import</span> <span class="calibre4">MultiOutputClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">RandomForestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.utils</span> <span class="calibre4">import</span> <span class="calibre4">shuffle</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y1</span> <span class="calibre4">=</span> <span class="calibre4">make_classification</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">n_features</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">n_informative</span><span class="calibre4">=</span><span class="calibre4">30</span><span class="calibre4">,</span> <span class="calibre4">n_classes</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y2</span> <span class="calibre4">=</span> <span class="calibre4">shuffle</span><span class="calibre4">(</span><span class="calibre4">y1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y3</span> <span class="calibre4">=</span> <span class="calibre4">shuffle</span><span class="calibre4">(</span><span class="calibre4">y1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">vstack</span><span class="calibre4">((</span><span class="calibre4">y1</span><span class="calibre4">,</span> <span class="calibre4">y2</span><span class="calibre4">,</span> <span class="calibre4">y3</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">n_samples</span><span class="calibre4">,</span> <span class="calibre4">n_features</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span> <span class="calibre4"># 10,100</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">n_outputs</span> <span class="calibre4">=</span> <span class="calibre4">Y</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span> <span class="calibre4"># 3</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">n_classes</span> <span class="calibre4">=</span> <span class="calibre4">3</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">forest</span> <span class="calibre4">=</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">(</span><span class="calibre4">n_estimators</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">multi_target_forest</span> <span class="calibre4">=</span> <span class="calibre4">MultiOutputClassifier</span><span class="calibre4">(</span><span class="calibre4">forest</span><span class="calibre4">,</span> <span class="calibre4">n_jobs</span><span class="calibre4">=-</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">multi_target_forest</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[2, 2, 0],</span>
<span class="calibre4">       [1, 2, 1],</span>
<span class="calibre4">       [2, 1, 0],</span>
<span class="calibre4">       [0, 0, 2],</span>
<span class="calibre4">       [0, 2, 1],</span>
<span class="calibre4">       [0, 0, 2],</span>
<span class="calibre4">       [1, 1, 0],</span>
<span class="calibre4">       [1, 1, 1],</span>
<span class="calibre4">       [0, 0, 2],</span>
<span class="calibre4">       [2, 0, 0]])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-486">
<h2 class="sigil_not_in_toc">1.12.7. 链式分类器</h2>
<p class="calibre2">Classifier chains (查看 <code class="docutils"><span class="calibre4">ClassifierChain</span></code>) 是一种集合多个二分类器为一个单独的多标签模型的方法，这种方法能够发掘目标之间的相关性信息。</p>
<p class="calibre10">对于有 N 个类的多标签分类问题，为 N 个二元分类器分配 0 到 N-1 之间的一个整数。这些整数定义了模型在 chain 中的顺序。将每个分类器拟合可用的训练数据与真实的类别标签，标签数字相对较小。</p>
<p class="calibre10">当进行预测时，真正的标签将无法使用。相反，每一个模型的预测结果将会传递给链上的下一个模型作为特征来进行使用。</p>
<p class="calibre10">很明显，链的顺序是十分重要的。链上的第一个模型没有关于其他标签的信息，而链上的最后一个模型将会具有所有其他标签的信息。
在一般情况下，我们并不知道链上模型最优的顺序，因此通常会使用许多随机的顺序，将他们的预测求平均。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<dl class="calibre10">
<dt class="calibre18">Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,</dt>
<dd class="calibre19">“Classifier Chains for Multi-label Classification”, 2009.</dd>
</dl>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-5">
<span id="calibre_link-487" class="calibre4"></span><h1 class="calibre5">1.13. 特征选择</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/yuezhao9210" class="calibre3 pcalibre">@yuezhao9210</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@BWM-蜜蜂</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@v</a><br class="calibre9" />     
    </div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.feature_selection" title="sklearn.feature_selection"><code class="docutils"><span class="calibre4">sklearn.feature_selection</span></code></a> 模块中的类可以用来对样本集进行 feature selection（特征选择）和 dimensionality reduction（降维），这将会提高估计器的准确度或者增强它们在高维数据集上的性能。</p>
<div class="toctree-wrapper" id="calibre_link-488">
<span id="calibre_link-489" class="calibre4"></span><h2 class="sigil_not_in_toc">1.13.1. 移除低方差特征</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold" title="sklearn.feature_selection.VarianceThreshold"><code class="docutils"><span class="calibre4">VarianceThreshold</span></code></a> 是特征选择的一个简单基本方法，它会移除所有那些方差不满足一些阈值的特征。默认情况下，它将会移除所有的零方差特征，即那些在所有的样本上的取值均不变的特征。</p>
<p class="calibre10">例如，假设我们有一个特征是布尔值的数据集，我们想要移除那些在整个数据集中特征值为0或者为1的比例超过80%的特征。布尔特征是伯努利（ Bernoulli ）随机变量，变量的方差为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000284.jpg" alt="\mathrm{Var}[X] = p(1 - p)" class="math" /></p>
</div>
<p class="calibre10">因此，我们可以使用阈值 <a href="#calibre_link-6" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-490">``</span></a>.8 * (1 - .8)``进行选择:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_selection</span> <span class="calibre4">import</span> <span class="calibre4">VarianceThreshold</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">sel</span> <span class="calibre4">=</span> <span class="calibre4">VarianceThreshold</span><span class="calibre4">(</span><span class="calibre4">threshold</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">.</span><span class="calibre4">8</span> <span class="calibre4">*</span> <span class="calibre4">(</span><span class="calibre4">1</span> <span class="calibre4">-</span> <span class="calibre4">.</span><span class="calibre4">8</span><span class="calibre4">)))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">sel</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[0, 1],</span>
<span class="calibre4">       [1, 0],</span>
<span class="calibre4">       [0, 0],</span>
<span class="calibre4">       [1, 1],</span>
<span class="calibre4">       [1, 0],</span>
<span class="calibre4">       [1, 1]])</span>
</pre>
</div>
</div>
<p class="calibre10">正如预期一样， <code class="docutils"><span class="calibre4">VarianceThreshold</span></code> 移除了第一列，它的值为 0 的概率为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000659.jpg" alt="p = 5/6 &gt; .8" /> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-491">
<span id="calibre_link-492" class="calibre4"></span><h2 class="sigil_not_in_toc">1.13.2. 单变量特征选择</h2>
<p class="calibre2">单变量的特征选择是通过基于单变量的统计测试来选择最好的特征。它可以当做是评估器的预处理步骤。Scikit-learn 将特征选择的内容作为实现了 transform 方法的对象：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" title="sklearn.feature_selection.SelectKBest"><code class="docutils"><span class="calibre4">SelectKBest</span></code></a> 移除那些除了评分最高的 K 个特征之外的所有特征</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" title="sklearn.feature_selection.SelectPercentile"><code class="docutils"><span class="calibre4">SelectPercentile</span></code></a> 移除除了用户指定的最高得分百分比之外的所有特征</li>
<li class="toctree-l">对每个特征应用常见的单变量统计测试:
假阳性率（false positive rate） <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr" title="sklearn.feature_selection.SelectFpr"><code class="docutils"><span class="calibre4">SelectFpr</span></code></a>, 伪发现率（false discovery rate）
<a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr" title="sklearn.feature_selection.SelectFdr"><code class="docutils"><span class="calibre4">SelectFdr</span></code></a> , 或者族系误差（family wise error） <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe" title="sklearn.feature_selection.SelectFwe"><code class="docutils"><span class="calibre4">SelectFwe</span></code></a> 。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect" title="sklearn.feature_selection.GenericUnivariateSelect"><code class="docutils"><span class="calibre4">GenericUnivariateSelect</span></code></a> 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">例如下面的实例，我们可以使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000789.jpg" alt="\chi^2" /> 检验样本集来选择最好的两个特征：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_selection</span> <span class="calibre4">import</span> <span class="calibre4">SelectKBest</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_selection</span> <span class="calibre4">import</span> <span class="calibre4">chi2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 4)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">SelectKBest</span><span class="calibre4">(</span><span class="calibre4">chi2</span><span class="calibre4">,</span> <span class="calibre4">k</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 2)</span>
</pre>
</div>
</div>
<p class="calibre10">这些对象将得分函数作为输入，返回单变量的得分和 p 值 （或者仅仅是 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" title="sklearn.feature_selection.SelectKBest"><code class="docutils"><span class="calibre4">SelectKBest</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile" title="sklearn.feature_selection.SelectPercentile"><code class="docutils"><span class="calibre4">SelectPercentile</span></code></a> 的分数）:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">对于回归: <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression" title="sklearn.feature_selection.f_regression"><code class="docutils"><span class="calibre4">f_regression</span></code></a> , <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" title="sklearn.feature_selection.mutual_info_regression"><code class="docutils"><span class="calibre4">mutual_info_regression</span></code></a></li>
<li class="toctree-l">对于分类: <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="docutils"><span class="calibre4">chi2</span></code></a> , <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" title="sklearn.feature_selection.f_classif"><code class="docutils"><span class="calibre4">f_classif</span></code></a> , <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif" title="sklearn.feature_selection.mutual_info_classif"><code class="docutils"><span class="calibre4">mutual_info_classif</span></code></a></li>
</ul>
</div>
</blockquote>
<p class="calibre10">这些基于 F-test 的方法计算两个随机变量之间的线性相关程度。另一方面，mutual information methods（互信息）能够计算任何种类的统计相关性，但是作为非参数的方法，互信息需要更多的样本来进行准确的估计。</p>
<div class="toctree-wrapper">
<p class="calibre10">稀疏数据的特征选择</p>
<dl class="calibre10">
<dt class="calibre18">如果你使用的是稀疏的数据 (例如数据可以由稀疏矩阵来表示),</dt>
<dd class="calibre19"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="docutils"><span class="calibre4">chi2</span></code></a> , <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression" title="sklearn.feature_selection.mutual_info_regression"><code class="docutils"><span class="calibre4">mutual_info_regression</span></code></a> , <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif" title="sklearn.feature_selection.mutual_info_classif"><code class="docutils"><span class="calibre4">mutual_info_classif</span></code></a>
可以处理数据并保持它的稀疏性。</dd>
</dl>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">不要使用一个回归评分函数来处理分类问题，你会得到无用的结果。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Examples:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py"><span class="calibre4">Univariate Feature Selection</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py"><span class="calibre4">Comparison of F-test and mutual information</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-493">
<span id="calibre_link-494" class="calibre4"></span><h2 class="sigil_not_in_toc">1.13.3. 递归式特征消除</h2>
<p class="calibre2">给定一个外部的估计器，可以对特征赋予一定的权重（比如，线性模型的相关系数），recursive feature elimination ( <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" title="sklearn.feature_selection.RFE"><code class="docutils"><span class="calibre4">RFE</span></code></a> )
通过考虑越来越小的特征集合来递归的选择特征。 首先，评估器在初始的特征集合上面训练并且每一个特征的重要程度是通过一个 <code class="docutils"><span class="calibre4">coef_</span></code> 属性
或者 <code class="docutils"><span class="calibre4">feature_importances_</span></code> 属性来获得。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为止。
<a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV" title="sklearn.feature_selection.RFECV"><code class="docutils"><span class="calibre4">RFECV</span></code></a> 在一个交叉验证的循环中执行 RFE 来找到最优的特征数量</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"><span class="calibre4">Recursive feature elimination</span></a> : 通过递归式特征消除来体现数字分类任务中像素重要性的例子。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"><span class="calibre4">Recursive feature elimination with cross-validation</span></a> : 通过递归式特征消除来自动调整交叉验证中选择的特征数。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-495">
<span id="calibre_link-496" class="calibre4"></span><h2 class="sigil_not_in_toc">1.13.4. 使用 SelectFromModel 选取特征</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="docutils"><span class="calibre4">SelectFromModel</span></code></a> 是一个 meta-transformer（元转换器） ，它可以用来处理任何带有 <code class="docutils"><span class="calibre4">coef_</span></code> 或者&nbsp;<code class="docutils"><span class="calibre4">feature_importances_</span></code> 属性的训练之后的评估器。
如果相关的``coef_`` 或者 <code class="docutils"><span class="calibre4">featureimportances</span></code> 属性值低于预先设置的阈值，这些特征将会被认为不重要并且移除掉。除了指定数值上的阈值之外，还可以通过给定字符串参数来使用内置的启发式方法找到一个合适的阈值。可以使用的启发式方法有 mean 、 median 以及使用浮点数乘以这些（例如，0.1*mean ）。</p>
<p class="calibre10">有关如何使用的例子，可以参阅下面的例子。</p>
<div class="toctree-wrapper">
<p class="calibre10">Examples</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_select_from_model_boston.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-boston-py"><span class="calibre4">Feature selection using SelectFromModel and LassoCV</span></a>: 从 Boston 数据中自动选择最重要两个特征而不需要提前得知这一信息。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-497">
<span id="calibre_link-498" class="calibre4"></span><h3 class="sigil_not_in_toc1">1.13.4.1. 基于 L1 的特征选取</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="linear_model.html#linear-model"><span class="calibre4">Linear models</span></a> 使用 L1 正则化的线性模型会得到稀疏解：他们的许多系数为 0。
当目标是降低使用另一个分类器的数据集的维度，
它们可以与 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="docutils"><span class="calibre4">feature_selection.SelectFromModel</span></code></a>
一起使用来选择非零系数。特别的，可以用于此目的的稀疏评估器有用于回归的
<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="docutils"><span class="calibre4">linear_model.Lasso</span></code></a> , 以及用于分类的&nbsp;<a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">linear_model.LogisticRegression</span></code></a> 和
<a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">svm.LinearSVC</span></code></a></p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">LinearSVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_selection</span> <span class="calibre4">import</span> <span class="calibre4">SelectFromModel</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 4)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lsvc</span> <span class="calibre4">=</span> <span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">0.01</span><span class="calibre4">,</span> <span class="calibre4">penalty</span><span class="calibre4">=</span><span class="calibre4">"l1"</span><span class="calibre4">,</span> <span class="calibre4">dual</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">SelectFromModel</span><span class="calibre4">(</span><span class="calibre4">lsvc</span><span class="calibre4">,</span> <span class="calibre4">prefit</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 3)</span>
</pre>
</div>
</div>
<p class="calibre10">在 SVM 和逻辑回归中，参数 C 是用来控制稀疏性的：小的 C 会导致少的特征被选择。使用 Lasso，alpha 的值越大，越少的特征会被选择。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a>: 不同算法的比较，当使用 L1 正则化的特征选择在文件分类任务上。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-499">
<p class="calibre10"><strong class="calibre14">L1-recovery 和 compressive sensing（压缩感知）</strong></p>
<p class="calibre10">当选择了正确的 alpha 值以后， <a class="calibre3 pcalibre" href="linear_model.html#lasso"><span class="calibre4">Lasso</span></a> 可以仅通过少量观察点便恢复完整的非零特征，
假设特定的条件可以被满足的话。特别的，数据量需要 “足够大” ，不然 L1 模型的表现将缺乏保障。
“足够大” 的定义取决于非零系数的个数、特征数量的对数值、噪音的数量、非零系数的最小绝对值、
以及设计矩阵（design maxtrix） X 的结构。特征矩阵必须有特定的性质，如数据不能过度相关。</p>
<p class="calibre10">关于如何选择 alpha 值没有固定的规则。alpha 值可以通过交叉验证来确定（ <code class="docutils"><span class="calibre4">LassoCV</span></code> 或者 <code class="docutils"><span class="calibre4">LassoLarsCV</span></code> ），尽管这可能会导致欠惩罚的模型：包括少量的无关变量对于预测值来说并非致命的。相反的， BIC（ <code class="docutils"><span class="calibre4">LassoLarsIC</span></code> ）倾向于给定高 alpha 值。</p>
<p class="calibre10"><strong class="calibre14">Reference（参考文献）</strong> Richard G. Baraniuk “Compressive Sensing”, IEEE Signal
Processing Magazine [120] July 2007
<a class="calibre3 pcalibre" href="http://dsp.rice.edu/sites/dsp.rice.edu/files/cs/baraniukCSlecture07.pdf">http://dsp.rice.edu/sites/dsp.rice.edu/files/cs/baraniukCSlecture07.pdf</a></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-500">
<h3 class="sigil_not_in_toc1">1.13.4.2. 基于 Tree（树）的特征选取</h3>
<p class="calibre2">基于树的 estimators （查阅 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.tree" title="sklearn.tree"><code class="docutils"><span class="calibre4">sklearn.tree</span></code></a> 模块和树的森林 在&nbsp;<a class="calibre3 pcalibre" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="docutils"><span class="calibre4">sklearn.ensemble</span></code></a>
模块） 可以用来计算特征的重要性，然后可以消除不相关的特征（当与 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="docutils"><span class="calibre4">sklearn.feature_selection.SelectFromModel</span></code></a> 等元转换器一同使用时）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.ensemble</span> <span class="calibre4">import</span> <span class="calibre4">ExtraTreesClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_selection</span> <span class="calibre4">import</span> <span class="calibre4">SelectFromModel</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 4)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">ExtraTreesClassifier</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">feature_importances_</span>  
<span class="calibre4">array([ 0.04...,  0.05...,  0.4...,  0.4...])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">SelectFromModel</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">prefit</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">shape</span>               
<span class="calibre4">(150, 2)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="calibre4">Feature importances with forests of trees</span></a>: 在合成数据上恢复有用特征的示例。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="calibre4">Pixel importances with a parallel forest of trees</span></a>: 在人脸识别数据上的示例。</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-501">
<h2 class="sigil_not_in_toc">1.13.5. 特征选取作为 pipeline（管道）的一部分</h2>
<p class="calibre2">特征选择通常在实际的学习之前用来做预处理。在 scikit-learn 中推荐的方式是使用 :<a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline</span></code></a>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([</span>
  <span class="calibre4">(</span><span class="calibre4">'feature_selection'</span><span class="calibre4">,</span> <span class="calibre4">SelectFromModel</span><span class="calibre4">(</span><span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">penalty</span><span class="calibre4">=</span><span class="calibre4">"l1"</span><span class="calibre4">))),</span>
  <span class="calibre4">(</span><span class="calibre4">'classification'</span><span class="calibre4">,</span> <span class="calibre4">RandomForestClassifier</span><span class="calibre4">())</span>
<span class="calibre4">])</span>
<span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">在这段代码中，我们利用 <a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">sklearn.svm.LinearSVC</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" title="sklearn.feature_selection.SelectFromModel"><code class="docutils"><span class="calibre4">sklearn.feature_selection.SelectFromModel</span></code></a> 来评估特征的重要性并且选择出相关的特征。
然后，在转化后的输出中使用一个 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestClassifier</span></code></a> 分类器，比如只使用相关的特征。你也可以使用其他特征选择的方法和可以提供评估特征重要性的分类器来执行相似的操作。
请查阅 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline</span></code></a> 来了解更多的实例。</p>
</div>
</div>


<div class="calibre" id="calibre_link-81">
<span id="calibre_link-502" class="calibre4"></span><h1 class="calibre5">1.14. 半监督学习</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@STAN,废柴0.1</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" /> 
    </div>
<p class="calibre10"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Semi-supervised_learning">半监督学习</a> 适用于在训练数据上的一些样本数据没有贴上标签的情况。
<a class="calibre3 pcalibre" href="classes.html#module-sklearn.semi_supervised" title="sklearn.semi_supervised"><code class="docutils"><span class="calibre4">sklearn.semi_supervised</span></code></a> 中的半监督估计, 能够利用这些附加的未标记数据来更好地捕获底层数据分布的形状，并将其更好地类推到新的样本。
当我们有非常少量的已标签化的点和大量的未标签化的点时，这些算法表现均良好。</p>
<div class="toctree-wrapper">
<p class="calibre10"><cite class="calibre13">y</cite> 中含有未标记的数据</p>
<p class="calibre10">在使用 <code class="docutils"><span class="calibre4">fit</span></code> 方法训练数据时, 将标识符与已标签化的数据一起分配给未标签化的点是尤其重要的.
实现该标记的方法是使用整数值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000602.jpg" alt="-1" />.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-503">
<span id="calibre_link-504" class="calibre4"></span><h2 class="sigil_not_in_toc">1.14.1. 标签传播</h2>
<p class="calibre2">标签传播表示半监督图推理算法的几个变体。</p>
<dl class="calibre10">
<dt class="calibre18">该模型的一些特性如下:</dt>
<dd class="calibre19"><ul class="first2">
<li class="toctree-l">可用于分类和回归任务</li>
<li class="toctree-l">Kernel methods to project data into alternate dimensional spaces</li>
</ul>
</dd>
</dl>
<p class="calibre10"><cite class="calibre13">scikit-learn</cite> 提供了两种标签传播模型:
<a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="docutils"><span class="calibre4">LabelPropagation</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="docutils"><span class="calibre4">LabelSpreading</span></code></a> 。
两者都通过在输入的 dataset（数据集）中的所有 items（项）上构建 similarity graph （相似图）来进行工作。</p>
<div class="toctree-wrapper" id="calibre_link-505">
<a class="calibre3 pcalibre" href="../auto_examples/semi_supervised/plot_label_propagation_structure.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_label_propagation_structure_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000882.jpg" class="calibre33" /></a>
<p class="calibre10"><span class="calibre4"><strong class="calibre14">标签传播说明:</strong> <em class="calibre13">未标签化的观察值结构与 class（类）结构一致, 因此可以将 class（类）标签传播到训练集的未标签化的观察值</em></span></p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="docutils"><span class="calibre4">LabelPropagation</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="docutils"><span class="calibre4">LabelSpreading</span></code></a> 在对图形的相似性矩阵, 以及对标签分布的 clamping effect（夹持效应）的修改方面不太一样。
Clamping 允许算法在一定程度上改变真实标签化数据的权重。
该 <a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="docutils"><span class="calibre4">LabelPropagation</span></code></a> 算法执行输入标签的 hard clamping, 这意味着 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000418.jpg" alt="\alpha=0" /> 。
这些 clamping factor 可以不是很严格的, 例如 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000592.jpg" alt="\alpha=0.2" /> , 这意味着我们将始终保留原始标签分配的 80%, 但该算法可以将其分布的置信度改变在 20% 以内。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelPropagation.html#sklearn.semi_supervised.LabelPropagation" title="sklearn.semi_supervised.LabelPropagation"><code class="docutils"><span class="calibre4">LabelPropagation</span></code></a> 使用原始相似性矩阵从未修改的数据来构建。
<a class="calibre3 pcalibre" href="generated/sklearn.semi_supervised.LabelSpreading.html#sklearn.semi_supervised.LabelSpreading" title="sklearn.semi_supervised.LabelSpreading"><code class="docutils"><span class="calibre4">LabelSpreading</span></code></a> 最小化具有正则化属性的损耗函数, 因此它通常更适用于噪声数据。
该算法在原始图形的修改版本上进行迭代, 并通过计算 normalized graph Laplacian matrix （归一化图拉普拉斯矩阵）来对边缘的权重进行归一化。
此过程也用于 <a class="calibre3 pcalibre" href="clustering.html#spectral-clustering"><span class="calibre4">Spectral clustering</span></a> 。</p>
<p class="calibre10">标签传播模型有两种内置的 kernel methods（核函数）。
kernel （核）的选择会影响算法的可扩展性和性能。
以下是可用的:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">rbf (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000755.jpg" alt="\exp(-\gamma |x-y|^2), \gamma &gt; 0" />). <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000566.jpg" alt="\gamma" /> 通过关键字 gamma 来指定。</li>
<li class="toctree-l">knn (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000558.jpg" alt="1[x&apos; \in kNN(x)]" />). <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 通过关键字 n_neighbors 来指定。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">RBF 核将产生一个完全连接的图形, 它通过密集矩阵在内存中表示。
该矩阵可能非常大, 与算法的每次迭代执行全矩阵乘法计算的成本相结合可导致超长的运行时间。
在另一方面, KNN 核将产生更多的内存友好的稀疏矩阵, 这样可以大幅度的减少运行时间。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/semi_supervised/plot_label_propagation_versus_svm_iris.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-versus-svm-iris-py"><span class="calibre4">Decision boundary of label propagation versus SVM on the Iris dataset</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/semi_supervised/plot_label_propagation_structure.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-structure-py"><span class="calibre4">Label Propagation learning a complex structure</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html#sphx-glr-auto-examples-semi-supervised-plot-label-propagation-digits-active-learning-py"><span class="calibre4">Label Propagation digits active learning</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<p class="calibre10">[1] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised
Learning (2006), pp. 193-216</p>
<p class="calibre10">[2] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient
Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005
<a class="calibre3 pcalibre" href="http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf">http://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf</a></p>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-8">
<span id="calibre_link-506" class="calibre4"></span><h1 class="calibre5">1.15. 等式回归</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@STAN,废柴0.1</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Damon</a><br class="calibre9" />
    </div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.isotonic.IsotonicRegression.html#sklearn.isotonic.IsotonicRegression" title="sklearn.isotonic.IsotonicRegression"><code class="docutils"><span class="calibre4">IsotonicRegression</span></code></a> 类对数据进行非降函数拟合.
它解决了如下的问题:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">最小化 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000251.jpg" alt="\sum_i w_i (y_i - \hat{y}_i)^2" /></p>
<p class="calibre10">服从于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000546.jpg" alt="\hat{y}_{min} = \hat{y}_1 \le \hat{y}_2 ... \le \hat{y}_n = \hat{y}_{max}" /></p>
</div>
</blockquote>
<p class="calibre10">其中每一个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000866.jpg" alt="w_i" /> 是 strictly 正数而且每个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是任意实
数. 它生成一个由平方误差接近的不减元素组成的向量.实际上这一些元素形成
一个分段线性的函数.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_isotonic_regression.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_isotonic_regression_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000492.jpg" class="math" /></a>
</div>
</div>


<div class="calibre" id="calibre_link-10">
<span id="calibre_link-507" class="calibre4"></span><h1 class="calibre5">1.16. 概率校准</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@曲晓峰</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" /> 
    </div>
<p class="calibre10">执行分类时, 您经常希望不仅可以预测类标签, 还要获得相应标签的概率.
这个概率给你一些预测的信心.
一些模型可以给你贫乏的概率估计, 有些甚至不支持概率预测.
校准模块可以让您更好地校准给定模型的概率, 或添加对概率预测的支持.</p>
<p class="calibre10">精确校准的分类器是概率分类器, 其可以将 predict_proba 方法的输出直接解释为 confidence level（置信度级别）.
例如，一个经过良好校准的（二元的）分类器应该对样本进行分类, 使得在给出一个接近 0.8 的 prediction_proba 值的样本中, 大约 80% 实际上属于正类.
以下图表比较了校准不同分类器的概率预测的良好程度:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_compare_calibration.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_compare_calibration_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000333.jpg" class="math" /></a>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 默认情况下返回良好的校准预测, 因为它直接优化了 log-loss（对数损失）情况.
相反，其他方法返回 biased probabilities（偏倚概率）;
每种方法有不同的偏差:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code class="docutils"><span class="calibre4">GaussianNB</span></code></a> 往往将概率推到 0 或 1（注意直方图中的计数）.
这主要是因为它假设特征在给定类别的条件下是独立的, 在该数据集中不包含 2 个冗余特征.</li>
</ul>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">RandomForestClassifier</span></code></a> 解释了相反的行为：直方图在约 0.2 和 0.9 的概率时显示峰值, 而接近 0 或 1 的概率非常罕见.
Niculescu-Mizil 和 Caruana [4] 给出了一个解释：”诸如 bagging 和 random forests（随机森林）的方法，
从基本模型的平均预测中可能难以将预测置于 0 和 1 附近, 因为基础模型的变化会偏离预测值, 它们应该接近于零或偏离这些值,
因为预测被限制在 [0,1] 的间隔, 由方差引起的误差往往是靠近 0 和 1 的一边，
例如，如果一个模型应该对于一个案例，预测 p = 0，bagging 可以实现的唯一方法是假设所有的 bagging 树预测为零.
如果我们在 bagging 树上增加噪声, 这种噪声将导致一些树预测大于 0 的值, 因此将 bagging 的平均预测从 0 移开.
我们用随机森林最强烈地观察到这种效应, 因为用随机森林训练的 base-level 树由于特征划分而具有相对较高的方差.
因此，校准曲线也被称为可靠性图 (Wilks 1995 [5] _) 显示了一种典型的 sigmoid 形状,
表明分类器可以更多地信任其 “直觉”, 并通常将接近 0 或 1 的概率返回.</li>
</ul>
<ul class="calibre6">
<li class="toctree-l">线性支持向量分类 (<a class="calibre3 pcalibre" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="docutils"><span class="calibre4">LinearSVC</span></code></a>) 显示了作为 RandomForestClassifier 更多的 Sigmoid 曲线,
这是经典的最大边距方法 (compare Niculescu-Mizil and Caruana <a class="calibre3 pcalibre" href="#calibre_link-11" id="calibre_link-12">[4]</a>),
其重点是靠近决策边界的 hard samples（支持向量）.</li>
</ul>
<p class="calibre10">提供了执行概率预测校准的两种方法: 基于 Platt 的 Sigmoid 模型的参数化方法和基于 isotonic regression（保序回归）的非参数方法 (<a class="calibre3 pcalibre" href="classes.html#module-sklearn.isotonic" title="sklearn.isotonic"><code class="docutils"><span class="calibre4">sklearn.isotonic</span></code></a>).
对于不用于模型拟合的新数据, 应进行概率校准.
类 <a class="calibre3 pcalibre" href="generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV" title="sklearn.calibration.CalibratedClassifierCV"><code class="docutils"><span class="calibre4">CalibratedClassifierCV</span></code></a> 使用交叉验证生成器, 并对每个拆分模型参数对训练样本和测试样本的校准进行估计.
然后对折叠预测的概率进行平均.
已经安装的分类器可以通过:class:<cite class="calibre13">CalibratedClassifierCV</cite> 传递参数 cv =”prefit” 这种方式进行校准.
在这种情况下, 用户必须手动注意模型拟合和校准的数据是不相交的.</p>
<p class="calibre10">以下图像展示了概率校准的好处.
第一个图像显示一个具有 2 个类和 3 个数据块的数据集.
中间的数据块包含每个类的随机样本.
此数据块中样本的概率应为 0.5.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000213.jpg" class="math" /></a>
</div>
<p class="calibre10">以下图像使用没有校准的高斯朴素贝叶斯分类器, 使用 sigmoid 校准和非参数的等渗校准来显示上述估计概率的数据.
可以观察到, 非参数模型为中间样本提供最准确的概率估计, 即0.5.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000766.jpg" class="math" /></a>
</div>
<p class="calibre10">对具有20个特征的100.000个样本（其中一个用于模型拟合）进行二元分类的人造数据集进行以下实验.
在 20个 特征中，只有 2 个是信息量, 10 个是冗余的.
该图显示了使用逻辑回归获得的估计概率, 线性支持向量分类器（SVC）和具有 sigmoid 校准和 sigmoid 校准的线性 SVC.
校准性能使用 Brier score <a class="calibre3 pcalibre" href="generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss" title="sklearn.metrics.brier_score_loss"><code class="docutils"><span class="calibre4">brier_score_loss</span></code></a> 来计算, 请看下面的图例（越销越好）.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration_curve.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_curve_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000091.jpg" class="math" /></a>
</div>
<p class="calibre10">这里可以观察到, 逻辑回归被很好地校准, 因为其曲线几乎是对角线.
线性 SVC 的校准曲线或可靠性图具有 sigmoid 曲线, 这是一个典型的不够自信的分类器.
在 LinearSVC 的情况下, 这是 hinge loss 的边缘属性引起的, 这使得模型集中在靠近决策边界（支持向量）的 hard samples（硬样本）上.
这两种校准都可以解决这个问题, 并产生几乎相同的结果.
下图显示了高斯朴素贝叶斯在相同数据上的校准曲线, 具有两种校准, 也没有校准.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration_curve.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_curve_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000682.jpg" class="math" /></a>
</div>
<p class="calibre10">可以看出， 高斯朴素贝叶斯的表现非常差, 但是以线性 SVC 的方式也是如此.
尽管线性 SVC 显示了 sigmoid 校准曲线, 但高斯朴素贝叶斯校准曲线具有转置的 sigmoid 结构.
这对于过分自信的分类器来说是非常经典的.
在这种情况下，分类器的过度自信是由违反朴素贝叶斯特征独立假设的冗余特征引起的.</p>
<p class="calibre10">用等渗回归法对高斯朴素贝叶斯概率的校准可以解决这个问题, 从几乎对角线校准曲线可以看出.
Sigmoid 校准也略微改善了 brier 评分, 尽管不如非参数等渗校准那样强烈.
这是 sigmoid 校准的固有限制，其参数形式假定为 sigmoid ，而不是转置的 sigmoid 曲线.
然而, 非参数等渗校准模型没有这样强大的假设, 并且可以处理任何形状, 只要有足够的校准数据.
通常，在校准曲线为 sigmoid 且校准数据有限的情况下, sigmoid 校准是优选的,
而对于非 sigmoid 校准曲线和大量数据可用于校准的情况，等渗校准是优选的.</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV" title="sklearn.calibration.CalibratedClassifierCV"><code class="docutils"><span class="calibre4">CalibratedClassifierCV</span></code></a> 也可以处理涉及两个以上类的分类任务, 如果基本估计器可以这样做的话.
在这种情况下, 分类器是以一对一的方式分别对每个类进行校准.
当预测未知数据的概率时, 分别预测每个类的校准概率.
由于这些概率并不总是一致, 因此执行后处理以使它们归一化.</p>
<p class="calibre10">下一个图像说明了 Sigmoid 校准如何改变 3 类分类问题的预测概率.
说明是标准的 2-simplex，其中三个角对应于三个类.
箭头从未校准分类器预测的概率向量指向在保持验证集上的 sigmoid 校准之后由同一分类器预测的概率向量.
颜色表示实例的真实类（red: class 1, green: class 2, blue: class 3）.</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration_multiclass.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_multiclass_0001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000668.jpg" class="math" /></a>
</div>
<p class="calibre10">基础分类器是具有 25 个基本估计器（树）的随机森林分类器.
如果这个分类器对所有 800 个训练数据点进行了训练, 那么它的预测过于自信, 从而导致了大量的对数损失.
校准在 600 个数据点上训练的相同分类器， 其余 200 个数据点上的 method =’sigmoid’ 减少了预测的置信度， 即将概率向量从单面的边缘向中心移动:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration_multiclass.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_calibration_multiclass_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000434.jpg" class="math" /></a>
</div>
<p class="calibre10">该校准导致较低的 log-loss（对数损失）.
请注意，替代方案是增加基准估计量的数量, 这将导致对数损失类似的减少.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Obtaining calibrated probability estimates from decision trees
and naive Bayesian classifiers, B. Zadrozny &amp; C. Elkan, ICML 2001</li>
<li class="toctree-l">Transforming Classifier Scores into Accurate Multiclass
Probability Estimates, B. Zadrozny &amp; C. Elkan, (KDD 2002)</li>
<li class="toctree-l">Probabilistic Outputs for Support Vector Machines and Comparisons to
Regularized Likelihood Methods, J. Platt, (1999)</li>
</ul>
<table class="docutils1" frame="void" id="calibre_link-11" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-12">[4]</a></td>
<td class="label1">Predicting Good Probabilities with Supervised Learning,
A. Niculescu-Mizil &amp; R. Caruana, ICML 2005</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-508" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[5]</td>
<td class="label1">On the combination of forecast probabilities for
consecutive precipitation periods. Wea. Forecasting, 5, 640&ndash;650.,
Wilks, D. S., 1990a</td>
</tr>
</tbody>
</table>
</div>
</div>


<div class="calibre" id="calibre_link-246">
<span id="calibre_link-509" class="calibre4"></span><h1 class="calibre5">1.17. 神经网络模型（有监督）</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@火星</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@A</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">此实现不适用于大规模应用程序。 特别是 scikit-learn 不支持 GPU。如果想要提高运行速度并使用基于 GPU 的实现以及为构建深度学习架构提供更多灵活性的框架，请参阅 <a class="calibre3 pcalibre" href="../related_projects.html#related-projects"><span class="calibre4">Related Projects</span></a>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-510">
<span id="calibre_link-511" class="calibre4"></span><h2 class="sigil_not_in_toc">1.17.1. 多层感知器</h2>
<p class="calibre2"><a href="#calibre_link-247" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-512">**</span></a>多层感知器（MLP）**是一种监督学习算法，通过在数据集上训练来学习函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000167.jpg" alt="f(\cdot): R^m \rightarrow R^o" />，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 是输入的维数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000689.jpg" alt="o" /> 是输出的维数。 给定一组特征 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000412.jpg" alt="X = {x_1, x_2, ..., x_m}" /> 和标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> ，它可以学习用于分类或回归的非线性函数。 与逻辑回归不同的是，在输入层和输出层之间，可以有一个或多个非线性层，称为隐藏层。 图1 展示了一个具有标量输出的单隐藏层 MLP。</p>
<div class="toctree-wrapper" id="calibre_link-513">
<a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/cn/0.19.0/_images/multilayerperceptron_network.png"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/multilayerperceptron_network.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000750.jpg" class="calibre34" /></a>
<p class="calibre10"><span class="calibre4"><strong class="calibre14">图1：单隐藏层MLP.</strong></span></p>
</div>
<p class="calibre10">最左层的输入层由一组代表输入特征的神经元 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000431.jpg" alt="\{x_i | x_1, x_2, ..., x_m\}" /> 组成。 每个隐藏层中的神经元将前一层的值进行加权线性求和转换 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000562.jpg" alt="w_1x_1 + w_2x_2 + ... + w_mx_m" /> ，再通过非线性激活函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000476.jpg" alt="g(\cdot):R \rightarrow R" /> - 比如双曲正切函数 tanh 。 输出层接收到的值是最后一个隐藏层的输出经过变换而来的。</p>
<p class="calibre10">该模块包含公共属性 <code class="docutils"><span class="calibre4">coefs_</span></code> 和 <code class="docutils"><span class="calibre4">intercepts_</span></code>。<code class="docutils"><span class="calibre4">coefs_</span></code> 是一系列权重矩阵，其中下标为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 的权重矩阵表示第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 层和第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000122.jpg" alt="i+1" /> 层之间的权重。 <code class="docutils"><span class="calibre4">intercepts_</span></code> 是一系列偏置向量，其中的下标为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 的向量表示添加到第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000122.jpg" alt="i+1" /> 等的偏置值。</p>
<p class="calibre10">多层感知器的优点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">可以学习得到非线性模型。</li>
<li class="toctree-l">使用``partial_fit`` 可以学习得到实时模型(在线学习)。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">多层感知器(MLP)的缺点:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">具有隐藏层的 MLP 具有非凸的损失函数，它有不止一个的局部最小值。 因此不同的随机权重初始化会导致不同的验证集准确率。</li>
<li class="toctree-l">MLP 需要调试一些超参数，例如隐藏层神经元的数量、层数和迭代轮数。</li>
<li class="toctree-l">MLP 对特征归一化很敏感.</li>
</ul>
</div>
</blockquote>
<p class="calibre10">解决这些缺点的方法请参阅 <a class="calibre3 pcalibre" href="#calibre_link-248"><span class="calibre4">实用使用技巧</span></a> 部分。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-514">
<h2 class="sigil_not_in_toc">1.17.2. 分类</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">MLPClassifier</span></code></a> 类实现了通过 <a class="calibre3 pcalibre" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">Backpropagation</a> 进行训练的多层感知器（MLP）算法。</div>
</blockquote>
<p class="calibre10">MLP 在两个 array 上进行训练:尺寸为 (n_samples, n_features) 的 array X 储存表示训练样本的浮点型特征向量; 尺寸为(n_samples,) 的 array y 储存训练样本的目标值（类别标签）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neural_network</span> <span class="calibre4">import</span> <span class="calibre4">MLPClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MLPClassifier</span><span class="calibre4">(</span><span class="calibre4">solver</span><span class="calibre4">=</span><span class="calibre4">'lbfgs'</span><span class="calibre4">,</span> <span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">1e-5</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                    <span class="calibre4">hidden_layer_sizes</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">),</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>                         
<span class="calibre4">MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',</span>
<span class="calibre4">       beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="calibre4">       epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',</span>
<span class="calibre4">       learning_rate_init=0.001, max_iter=200, momentum=0.9,</span>
<span class="calibre4">       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,</span>
<span class="calibre4">       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,</span>
<span class="calibre4">       warm_start=False)</span>
</pre>
</div>
</div>
<p class="calibre10">拟合（训练）后，该模型可以预测新样本的标签:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([1, 0])</span>
</pre>
</div>
</div>
<p class="calibre10">MLP 可以为训练数据拟合一个非线性模型。<code class="docutils"><span class="calibre4">clf.coefs_</span></code> 包含了构建模型的权值矩阵:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">[</span><span class="calibre4">coef</span><span class="calibre4">.</span><span class="calibre4">shape</span> <span class="calibre4">for</span> <span class="calibre4">coef</span> <span class="calibre4">in</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">coefs_</span><span class="calibre4">]</span>
<span class="calibre4">[(2, 5), (5, 2), (2, 1)]</span>
</pre>
</div>
</div>
<p class="calibre10">目前， <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">MLPClassifier</span></code></a> 只支持交叉熵损失函数，通过运行 <code class="docutils"><span class="calibre4">predict_proba</span></code> 方法进行概率估计。</p>
<p class="calibre10">MLP 算法使用的是反向传播的方式。 更准确地说，它使用了通过反向传播计算得到的梯度和某种形式的梯度下降来进行训练。 对于分类来说，它最小化交叉熵损失函数，为每个样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 给出一个向量形式的概率估计 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000436.jpg" alt="P(y|x)" /></p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict_proba</span><span class="calibre4">([[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>  
<span class="calibre4">array([[  1.967...e-04,   9.998...-01],</span>
<span class="calibre4">       [  1.967...e-04,   9.998...-01]])</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">MLPClassifier</span></code></a> 通过应用 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Softmax_activation_function">Softmax</a> 作为输出函数来支持多分类。</p>
<p class="calibre10">此外，该模型支持 <a class="calibre3 pcalibre" href="multiclass.html#multiclass"><span class="calibre4">多标签分类</span></a>，样本可能有多个类别可能。 对于每个类，原始输出经过 logistic 函数变换后，大于或等于 0.5 的值将进为 1，否则为 0。 对于样本的预测输出，值为 1 的索引位置表示该样本的分类类别:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MLPClassifier</span><span class="calibre4">(</span><span class="calibre4">solver</span><span class="calibre4">=</span><span class="calibre4">'lbfgs'</span><span class="calibre4">,</span> <span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">1e-5</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                    <span class="calibre4">hidden_layer_sizes</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">15</span><span class="calibre4">,),</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>                         
<span class="calibre4">MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',</span>
<span class="calibre4">       beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="calibre4">       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',</span>
<span class="calibre4">       learning_rate_init=0.001, max_iter=200, momentum=0.9,</span>
<span class="calibre4">       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,</span>
<span class="calibre4">       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,</span>
<span class="calibre4">       warm_start=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">array([[1, 1]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">]])</span>
<span class="calibre4">array([[0, 1]])</span>
</pre>
</div>
</div>
<p class="calibre10">了解更多，请参阅下面的示例和文档 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit" title="sklearn.neural_network.MLPClassifier.fit"><code class="docutils"><span class="calibre4">MLPClassifier.fit</span></code></a>。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py"><span class="calibre4">Compare Stochastic learning strategies for MLPClassifier</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py"><span class="calibre4">Visualization of MLP weights on MNIST</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-515">
<h2 class="sigil_not_in_toc">1.17.3. 回归</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="docutils"><span class="calibre4">MLPRegressor</span></code></a> 类实现了一个多层感知器（MLP），它在使用反向传播进行训练时的输出层没有使用激活函数，也可以看作是使用身份函数作为激活函数。 因此，它使用平方误差作为损失函数，输出是一组连续值。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="docutils"><span class="calibre4">MLPRegressor</span></code></a> 还支持多输出回归，其中样本可以有多个目标。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-516">
<h2 class="sigil_not_in_toc">1.17.4. 正则化</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="docutils"><span class="calibre4">MLPRegressor</span></code></a> 类和 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">MLPClassifier</span></code></a> 类都使用参数 <code class="docutils"><span class="calibre4">alpha</span></code> 作为正则化( L2 正则化)系数，正则化通过惩罚大数量级的权重值以避免过拟合问题。 下面的图表展示了不同的 alpha 值情况下的变化。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_mlp_alpha.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mlp_alpha_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000884.jpg" class="calibre35" /></a>
</div>
<p class="calibre10">详细信息，请参阅下面的示例。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py"><span class="calibre4">Varying regularization in Multi-layer Perceptron</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-517">
<h2 class="sigil_not_in_toc">1.17.5. 算法</h2>
<p class="calibre2">MLP 使用 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent（随机梯度下降）(SGD)</a>,
<a class="calibre3 pcalibre" href="http://arxiv.org/abs/1412.6980">Adam</a>, 或者 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> 进行训练。
Stochastic Gradient Descent （随机梯度下降）(SGD) 使用带有自适应参数的损失函数梯度来更新参数，即</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000864.jpg" alt="w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial Loss}{\partial w})" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000371.jpg" alt="\eta" /> 是控制训练过程参数更新步长的学习率( learning rate )。 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000743.jpg" alt="Loss" /> 是损失函数( loss function )。</p>
<p class="calibre10">更多细节可以在这个文档中找到 <a class="calibre3 pcalibre" href="http://scikit-learn.org/stable/modules/sgd.html">SGD</a> 。</p>
<p class="calibre10">Adam 类似于 SGD，因为它是 stochastic optimizer （随机优化器），但它可以根据低阶矩的自适应估计自动调整参数更新的量。</p>
<p class="calibre10">使用 SGD 或 Adam ，训练过程支持在线模式和小批量学习模式。</p>
<p class="calibre10">L-BFGS 是利用 Hessian 矩阵的近似表示的方法，矩阵中是函数的二阶偏导数。 它近似用 Hessian 矩阵的逆来进行参数更新。 该实现使用 Scipy 版本的 <a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html">L-BFGS</a>。</p>
<p class="calibre10">如果所选择的方法是 ‘L-BFGS’，训练过程不支持在线模式和小批量学习模式。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-518">
<h2 class="sigil_not_in_toc">1.17.6. 复杂性</h2>
<p class="calibre2">假设有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个训练样本， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 个特征， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个隐藏层，每个包含 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 个神经元 - 为简单起见， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000689.jpg" alt="o" /> 个输出神经元。 反向传播的时间复杂度是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000868.jpg" alt="O(n\cdot m \cdot h^k \cdot o \cdot i)" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 是迭代次数。 由于反向传播具有高时间复杂性，最好以较少数量的隐藏层神经元和较少的隐藏层个数开始训练。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-519">
<h2 class="sigil_not_in_toc">1.17.7. 数学公式</h2>
<p class="calibre2">给出一组训练样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000710.jpg" alt="(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)" /> 其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000398.jpg" alt="x_i \in \mathbf{R}^n" />  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000258.jpg" alt="y_i \in \{0, 1\}" />，一个单隐藏层单神经元 MLP 学习到的函数是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000895.jpg" alt="f(x) = W_2 g(W_1^T x + b_1) + b_2" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000106.jpg" alt="W_1 \in \mathbf{R}^m" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000690.jpg" alt="W_2, b_1, b_2 \in \mathbf{R}" /> 是模型参数.
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000099.jpg" alt="W_1, W_2" /> 分别是输入层与隐藏层之间和隐藏层与输出层之间的权重， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000677.jpg" alt="b_1, b_2" /> 分别是隐藏层和输出层的偏置值.
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000376.jpg" alt="g(\cdot) : R \rightarrow R" /> 是激活函数，默认为双曲正切函数。 具体形式如下，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000548.jpg" alt="g(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}" class="math" /></p>
</div>
<p class="calibre10">对于二分类， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000495.jpg" alt="f(x)" /> 经过 logistic 函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000173.jpg" alt="g(z)=1/(1+e^{-z})" /> 得到 0 到 1 之间的输出值。 0.5 的阈值将输出大于等于 0.5 的样本分到 positive class （正类），其他的分为 negative class （负类）。</p>
<p class="calibre10">如果多于两类，则 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000495.jpg" alt="f(x)" /> 本身将是一个尺寸为(n_classes,)的向量。 它需要经过 softmax 函数而不是 logistic 函数进行变换，具体形式如下，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000355.jpg" alt="\text{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{l=1}^k\exp(z_l)}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000256.jpg" alt="z_i" /> 表示 softmax 函数的第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 个输入的元素，它对应于第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 类， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 是类别的数量。 计算结果是样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 属于每个类别的概率的向量。 最终输出的分类结果是具有最高概率的类别。</p>
<p class="calibre10">在回归问题中，输出依然是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000495.jpg" alt="f(x)" /> ;因此，输出激活函数就是身份函数。</p>
<p class="calibre10">MLP 根据特定问题使用不同的损失函数。 二分类问题的损失函数的是交叉熵，具体形式如下，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000850.jpg" alt="Loss(\hat{y},y,W) = -y \ln {\hat{y}} - (1-y) \ln{(1-\hat{y})} + \alpha ||W||_2^2" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000393.jpg" alt="\alpha ||W||_2^2" /> 是 L2 正则化的模型复杂度惩罚项;  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000172.jpg" alt="\alpha &gt; 0" /> 这个非负的超参数控制惩罚的程度。</p>
<p class="calibre10">对于回归问题，MLP 使用平方误差损失函数，具体形式如下，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000883.jpg" alt="Loss(\hat{y},y,W) = \frac{1}{2}||\hat{y} - y ||_2^2 + \frac{\alpha}{2} ||W||_2^2" class="math" /></p>
</div>
<p class="calibre10">从随机初始化权重开始，多层感知器（MLP）不断更新这些权重值来最小化损失函数。计算完损失之后，从输出层到前面各层进行反向传递，为旨在减小损失函数值的参数提供更新值。</p>
<p class="calibre10">在梯度下降中，计算得到损失函数关于每个权重的梯度 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000132.jpg" alt="\nabla Loss_{W}" /> 并从权重 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000229.jpg" alt="W" /> 中减掉。用公式表示为，</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000521.jpg" alt="W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 是当前迭代步数， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000516.jpg" alt="\epsilon" /> 是大于 0 学习率。</p>
<p class="calibre10">算法停止的条件或者是达到预设的最大迭代次数，或者是损失函数低于某个特定值。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-248">
<span id="calibre_link-520" class="calibre4"></span><h2 class="sigil_not_in_toc">1.17.8. 实用技巧</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><p class="first">多层感知器对特征的缩放是敏感的，所以它强烈建议您归一化你的数据。 例如，将输入向量 X 的每个属性放缩到到 [0, 1] 或 [-1，+1] ，或者将其标准化使它具有 0 均值和方差 1。 注意，为了得到有意义的结果，您必须对测试集也应用 <em class="calibre13">相同的</em> 缩放尺度。 您可以使用 <code class="docutils"><span class="calibre4">StandardScaler</span></code> 进行标准化。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">StandardScaler</span>  
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span> <span class="calibre4">=</span> <span class="calibre4">StandardScaler</span><span class="calibre4">()</span>  
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Don't cheat - fit only on training data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>  
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>  
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># apply same transformation to test data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>  
</pre>
</div>
</div>
<p class="calibre10">一个推荐的可替代的方案是在 <code class="docutils"><span class="calibre4">Pipeline</span></code> 中使用的 <code class="docutils"><span class="calibre4">StandardScaler</span></code> 。</p>
</li>
<li class="toctree-l"><p class="first">最好使用 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 找到一个合理的正则化参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> ，通常范围是在 <code class="docutils"><span class="calibre4">10.0</span> <span class="calibre4">**</span> <span class="calibre4">-np.arange(1,</span> <span class="calibre4">7)</span></code> 。</p>
</li>
<li class="toctree-l"><p class="first">据经验可知，我们观察到 <cite class="calibre13">L-BFGS</cite> 收敛速度是更快的并且是小数据集上更好的解决方案。对于规模相对比较大的数据集，<cite class="calibre13">Adam</cite> 是非常鲁棒的。 它通常会迅速收敛，并得到相当不错的表现。 另一方面，如果学习率调整地正确， 使用 momentum 或 nesterov’s momentum 的 <cite class="calibre13">SGD</cite> 可以比这两种算法更好。</p>
</li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-521">
<h2 class="sigil_not_in_toc">1.17.9. 使用 warm_start 的更多控制</h2>
<p class="calibre2">如果您希望更多地控制 SGD 中的停止标准或学习率，或者想要进行额外的监视，使用 <code class="docutils"><span class="calibre4">warm_start=True</span></code> 和 <code class="docutils"><span class="calibre4">max_iter=1</span></code> 并且自身迭代可能会有所帮助:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MLPClassifier</span><span class="calibre4">(</span><span class="calibre4">hidden_layer_sizes</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">15</span><span class="calibre4">,),</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">max_iter</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">warm_start</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">i</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">10</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4"># additional monitoring / inspection </span>
<span class="calibre4">MLPClassifier(...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf">“Learning representations by back-propagating errors.”</a>
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">“Backpropagation”</a>
Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://arxiv.org/pdf/1412.6980v8.pdf">“Adam: A method for stochastic optimization.”</a>
Kingma, Diederik, and Jimmy Ba. arXiv preprint arXiv:1412.6980 (2014).</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-13">
<span id="calibre_link-522" class="calibre4"></span><h1 class="calibre5">2. 无监督学习</h1>
<div class="toctree-wrapper">
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html">2.1. 高斯混合模型</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id2">2.1.1. 高斯混合</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#gaussianmixture">2.1.1.1. 优缺点 <code class="docutils"><span class="calibre4">GaussianMixture</span></code></a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id3">2.1.1.1.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id4">2.1.1.1.2. 缺点</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id5">2.1.1.2. 选择经典高斯混合模型中分量的个数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#em">2.1.1.3. 估计算法期望最大化（EM）</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#bgmm">2.1.2. 变分贝叶斯高斯混合</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#variational-inference">2.1.2.1. 估计算法: 变分推断（variational inference）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id8">2.1.2.1.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#id9">2.1.2.1.2. 缺点</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/mixture.html#the-dirichlet-process">2.1.2.2. The Dirichlet Process（狄利克雷过程）</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html">2.2. 流形学习</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id2">2.2.1. 介绍</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#isomap">2.2.2. Isomap</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id4">2.2.2.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#locally-linear-embedding">2.2.3. 局部线性嵌入</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id6">2.2.3.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#mlle">2.2.4. 改进型局部线性嵌入（MLLE）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id7">2.2.4.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#he">2.2.5. 黑塞特征映射（HE）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id8">2.2.5.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#spectral-embedding">2.2.6. 谱嵌入</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id10">2.2.6.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#ltsa">2.2.7. 局部切空间对齐（LTSA）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id11">2.2.7.1. 复杂度</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#mds">2.2.8. 多维尺度分析（MDS）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id13">2.2.8.1. 度量 MDS</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id14">2.2.8.2. 非度量 MDS</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#t-t-sne">2.2.9. t 分布随机邻域嵌入（t-SNE）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id15">2.2.9.1. 优化 t-SNE</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#barnes-hut-t-sne">2.2.9.2. Barnes-Hut t-SNE</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/manifold.html#id17">2.2.10. 实用技巧</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html">2.3. 聚类</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id2">2.3.1. 聚类方法概述</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#k-means">2.3.2. K-means</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#mini-batch-kmeans">2.3.2.1. 小批量 K-Means</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#affinity-propagation">2.3.3. Affinity Propagation</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#mean-shift">2.3.4. Mean Shift</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#spectral-clustering">2.3.5. Spectral clustering</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id10">2.3.5.1. 不同的标记分配策略</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#hierarchical-clustering">2.3.6. 层次聚类</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#different-linkage-type-ward-complete-and-average-linkage">2.3.6.1. Different linkage type: Ward, complete and average linkage</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id12">2.3.6.2. 添加连接约束</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#varying-the-metric">2.3.6.3. Varying the metric</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#dbscan">2.3.7. DBSCAN</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#birch">2.3.8. Birch</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#clustering-evaluation">2.3.9. 聚类性能度量</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#rand">2.3.9.1. 调整后的 Rand 指数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id24">2.3.9.1.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id25">2.3.9.1.2. 缺点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id26">2.3.9.1.3. 数学表达</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#mutual-information">2.3.9.2. 基于 Mutual Information （互信息）的分数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id27">2.3.9.2.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id28">2.3.9.2.2. 缺点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id29">2.3.9.2.3. 数学公式</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#v-measure">2.3.9.3. 同质性，完整性和 V-measure</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id32">2.3.9.3.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id33">2.3.9.3.2. 缺点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id34">2.3.9.3.3. 数学表达</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#fowlkes-mallows">2.3.9.4. Fowlkes-Mallows 分数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id35">2.3.9.4.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id36">2.3.9.4.2. 缺点</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#silhouette">2.3.9.5. Silhouette 系数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id37">2.3.9.5.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id38">2.3.9.5.2. 缺点</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#calinski-harabaz">2.3.9.6. Calinski-Harabaz 指数</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id39">2.3.9.6.1. 优点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/clustering.html#id40">2.3.9.6.2. 缺点</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html">2.4. 双聚类</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html#spectral-co-clustering">2.4.1. Spectral Co-Clustering</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html#id2">2.4.1.1. 数学公式</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html#spectral-biclustering">2.4.2. Spectral Biclustering</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html#id4">2.4.2.1. 数学表示</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/biclustering.html#biclustering-evaluation">2.4.3. Biclustering 评测</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html">2.5. 分解成分中的信号（矩阵分解问题）</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#pca">2.5.1. 主成分分析（PCA）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#pca-exact-pca-and-probabilistic-interpretation">2.5.1.1. 准确的PCA和概率解释（Exact PCA and probabilistic interpretation）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#pca-incremental-pca">2.5.1.2. 增量PCA (Incremental PCA)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#pca-svd">2.5.1.3. PCA 使用随机SVD</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#kernel-pca">2.5.1.4. 核 PCA</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#sparsepca-minibatchsparsepca">2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#lsa">2.5.2. 截断奇异值分解和隐语义分析</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#dictionarylearning">2.5.3. 词典学习</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#sparsecoder">2.5.3.1. 带有预计算词典的稀疏编码</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#id9">2.5.3.2. 通用词典学习</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#minibatchdictionarylearning">2.5.3.3. 小批量字典学习</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#fa">2.5.4. 因子分析</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#ica">2.5.5. 独立成分分析（ICA）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#nmf-nnmf">2.5.6. 非负矩阵分解(NMF 或 NNMF)</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#nmf-frobenius">2.5.6.1. NMF 与 Frobenius 范数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#beta-divergence-nmf">2.5.6.2. 具有 beta-divergence 的 NMF</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/decomposition.html#dirichlet-lda">2.5.7. 隐 Dirichlet 分配（LDA）</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html">2.6. 协方差估计</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#id2">2.7. 经验协方差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#shrunk-covariance">2.8. 收敛协方差</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#id4">2.8.1. 基本收敛</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#ledoit-wolf">2.8.2. Ledoit-Wolf 收敛</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#oracle">2.8.3. Oracle 近似收缩</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#sparse-inverse-covariance">2.9. 稀疏逆协方差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#robust">2.10. Robust 协方差估计</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/covariance.html#id11">2.10.1. 最小协方差决定</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html">2.11. 新奇和异常值检测</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#novelty-detection">2.11.1. Novelty Detection（新奇检测）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#id2">2.11.2. Outlier Detection（异常值检测）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#fitting-an-elliptic-envelope">2.11.2.1. Fitting an elliptic envelope（椭圆模型拟合）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#isolation-forest">2.11.2.2. Isolation Forest（隔离森林）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#local-outlier-factor">2.11.2.3. Local Outlier Factor（局部异常系数）</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/outlier_detection.html#id4">2.11.2.4. 一类支持向量机与椭圆模型与隔离森林与局部异常系数</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/density.html">2.12. 密度估计</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/density.html#id2">2.12.1. 密度估计: 直方图</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/density.html#kernel-density">2.12.2. 核密度估计</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_unsupervised.html">2.13. 神经网络模型（无监督）</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_unsupervised.html#rbm">2.13.1. 限制波尔兹曼机</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_unsupervised.html#id3">2.13.1.1. 图形模型和参数化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_unsupervised.html#id4">2.13.1.2. 伯努利限制玻尔兹曼机</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/neural_networks_unsupervised.html#sml">2.13.1.3. 随机最大似然学习</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-255">
<span id="calibre_link-523" class="calibre4"></span><span id="calibre_link-524" class="calibre4"></span><h1 class="calibre5">2.1. 高斯混合模型</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/why2lyj" class="calibre3 pcalibre">@why2lyj</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Shao Y.</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@glassy</a><br class="calibre9" />
    </div>
<p class="calibre10"><code class="docutils"><span class="calibre4">sklearn.mixture</span></code> 是一个应用高斯混合模型进行非监督学习的包，支持 diagonal，spherical，tied，full 四种协方差矩阵
（注：diagonal 指每个分量有各自不同对角协方差矩阵， spherical 指每个分量有各自不同的简单协方差矩阵， tied 指所有分量有相同的标准协方差矩阵， full 指每个分量有各自不同的标准协方差矩阵），它对数据进行抽样，并且根据数据估计模型。同时包也提供了相关支持，来帮助用户决定合适的分量数（分量个数）。
<em class="calibre13">（译注：在高斯混合模型中，我们将每一个高斯分布称为一个分量，即 component ）</em></p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper" id="calibre_link-525">
<a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_pdf.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gmm_pdf_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000701.jpg" class="calibre11" /></a>
<p class="calibre10"><span class="calibre4"><strong class="calibre14">二分量高斯混合模型:</strong> <em class="calibre13">数据点，以及模型的等概率线。</em></span></p>
</div>
</div>
</blockquote>
<p class="calibre10">高斯混合模型是一个假设所有的数据点都是生成于一个混合的有限数量的并且未知参数的高斯分布的概率模型。
我们可以将混合模型看作是 k-means 聚类算法的推广，它利用了关于数据的协方差结构以及潜在高斯中心的信息。</p>
<p class="calibre10">对应不同的估算策略，Scikit-learn 实现了不同的类来估算高斯混合模型。
详细描述如下：</p>
<div class="toctree-wrapper" id="calibre_link-526">
<h2 class="sigil_not_in_toc">2.1.1. 高斯混合</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">GaussianMixture</span></code></a> 对象实现了用来拟合高斯混合模型的
<a class="calibre3 pcalibre" href="#calibre_link-256"><span class="calibre4">期望最大化</span></a> (EM) 算法。它还可以为多变量模型绘制置信区间，同时计算 BIC（Bayesian Information Criterion，贝叶斯信息准则）来评估数据中聚类的数量。
<a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.fit" title="sklearn.mixture.GaussianMixture.fit"><code class="docutils"><span class="calibre4">GaussianMixture.fit</span></code></a> 提供了从训练数据中学习高斯混合模型的方法。</div>
</blockquote>
<p class="calibre10">给定测试数据，通过使用 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture.predict" title="sklearn.mixture.GaussianMixture.predict"><code class="docutils"><span class="calibre4">GaussianMixture.predict</span></code></a> 方法，可以为每个样本分配最有可能对应的高斯分布。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">GaussianMixture</span></code></a> 方法中自带了不同的选项来约束不同估类的协方差：spherical，diagonal，tied 或 full 协方差。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_covariances.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gmm_covariances_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000632.jpg" class="calibre36" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">一个利用高斯混合模型在鸢尾花卉数据集（IRIS 数据集）上做聚类的协方差实例，请查阅 <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py"><span class="calibre4">GMM covariances</span></a></li>
<li class="toctree-l">一个绘制密度估计的例子，请查阅 <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_pdf.html#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py"><span class="calibre4">Density Estimation for a Gaussian mixture</span></a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-527">
<h3 class="sigil_not_in_toc1">2.1.1.1. 优缺点 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils2"><span class="calibre4">GaussianMixture</span></code></a></h3>
<div class="toctree-wrapper" id="calibre_link-528">
<h4 class="sigil_not_in_toc1">2.1.1.1.1. 优点</h4>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">速度:</th>
<td class="label1">是混合模型学习算法中最快的算法。</td>
</tr>
<tr class="row-odd"><th class="head">无偏差性:</th>
<td class="label1">这个算法仅仅只是最大化可能性，并不会使均值偏向于0，或是使聚类大小偏向于可能适用或者可能不适用的特殊结构。</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-529">
<h4 class="sigil_not_in_toc1">2.1.1.1.2. 缺点</h4>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">奇异性:</th>
<td class="label1">当每个混合模型没有足够多的点时，估算协方差变得困难起来，同时算法会发散并且找具有无穷大似然函数值的解，除非人为地对协方差进行正则化。</td>
</tr>
<tr class="row-odd"><th class="head">分量的数量:</th>
<td class="label1">这个算法将会总是用所有它能用的分量，所以在没有外部线索的情况下需要留存数据或者用信息理论标准来决定用多少分量。</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-530">
<h3 class="sigil_not_in_toc1">2.1.1.2. 选择经典高斯混合模型中分量的个数</h3>
<p class="calibre2">一种高效的方法是利用 BIC（贝叶斯信息准则）来选择高斯混合的分量数。
理论上，它仅当在近似状态下可以恢复正确的分量数（即如果有大量数据可用，并且假设这些数据实际上是一个混合高斯模型独立同分布生成的）。注意：使用 <a class="calibre3 pcalibre" href="#calibre_link-257"><span class="calibre4">变分贝叶斯高斯混合</span></a> 可以避免高斯混合模型中分量数的选择。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_selection.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gmm_selection_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000169.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">一个用典型的高斯混合进行的模型选择的例子，请查阅 <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_selection.html#sphx-glr-auto-examples-mixture-plot-gmm-selection-py"><span class="calibre4">Gaussian Mixture Model Selection</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-531">
<span id="calibre_link-256" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.1.1.3. 估计算法期望最大化（EM）</h3>
<p class="calibre2">在从无标记的数据中应用高斯混合模型主要的困难在于：通常不知道哪个点来自哪个潜在的分量
（如果可以获取到这些信息，就可以很容易通过相应的数据点，拟合每个独立的高斯分布）。
<a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">期望最大化（Expectation-maximization，EM）</a> 是一个理论完善的统计算法，其通过迭代方式来解决这个问题。首先，假设一个随机分量
（随机地选择一个中心点，可以用 k-means 算法得到，或者甚至就直接地随便在原点周围选取），并且为每个点计算由模型的每个分量生成的概率。然后，调整模型参数以最大化模型生成这些参数的可能性。重复这个过程，该算法保证过程中的参数总会收敛到局部最优解。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-257">
<span id="calibre_link-532" class="calibre4"></span><h2 class="sigil_not_in_toc">2.1.2. 变分贝叶斯高斯混合</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 对象实现了具有变分的高斯混合模型的变体推理算法。
这个API和 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">GaussianMixture</span></code></a> 相似。</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-533">
<span id="calibre_link-534" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.1.2.1. 估计算法: 变分推断（variational inference）</h3>
<p class="calibre2">变分推断是期望最大化（EM）的扩展，它最大化模型证据（包括先验）的下界，而不是数据似然函数。
变分方法的原理与期望最大化相同（二者都是迭代算法，在寻找由混合产生的每个点的概率和根据所分配的点拟合之间两步交替），但是变分方法通过整合先验分布信息来增加正则化限制。
这避免了期望最大化解决方案中常出现的奇异性，但是也对模型带来了微小的偏差。
变分方法计算过程通常明显较慢，但通常不会慢到无法使用。</p>
<p class="calibre10">由于它的贝叶斯特性，变分算法比预期最大化（EM）需要更多的超参数（即先验分布中的参数），其中最重要的就是
浓度参数 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> 。指定一个低浓度先验，将会使模型将大部分的权重放在少数分量上，
其余分量的权重则趋近 0。而高浓度先验将使混合模型中的大部分分量都有一定的权重。 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 类的参数实现提出了两种权重分布先验：
一种是利用 Dirichlet distribution（狄利克雷分布）的有限混合模型，另一种是利用 Dirichlet Process（狄利克雷过程）的无限混合模型。
在实际应用上，狄利克雷过程推理算法是近似的，并且使用具有固定最大分量数的截尾分布（称之为 Stick-breaking representation）。使用的分量数实际上几乎总是取决于数据。</p>
<p class="calibre10">下图比较了不同类型的权重浓度先验（参数 <code class="docutils"><span class="calibre4">weight_concentration_prior_type</span></code> ）
不同的 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> 的值获得的结果。
在这里，我们可以发现 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> 参数的值对获得的有效的激活分量数（即权重较大的分量的数量）有很大影响。
我们也能注意到当先验是 ‘dirichlet_distribution’ 类型时，大的浓度权重先验会导致更均匀的权重，然而 ‘dirichlet_process’ 类型（默认类型）却不是这样。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_concentration_prior.html"><img alt="plot_bgmm" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000296.jpg" class="calibre37" /></a> <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_concentration_prior.html"><img alt="plot_dpgmm" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000339.jpg" class="calibre37" /></a></strong></p>
<p class="calibre10">下面的例子将具有固定数量分量的高斯混合模型与
Dirichlet process prior（狄利克雷过程先验）的变分高斯混合模型进行比较。
这里，典型高斯混合模型被指定由 2 个聚类组成的有 5 个分量的数据集。
我们可以看到，具有狄利克雷过程的变分高斯混合可以将自身限制在 2 个分量，而高斯混合必须按照用户先验设置的固定数量的分量来拟合数据。
在例子中，用户选择了 <code class="docutils"><span class="calibre4">n_components=5</span></code> ，这与真正的试用数据集（toy dataset）的生成分量不符。
稍微观察就能注意到，狄利克雷过程先验的变分高斯混合模型可以采取保守的立场，并且只适合一个分量。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gmm_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000849.jpg" class="calibre38" /></a>
</div>
<p class="calibre10">在下图中，我们将拟合一个并不能被高斯混合模型很好描述的数据集。
调整 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 类的参数 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> ，这个参数决定了用来拟合数据的分量数。我们在最后两个图上展示了从两个混合结果产生的随机抽样。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_sin.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_gmm_sin_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000892.jpg" class="calibre39" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">一个用 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">GaussianMixture</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 绘制置信椭圆体的例子，
请查阅 <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm.html#sphx-glr-auto-examples-mixture-plot-gmm-py"><span class="calibre4">Gaussian Mixture Model Ellipsoids</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_gmm_sin.html#sphx-glr-auto-examples-mixture-plot-gmm-sin-py"><span class="calibre4">Gaussian Mixture Model Sine Curve</span></a> 这个例子展示了用 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">GaussianMixture</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 来拟合正弦波。</li>
<li class="toctree-l">一个使用不同的 <code class="docutils"><span class="calibre4">weight_concentration_prior_type</span></code> 用以不同的 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> 参数值的:class:<cite class="calibre13">BayesianGaussianMixture</cite> 来绘制置信椭圆体的例子。
请查阅 <a class="calibre3 pcalibre" href="../auto_examples/mixture/plot_concentration_prior.html#sphx-glr-auto-examples-mixture-plot-concentration-prior-py"><span class="calibre4">Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture</span></a></li>
</ul>
</div>
</blockquote>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture" title="sklearn.mixture.BayesianGaussianMixture"><code class="docutils"><span class="calibre4">BayesianGaussianMixture</span></code></a> 下的变分推理的优缺点</p>
</div>
<hr class="docutils3" />
<div class="toctree-wrapper" id="calibre_link-535">
<h4 class="sigil_not_in_toc1">2.1.2.1.1. 优点</h4>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">自动选择:</th>
<td class="label1">当 <code class="docutils"><span class="calibre4">weight_concentration_prior</span></code> 足够小以及
<code class="docutils"><span class="calibre4">n_components</span></code> 比模型需要的更大时，变分贝叶斯混合模型计算的结果可以让一些混合权重值趋近 0。
这让模型可以自动选择合适的有效分量数。这仅仅需要提供分量的数量上限。但是请注意，“理想” 的激活分量数只在应用场景中比较明确，在数据挖掘参数设置中通常并不明确。</td>
</tr>
<tr class="row-odd"><th class="head">对参数数量的敏感度较低:</th>
<td class="label1">与总是用尽可以用的分量，因而将为不同数量的组件产生不同的解决方案有限模型不同，变分推理狄利克雷过程先验变分推理（ <code class="docutils"><span class="calibre4">weight_concentration_prior_type='dirichlet_process'</span></code> ）改变参数后结果并不会改变太多，使之更稳定和更少的调优。</td>
</tr>
<tr class="calibre23"><th class="head">正则化:</th>
<td class="label1">由于结合了先验信息，变分的解比期望最大化（EM）的解有更少的病理特征（pathological special cases）。</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-536">
<h4 class="sigil_not_in_toc1">2.1.2.1.2. 缺点</h4>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">速度:</th>
<td class="label1">变分推理所需要的额外参数化使推理速度变慢，尽管并没有慢很多。</td>
</tr>
<tr class="row-odd"><th class="head">超参数:</th>
<td class="label1">这个算法需要一个额外的可能需要通过交叉验证进行实验调优的超参数。</td>
</tr>
<tr class="calibre23"><th class="head">偏差:</th>
<td class="label1">在推理算法存在许多隐含的偏差（如果用到狄利克雷过程也会有偏差），
每当这些偏差和数据之间不匹配时，用有限模型可能可以拟合更好的模型。</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-537">
<span id="calibre_link-538" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.1.2.2. The Dirichlet Process（狄利克雷过程）</h3>
<p class="calibre2">这里我们描述了狄利克雷过程混合的变分推理算法。狄利克雷过程是在 <em class="calibre13">具有无限大，无限制的分区数的聚类</em> 上的先验概率分布。相比于有限高斯混合模型，变分技术让我们在推理时间几乎没有惩罚（penalty）的情况下纳入了高斯混合模型的先验结构。</p>
<p class="calibre10">一个重要的问题是狄利克雷过程是如何实现用无限的，无限制的聚类数，并且结果仍然是一致的。
本文档不做出完整的解释，但是你可以看这里 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Dirichlet_process#The_stick-breaking_process">stick breaking process</a> 来帮助你理解它。折棍（stick breaking）过程是狄利克雷过程的衍生。我们每次从一个单位长度的 stick 开始，且每一步都折断剩下的一部分。每次，我们把每个 stick 的长度联想成落入一组混合的点的比例。
最后，为了表示无限混合，我们联想成最后每个 stick 的剩下的部分到没有落入其他组的点的比例。
每段的长度是随机变量，概率与浓度参数成比例。较小的浓度值将单位长度分成较大的 stick 段（即定义更集中的分布）。较高的浓度值将生成更小的 stick 段（即增加非零权重的分量数）。</p>
<p class="calibre10">狄利克雷过程的变分推理技术，在对该无限混合模型进行有限近似情形下，仍然可以运用。
用户不必事先指定想要的分量数，只需要指定浓度参数和混合分量数的上界（假定上界高于“真实”的分量数，仅仅影响算法复杂度，而不是实际上使用的分量数）。</p>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-84">
<span id="calibre_link-539" class="calibre4"></span><h1 class="calibre5">2.2. 流形学习</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi" class="calibre3 pcalibre">@XuJianzhi</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/RyanZhiNie" class="calibre3 pcalibre">@RyanZhiNie</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@羊三</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/XuJianzhi" class="calibre3 pcalibre">@XuJianzhi</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@羊三</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper">
<div class="toctree-wrapper">Look for the bare necessities</div>
<div class="toctree-wrapper">The simple bare necessities</div>
<div class="toctree-wrapper">Forget about your worries and your strife</div>
<div class="toctree-wrapper">I mean the bare necessities</div>
<div class="toctree-wrapper">Old Mother Nature’s recipes</div>
<div class="toctree-wrapper">That bring the bare necessities of life</div>
<div class="toctree-wrapper"><br class="calibre9" /></div>
<div class="toctree-wrapper">
<div class="toctree-wrapper">&ndash; Baloo的歌 [奇幻森林]</div>
</div>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_compare_methods.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_compare_methods_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000587.jpg" class="calibre40" /></a>
</div>
<p class="calibre10">流形学习是一种非线性降维方法。其算法基于的思想是：许多数据集的维度过高只是由人为导致的。</p>
<div class="toctree-wrapper" id="calibre_link-540">
<h2 class="sigil_not_in_toc">2.2.1. 介绍</h2>
<p class="calibre2">高维数据集会非常难以可视化。 虽然可以绘制两维或三维的数据来显示数据的固有结构，但与之等效的高维图不太直观。 为了帮助数据集结构的可视化，必须以某种方式降低维度。</p>
<p class="calibre10">通过对数据的随机投影来实现降维是最简单的方法。 虽然这样做能实现数据结构一定程度的可视化，但随机选择投影仍有许多有待改进之处。 在随机投影中，数据中更有趣的结构很可能会丢失。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="digits_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000625.jpg" class="calibre11" /></a> <a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="projected_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000194.jpg" class="calibre11" /></a></strong></p>
<p class="calibre10">为了解决这一问题，一些监督和无监督的线性降维框架被设计出来，如主成分分析（PCA），独立成分分析，线性判别分析等。 这些算法定义了明确的规定来选择数据的“有趣的”线性投影。 它们虽然强大，但是会经常错失数据中重要的非线性结构。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="PCA_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000528.jpg" class="calibre11" /></a> <a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="LDA_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000075.jpg" class="calibre11" /></a></strong></p>
<p class="calibre10">流形学习可以被认为是一种将线性框架（如 PCA ）推广为对数据中非线性结构敏感的尝试。 虽然存在监督变量，但是典型的流形学习问题是无监督的：它从数据本身学习数据的高维结构，而不使用预定的分类。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="calibre4">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a> ,手写数字降维的例子。</li>
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py"><span class="calibre4">Comparison of Manifold Learning methods</span></a> ,玩具 “S曲线” 数据集降维的例子。</li>
</ul>
</div>
<p class="calibre10">以下概述了 scikit-learn 中可用的流形学习实现</p>
</div>
<div class="toctree-wrapper" id="calibre_link-541">
<span id="calibre_link-542" class="calibre4"></span><h2 class="sigil_not_in_toc">2.2.2. Isomap</h2>
<p class="calibre2">流形学习的最早方法之一是 Isomap 算法，等距映射（Isometric Mapping）的缩写。 Isomap 可以被视为多维缩放（Multi-dimensional Scaling：MDS）或核主成分分析（Kernel PCA）的扩展。 Isomap 寻求一个较低维度的嵌入，它保持所有点之间的测量距离。 Isomap 可以通过 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="docutils"><span class="calibre4">Isomap</span></code></a> 对象执行。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0051.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000753.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-543">
<h3 class="sigil_not_in_toc1">2.2.2.1. 复杂度</h3>
<p class="calibre2">Isomap 算法包括三个阶段:</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">最近邻搜索.</strong>  Isomap 使用
<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="docutils"><span class="calibre4">sklearn.neighbors.BallTree</span></code></a> 进行有效的近邻搜索。
对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 维中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 个点的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个最近邻，代价约为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000224.jpg" alt="O[D \log(k) N \log(N)]" /></li>
<li class="toctree-l"><strong class="calibre14">最短路径图搜索.</strong>  最有效的已知算法是 Dijkstra 算法或 Floyd-Warshall 算法，其复杂度分别是约
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000019.jpg" alt="O[N^2(k + \log(N))]" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000403.jpg" alt="O[N^3]" /> 。 用户可通过使用 isomap 的 path_method 关键字选择该算法。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解.</strong>  对应于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000424.jpg" alt="N \times N" /> isomap核中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 个最大特征值的特征向量，进行嵌入编码。 对于密集求解器，代价约为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000776.jpg" alt="O[d N^2]" /> 。 通常可以使用 ARPACK 求解器来减少代价。 用户可通过使用 isomap 的 path_method 关键字指定特征求解器。 如果未指定，则代码尝试为输入数据选择最佳算法。</li>
</ol>
<p class="calibre10">Isomap 的整体复杂度是
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000842.jpg" alt="O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]" />.</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://science.sciencemag.org/content/290/5500/2319.full">“A global geometric framework for nonlinear dimensionality reduction”</a>
Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C.  Science 290 (5500)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-544">
<span id="calibre_link-545" class="calibre4"></span><h2 class="sigil_not_in_toc">2.2.3. 局部线性嵌入</h2>
<p class="calibre2">局部线性嵌入（LLE）通过保留局部邻域内的距离来寻求数据的低维投影。 它可以被认为是一系列的局部主成分分析，与全局相比，找到最优的局部非线性嵌入。</p>
<p class="calibre10">局部线性嵌入可以使用
<a class="calibre3 pcalibre" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="docutils"><span class="calibre4">locally_linear_embedding</span></code></a> 函数或其面向对象的等效方法
<a class="calibre3 pcalibre" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="docutils"><span class="calibre4">LocallyLinearEmbedding</span></code></a> 来实现。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0061.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000200.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-546">
<h3 class="sigil_not_in_toc1">2.2.3.1. 复杂度</h3>
<p class="calibre2">标准的 LLE 算法包括三个阶段:</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">最邻近搜索</strong>.  参见上述 Isomap 讨论。</li>
<li class="toctree-l"><strong class="calibre14">构造权重矩阵</strong>. <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000813.jpg" alt="O[D N k^3]" />.
LLE 权重矩阵的构造包括每个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 局部邻域的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000433.jpg" alt="k \times k" /> 线性方程的解</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解</strong>. 参见上述 Isomap 讨论。</li>
</ol>
<p class="calibre10">标准 LLE 的整体复杂度是
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000480.jpg" alt="O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]" />.</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.sciencemag.org/content/290/5500/2323.full">“Nonlinear dimensionality reduction by locally linear embedding”</a>
Roweis, S. &amp; Saul, L.  Science 290:2323 (2000)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-547">
<h2 class="sigil_not_in_toc">2.2.4. 改进型局部线性嵌入（MLLE）</h2>
<p class="calibre2">关于局部线性嵌入（LLE）的一个众所周知的问题是正则化问题。当 neighbors（邻域）的数量多于输入的维度数量时，定义每个局部邻域的矩阵是不满秩的。为解决这个问题，标准的局部线性嵌入算法使用一个任意正则化参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> ，它的取值受局部权重矩阵的迹的影响。虽然可以认为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000597.jpg" alt="r \to 0" /> ，即解收敛于嵌入情况，但是不保证最优解情况下 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000126.jpg" alt="r &gt; 0" /> 。此问题说明，在嵌入时此问题会扭曲流形的内部几何形状，使其失真。</p>
<p class="calibre10">解决正则化问题的一种方法是对邻域使用多个权重向量。这就是改进型局部线性嵌入（MLLE）算法的精髓。MLLE 可被执行于函数 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="docutils"><span class="calibre4">locally_linear_embedding</span></code></a> ，或者面向对象的副本 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="docutils"><span class="calibre4">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils"><span class="calibre4">method</span> <span class="calibre4">=</span> <span class="calibre4">'modified'</span></code> 。它需要满足 <code class="docutils"><span class="calibre4">n_neighbors</span> <span class="calibre4">&gt;</span> <span class="calibre4">n_components</span></code> 。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0071.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000808.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-548">
<h3 class="sigil_not_in_toc1">2.2.4.1. 复杂度</h3>
<p class="calibre2">MLLE 算法分为三部分：</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">近邻搜索</strong>。与标准 LLE 的相同。</li>
<li class="toctree-l"><strong class="calibre14">权重矩阵构造</strong>。大约是
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000419.jpg" alt="O[D N k^3] + O[N (k-D) k^2]" /> 。该式第一项恰好等于标准 LLE 算法的复杂度。该式第二项与由多个权重来构造权重矩阵相关。在实践中，（在第二步中）构造 MLLE 权重矩阵（对复杂度）增加的影响，比第一步和第三步的小。</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p class="calibre10">综上，MLLE 的复杂度为
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000158.jpg" alt="O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]" /> 。</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练集数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻域的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出的维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">“MLLE: Modified Locally Linear Embedding Using Multiple Weights”</a>
Zhang, Z. &amp; Wang, J.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-549">
<h2 class="sigil_not_in_toc">2.2.5. 黑塞特征映射（HE）</h2>
<p class="calibre2">黑塞特征映射 (也称作基于黑塞的 LLE: HLLE ）是解决 LLE 正则化问题的另一种方法。在每个用于恢复局部线性结构的邻域内，它会围绕一个基于黑塞的二次型展开。虽然其他实现表明它对数据大小缩放较差，但是 sklearn 实现了一些算法改进，使得在输出低维度时它的损耗可与其他 LLE 变体相媲美。HLLE 可实现为函数 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="docutils"><span class="calibre4">locally_linear_embedding</span></code></a> 或其面向对象的形式 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="docutils"><span class="calibre4">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils"><span class="calibre4">method</span> <span class="calibre4">=</span> <span class="calibre4">'hessian'</span></code> 。它需满足 <code class="docutils"><span class="calibre4">n_neighbors</span> <span class="calibre4">&gt;</span> <span class="calibre4">n_components</span> <span class="calibre4">*</span> <span class="calibre4">(n_components</span> <span class="calibre4">+</span> <span class="calibre4">3)</span> <span class="calibre4">/</span> <span class="calibre4">2</span></code> 。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0081.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000377.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-550">
<h3 class="sigil_not_in_toc1">2.2.5.1. 复杂度</h3>
<p class="calibre2">HLLE 算法分为三部分:</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">近邻搜索</strong>。与标准 LLE 的相同。</li>
<li class="toctree-l"><strong class="calibre14">权重矩阵构造</strong>. 大约是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000728.jpg" alt="O[D N k^3] + O[N d^6]" /> 。其中第一项与标准 LLE 相似。第二项来自于局部黑塞估计量的一个 QR 分解。</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解</strong>。与标准 LLE 的相同。</li>
</ol>
<p class="calibre10">综上，HLLE 的复杂度为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000443.jpg" alt="O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]" /> 。</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练集数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻域的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出的维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.pnas.org/content/100/10/5591">“Hessian Eigenmaps: Locally linear embedding techniques for
high-dimensional data”</a>
Donoho, D. &amp; Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-551">
<span id="calibre_link-552" class="calibre4"></span><h2 class="sigil_not_in_toc">2.2.6. 谱嵌入</h2>
<p class="calibre2">谱嵌入是计算非线性嵌入的一种方法。scikit-learn 执行拉普拉斯特征映射，该映射是用图拉普拉斯的谱分解的方法把数据进行低维表达。这个生成的图可认为是低维流形在高维空间里的离散近似值。基于图的代价函数的最小化确保流形上彼此临近的点被映射后在低维空间也彼此临近，低维空间保持了局部距离。谱嵌入可执行为函数 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding" title="sklearn.manifold.spectral_embedding"><code class="docutils"><span class="calibre4">spectral_embedding</span></code></a> 或它的面向对象的对应形式 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding" title="sklearn.manifold.SpectralEmbedding"><code class="docutils"><span class="calibre4">SpectralEmbedding</span></code></a> 。</p>
<div class="toctree-wrapper" id="calibre_link-553">
<h3 class="sigil_not_in_toc1">2.2.6.1. 复杂度</h3>
<p class="calibre2">谱嵌入（拉普拉斯特征映射）算法含三部分：</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">加权图结构</strong>。把原输入数据转换为用相似（邻接）矩阵表达的图表达。</li>
<li class="toctree-l"><strong class="calibre14">图拉普拉斯结构</strong>。非规格化的图拉普拉斯是按 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000681.jpg" alt="L = D - A" /> 构造，并按 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000130.jpg" alt="L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}" /> 规格化的。</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解</strong>。在图拉普拉斯上进行特征值分解。</li>
</ol>
<p class="calibre10">综上，谱嵌入的复杂度是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000480.jpg" alt="O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]" /> 。</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练集数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻域的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出的维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf">“Laplacian Eigenmaps for Dimensionality Reduction
and Data Representation”</a>
M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-554">
<h2 class="sigil_not_in_toc">2.2.7. 局部切空间对齐（LTSA）</h2>
<p class="calibre2">尽管局部切空间对齐（LTSA）在技术上并不是 LLE 的变体，但它与 LLE 足够相近，可以放入这个目录。与 LLE 算法关注于保持临点距离不同，LTSA 寻求通过切空间来描述局部几何形状，并（通过）实现全局最优化来对其这些局部切空间，从而学会嵌入。
LTSA 可执行为函数
<a class="calibre3 pcalibre" href="generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding" title="sklearn.manifold.locally_linear_embedding"><code class="docutils"><span class="calibre4">locally_linear_embedding</span></code></a> 或它的面向对象的对应形式
<a class="calibre3 pcalibre" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="docutils"><span class="calibre4">LocallyLinearEmbedding</span></code></a> ，附带关键词 <code class="docutils"><span class="calibre4">method</span> <span class="calibre4">=</span> <span class="calibre4">'ltsa'</span></code> 。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0091.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000174.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-555">
<h3 class="sigil_not_in_toc1">2.2.7.1. 复杂度</h3>
<p class="calibre2">LTSA 算法含三部分:</p>
<ol class="arabic">
<li class="toctree-l"><strong class="calibre14">近邻搜索</strong>。与标准 LLE 的相同。</li>
<li class="toctree-l"><strong class="calibre14">加权矩阵构造</strong>。大约是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000627.jpg" alt="O[D N k^3] + O[k^2 d]" /> 。其中第一项与标准 LLE 相似。</li>
<li class="toctree-l"><strong class="calibre14">部分特征值分解</strong>。同于标准 LLE 。</li>
</ol>
<p class="calibre10">综上，复杂度是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000785.jpg" alt="O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]" /> 。</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> : 训练集数据点的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> : 输入维度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> : 最近邻域的个数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> : 输出的维度</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693">“Principal manifolds and nonlinear dimensionality reduction via
tangent space alignment”</a>
Zhang, Z. &amp; Zha, H. Journal of Shanghai Univ. 8:406 (2004)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-556">
<span id="calibre_link-557" class="calibre4"></span><h2 class="sigil_not_in_toc">2.2.8. 多维尺度分析（MDS）</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Multidimensional_scaling">多维尺度分析 Multidimensional scaling</a> （ <a class="calibre3 pcalibre" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="docutils"><span class="calibre4">MDS</span></code></a> ） 寻求数据的低维表示，（低维下的）它的距离保持了在初始高维空间中的距离。</p>
<p class="calibre10">一般来说，（MDS）是一种用来分析在几何空间距离相似或相异数据的技术。MDS 尝试将相似或相异的数据建模为几何空间距离。这些数据可以是物体间的相似等级，也可是分子的作用频率，还可以是国家简单贸易指数。</p>
<p class="calibre10">MDS算法有2类：度量和非度量。在 scikit-learn 中， <a class="calibre3 pcalibre" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="docutils"><span class="calibre4">MDS</span></code></a> 类中二者都有。在度量 MDS 中，输入相似度矩阵源自度量(并因此遵从三角形不等式)，输出两点之间的距离被设置为尽可能接近相似度或相异度的数据。在非度量版本中，算法尝试保持距离的控制，并因此寻找在所嵌入空间中的距离和相似/相异之间的单调关系。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0101.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000175.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">设 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000248.jpg" alt="S" /> 是相似度矩阵，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个输入点的坐标。差异 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000259.jpg" alt="\hat{d}_{ij}" /> 是以某种最佳方式选择的相似度的转换。然后，通过 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000727.jpg" alt="sum_{i &lt; j} d_{ij}(X) - \hat{d}_{ij}(X)" /> 定义称为 Stress （应力值）的对象。</p>
<div class="toctree-wrapper" id="calibre_link-558">
<h3 class="sigil_not_in_toc1">2.2.8.1. 度量 MDS</h3>
<p class="calibre2">最简单的度量 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="docutils"><span class="calibre4">MDS</span></code></a> 模型称为 <em class="calibre13">absolute MDS（绝对MDS）</em>，差异由 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000511.jpg" alt="\hat{d}_{ij} = S_{ij}" /> 定义。对于绝对 MDS，值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000810.jpg" alt="S_{ij}" /> 应精确地对应于嵌入点的点 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000457.jpg" alt="j" /> 之间的距离。</p>
<p class="calibre10">大多数情况下，差异应设置为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000767.jpg" alt="\hat{d}_{ij} = b S_{ij}" /> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-559">
<h3 class="sigil_not_in_toc1">2.2.8.2. 非度量 MDS</h3>
<p class="calibre2">非度量 <a class="calibre3 pcalibre" href="generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS" title="sklearn.manifold.MDS"><code class="docutils"><span class="calibre4">MDS</span></code></a> 关注数据的排序。如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000596.jpg" alt="S_{ij} &lt; S_{kl}" /> ，则嵌入应执行 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000512.jpg" alt="d_{ij} &lt; d_{jk}" /> 。这样执行的一个简单算法是在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000810.jpg" alt="S_{ij}" /> 上使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000183.jpg" alt="d_{ij}" /> 的单调回归，产生与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000810.jpg" alt="S_{ij}" /> 相同顺序的差异 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000259.jpg" alt="\hat{d}_{ij}" /> 。</p>
<p class="calibre10">此问题的 a trivial solution（一个平凡解）是把所有点设置到原点上。为了避免这种情况，将差异 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000810.jpg" alt="S_{ij}" /> 标准化。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_mds.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mds_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000697.jpg" class="calibre41" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.springer.com/fr/book/9780387251509">“Modern Multidimensional Scaling - Theory and Applications”</a>
Borg, I.; Groenen P. Springer Series in Statistics (1997)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://link.springer.com/article/10.1007%2FBF02289694">“Nonmetric multidimensional scaling: a numerical method”</a>
Kruskal, J. Psychometrika, 29 (1964)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://link.springer.com/article/10.1007%2FBF02289565">“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”</a>
Kruskal, J. Psychometrika, 29, (1964)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-560">
<span id="calibre_link-561" class="calibre4"></span><h2 class="sigil_not_in_toc">2.2.9. t 分布随机邻域嵌入（t-SNE）</h2>
<p class="calibre2">t-SNE（ <a class="calibre3 pcalibre" href="generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="sklearn.manifold.TSNE"><code class="docutils"><span class="calibre4">TSNE</span></code></a> ）将数据点的相似性转换为概率。原始空间中的相似性表示为高斯联合概率，嵌入空间中的相似性表示为 “学生” 的 t 分布。这允许 t-SNE 对局部结构特别敏感，并且有超过现有技术的一些其它优点:</p>
<ul class="calibre6">
<li class="toctree-l">在一个单一映射上以多种比例显示结构</li>
<li class="toctree-l">显示位于多个、不同的流形或聚类中的数据</li>
<li class="toctree-l">减轻在中心聚集的趋势</li>
</ul>
<p class="calibre10">Isomap、LLE 和其它变体最适合展开单个连续低维流形，而 t-SNE 将侧重于数据的局部结构，并倾向于提取聚类的局部样本组，就像S曲线示例中突出显示的那样。这种基于局部结构对样本进行分组的能力可能有助于在视觉上同时解开包括多个流形的数据集，如数字数据集中的情况。</p>
<p class="calibre10">原始空间和嵌入空间中的联合概率的 Kullback-Leibler（KL） 散度将通过梯度下降而最小化。注意，KL 发散不是凸的，即具有不同初始化的多次重新开始将以KL发散的局部最小值结束。因此，尝试不同的开始值并选择具有最低KL散度的嵌入有时是有用的。</p>
<p class="calibre10">使用 t - SNE 的缺点大致如下:</p>
<ul class="calibre6">
<li class="toctree-l">t-SNE 的计算成本很高，在百万样本数据集上可能需要几个小时，而PCA将在几秒或几分钟内完成同样工作。</li>
<li class="toctree-l">Barnes-Hut t-SNE 方法仅限于二维或三维嵌入。</li>
<li class="toctree-l">该算法是随机的，不同种子的多次重新开始可以产生不同的嵌入。然而，以最小的误差选择嵌入是完全合理的。</li>
<li class="toctree-l">未明确保留全局结构。用PCA初始化点(使用 <cite class="calibre13">init=’pca’</cite> )，可以减轻此问题。</li>
</ul>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/manifold/plot_lle_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lle_digits_0131.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000879.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-562">
<h3 class="sigil_not_in_toc1">2.2.9.1. 优化 t-SNE</h3>
<p class="calibre2">t-SNE 的主要目的是实现高维数据的可视化。因此，当数据将嵌入到二维或三维时，它效果最好。</p>
<p class="calibre10">优化KL发散有时可能有点棘手。有五个参数控制 t-SNE 的优化，因此可能也控制最终嵌入的质量:</p>
<ul class="calibre6">
<li class="toctree-l">复杂度</li>
<li class="toctree-l">早期增长因子</li>
<li class="toctree-l">学习率</li>
<li class="toctree-l">最大迭代次数</li>
<li class="toctree-l">角度（不在精确方法中使用）</li>
</ul>
<p class="calibre10">复杂度（perplexity）定义为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000482.jpg" alt="k=2^(S)" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000248.jpg" alt="S" /> 是条件概率分布的香农熵。k 面色子的复杂度是 k ，因此 k 实际上是生成条件概率时 t-SNE 考虑的最近邻域的个数。复杂度越大导致有越多的近邻域，则对小结构越不敏感。相反地，越低的复杂度考虑越少的邻域，并因此忽略越多的全局信息而越关注局部邻域。当数据集的大小变大时，需要更多的点来获得局部邻域的合理样本，因此可能需要更大的复杂度。类似地，噪声越大的数据集需要越大的复杂度来包含足够的局部邻域，以超出背景噪声。</p>
<p class="calibre10">最大迭代次数通常足够高，不需要任何调整。优化分为两个阶段:早期增长阶段和最终优化阶段。在早期增长中，原始空间中的联合概率将通过与给定因子相乘而被人为地增加。越大的因子导致数据中的自然聚类之间的差距越大。如果因子过高，KL 发散可能在此阶段增加。通常不需要对其进行调谐。学习率是一个关键参数。如果梯度太低，下降会陷入局部极小值。如果过高，KL发散将在优化阶段增加。可以在 Laurens van derMaaten 的常见问题解答中找到更多提示(见参考资料)。最后一个参数角度是性能和精度之间的折衷。角度越大意味着我们可以通过单个点来近似的区域越大，从而导致越快的速度，但结果越不准确。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="http://distill.pub/2016/misread-tsne/">“如何高效使用 t-SNE”</a> 提供了一个关于各种参数效果的很好的讨论，以及用来探索不同参数效果的交互式绘图。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-563">
<h3 class="sigil_not_in_toc1">2.2.9.2. Barnes-Hut t-SNE</h3>
<p class="calibre2">在此实现的 Barnes-Hut t-SNE 通常比其他流形学习算法慢得多。优化是很困难的，梯度的计算是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000442.jpg" alt="O[d N log(N)]" /> ，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 是输出维数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 是样本个数。Barnes-Hut 方法在 t-SNE 复杂度为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000776.jpg" alt="O[d N^2]" /> 的精确方法上有所改进，但有其他几个显著区别:</p>
<ul class="calibre6">
<li class="toctree-l">Barnes-Hut 实现仅在目标维度为3或更小时才有效。构建可视化时，2D 案例是典型的。</li>
<li class="toctree-l">Barnes-Hut 仅适用于密集的输入数据。稀疏数据矩阵只能用精确方法嵌入，或者可以通过密集的低阶投影来近似，例如使用 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="docutils"><span class="calibre4">sklearn.decomposition.TruncatedSVD</span></code></a></li>
<li class="toctree-l">Barnes-Hut 是精确方法的近似。近似使用 angle 作为参数，因此当参数 method=”exact” 时，angle 参数无效。</li>
<li class="toctree-l">Barnes-Hut 的拓展性很高。Barnes-Hut 可用于嵌入数十万个数据点，而精确方法只能处理数千个样本，再多就很困难了。</li>
</ul>
<p class="calibre10">出于可视化目的（ t-SNE 的主要使用情况），强烈建议使用 Barnes-Hut 方法。精确的 t-SNE 方法可用于检验高维空间中嵌入的理论性质，但由于计算约束而仅限于小数据集。</p>
<p class="calibre10">还要注意，数字 label 与 t-SNE 发现的自然聚类大致匹配，而 PCA 模型的线性 2D 投影产生标签区域在很大程度上重叠的表示。这是一个强有力的线索，表明该数据可以通过关注局部结构的非线性方法（例如，具有高斯 RBF 核的 SVM ）很好地分离。然而，如果不能在二维中用 t-SNE 来可视化分离良好的均匀标记组，并不一定意味着数据不能被监督模型正确地分类。可能是因为 2 维不够低，无法准确表示数据的内部结构。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://jmlr.org/papers/v9/vandermaaten08a.html">“Visualizing High-Dimensional Data Using t-SNE”</a>
van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research
(2008)</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://lvdmaaten.github.io/tsne/">“t-Distributed Stochastic Neighbor Embedding”</a>
van der Maaten, L.J.P.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf">“Accelerating t-SNE using Tree-Based Algorithms.”</a>
L.J.P. van der Maaten.  Journal of Machine Learning Research 15(Oct):3221-3245, 2014.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-564">
<h2 class="sigil_not_in_toc">2.2.10. 实用技巧</h2>
<ul class="calibre6">
<li class="toctree-l">确保对所有特征使用相同的缩放。因为流形学习方法是基于最近邻搜索的，否则算法的性能可能很差。有关缩放异构数据的方便方法，请参阅 <a class="calibre3 pcalibre" href="preprocessing.html#preprocessing-scaler"><span class="calibre4">StandardScaler</span></a> 。</li>
<li class="toctree-l">由每个例程计算的重构误差可用于选择最佳输出维度。对于嵌入在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 维参数空间中的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 维流形，重构误差将随着 <code class="docutils"><span class="calibre4">n_components</span></code> 的增加而减小，直到 <code class="docutils"><span class="calibre4">n_components</span> <span class="calibre4">==</span> <span class="calibre4">d</span></code> 。</li>
<li class="toctree-l">注意，噪声数据可以对流形造成“短路”，其实质上充当了一个桥梁，用于连接流形的不同部分，否则（没有这样的“桥梁”）这些流形将被很好地划分开。噪声和/或不完全数据的流形学习是一个活跃的研究领域。</li>
<li class="toctree-l">某些输入配置可能导致奇异加权矩阵，例如，当数据集中的两个以上点完全相同时，或者当数据被分割成不连续的组时。在这种情况下， <code class="docutils"><span class="calibre4">solver='arpack'</span></code> 将无法找到空空间。解决这一问题的最简单方法是使用 <code class="docutils"><span class="calibre4">solver='dense'</span></code> ，它将在一个奇异矩阵上进行，尽管它可能因为输入点的数量而非常缓慢。或者，人们可以尝试理解奇异的来源:如果它是由于不相交的集合，增加 <code class="docutils"><span class="calibre4">n_neighbors</span></code> 可能有所帮助；如果是由于数据集中的相同点，则删除这些点可能有所帮助。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">See also</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="ensemble.html#random-trees-embedding"><span class="calibre4">完全随机树嵌入</span></a> 也可以用于得到特征空间的非线性表示，另外它不用降维。</p>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-14">
<span id="calibre_link-565" class="calibre4"></span><h1 class="calibre5">2.3. 聚类</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@花开无声</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@krokyin</a><br class="calibre9" /> 
    </div>
<p class="calibre10">未标记的数据的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering（聚类）</a> 可以使用模块 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.cluster" title="sklearn.cluster"><code class="docutils"><span class="calibre4">sklearn.cluster</span></code></a> 来实现。</p>
<p class="calibre10">每个 clustering algorithm （聚类算法）有两个变体: 一个是 class, 它实现了 <code class="docutils"><span class="calibre4">fit</span></code> 方法来学习 train data（训练数据）的 clusters（聚类），还有一个 function（函数），是给定 train data（训练数据），返回与不同 clusters（聚类）对应的整数标签 array（数组）。对于 class（类），training data（训练数据）上的标签可以在 <code class="docutils"><span class="calibre4">labels_</span></code> 属性中找到。</p>
<div class="toctree-wrapper">
<p class="calibre10">输入数据</p>
<p class="calibre10">需要注意的一点是，该模块中实现的算法可以采用不同种类的 matrix （矩阵）作为输入。所有这些都接受 shape <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_features]</span></code> 的标准数据矩阵。
这些可以从以下的 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code class="docutils"><span class="calibre4">sklearn.feature_extraction</span></code></a> 模块的 classes （类）中获得。对于 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code class="docutils"><span class="calibre4">AffinityPropagation</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="docutils"><span class="calibre4">SpectralClustering</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="docutils"><span class="calibre4">DBSCAN</span></code></a> 也可以输入 shape <code class="docutils"><span class="calibre4">[n_samples,</span> <span class="calibre4">n_samples]</span></code> 的相似矩阵。这些可以从 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a> 模块中的函数获得。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-566">
<h2 class="sigil_not_in_toc">2.3.1. 聚类方法概述</h2>
<div class="toctree-wrapper" id="calibre_link-567">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_cluster_comparison.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_cluster_comparison_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000576.jpg" class="calibre42" /></a>
<p class="calibre10"><span class="calibre4">在 scikit-learn 中的 clustering algorithms （聚类算法）的比较</span></p>
</div>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="15%" class="label"></col>
<col width="16%" class="label"></col>
<col width="20%" class="label"></col>
<col width="27%" class="label"></col>
<col width="22%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">Method name（方法名称）</th>
<th class="head">Parameters（参数）</th>
<th class="head">Scalability（可扩展性）</th>
<th class="head">Usecase（使用场景）</th>
<th class="head">Geometry (metric used)（几何图形（公制使用））</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-15"><span class="calibre4">K-Means（K-均值）</span></a></td>
<td class="label1">number of clusters（聚类形成的簇的个数）</td>
<td class="label1">非常大的 <code class="docutils"><span class="calibre4">n_samples</span></code>, 中等的 <code class="docutils"><span class="calibre4">n_clusters</span></code> 使用
<a class="calibre3 pcalibre" href="#calibre_link-16"><span class="calibre4">MiniBatch code（MiniBatch 代码）</span></a></td>
<td class="label1">通用, 均匀的 cluster size（簇大小）, flat geometry（平面几何）, 不是太多的 clusters（簇）</td>
<td class="label1">Distances between points（点之间的距离）</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-17"><span class="calibre4">Affinity propagation</span></a></td>
<td class="label1">damping（阻尼）, sample preference（样本偏好）</td>
<td class="label1">Not scalable with n_samples（n_samples 不可扩展）</td>
<td class="label1">Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td class="label1">Graph distance (e.g. nearest-neighbor graph)（图形距离（例如，最近邻图））</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-18"><span class="calibre4">Mean-shift</span></a></td>
<td class="label1">bandwidth（带宽）</td>
<td class="label1">Not scalable with <code class="docutils"><span class="calibre4">n_samples</span></code> （不可扩展的 <code class="docutils"><span class="calibre4">n_samples</span></code>）</td>
<td class="label1">Many clusters, uneven cluster size, non-flat geometry（许多簇，不均匀的簇大小，非平面几何）</td>
<td class="label1">Distances between points（点之间的距离）</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-19"><span class="calibre4">Spectral clustering</span></a></td>
<td class="label1">number of clusters（簇的个数）</td>
<td class="label1">中等的 <code class="docutils"><span class="calibre4">n_samples</span></code>, 小的 <code class="docutils"><span class="calibre4">n_clusters</span></code></td>
<td class="label1">Few clusters, even cluster size, non-flat geometry（几个簇，均匀的簇大小，非平面几何）</td>
<td class="label1">Graph distance (e.g. nearest-neighbor graph)（图形距离（例如最近邻图））</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-20"><span class="calibre4">Ward hierarchical clustering</span></a></td>
<td class="label1">number of clusters（簇的个数）</td>
<td class="label1">大的 <code class="docutils"><span class="calibre4">n_samples</span></code> 和 <code class="docutils"><span class="calibre4">n_clusters</span></code></td>
<td class="label1">Many clusters, possibly connectivity constraints（很多的簇，可能连接限制）</td>
<td class="label1">Distances between points（点之间的距离）</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-20"><span class="calibre4">Agglomerative clustering</span></a></td>
<td class="label1">number of clusters（簇的个数）, linkage type（链接类型）, distance（距离）</td>
<td class="label1">大的 <code class="docutils"><span class="calibre4">n_samples</span></code> 和 <code class="docutils"><span class="calibre4">n_clusters</span></code></td>
<td class="label1">Many clusters, possibly connectivity constraints, non Euclidean distances（很多簇，可能连接限制，非欧几里得距离）</td>
<td class="label1">Any pairwise distance（任意成对距离）</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-21"><span class="calibre4">DBSCAN</span></a></td>
<td class="label1">neighborhood size（neighborhood 的大小）</td>
<td class="label1">非常大的 <code class="docutils"><span class="calibre4">n_samples</span></code>, 中等的 <code class="docutils"><span class="calibre4">n_clusters</span></code></td>
<td class="label1">Non-flat geometry, uneven cluster sizes（非平面几何，不均匀的簇大小）</td>
<td class="label1">Distances between nearest points（最近点之间的距离）</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="mixture.html#mixture"><span class="calibre4">Gaussian mixtures（高斯混合）</span></a></td>
<td class="label1">many（很多）</td>
<td class="label1">Not scalable（不可扩展）</td>
<td class="label1">Flat geometry, good for density estimation（平面几何，适用于密度估计）</td>
<td class="label1">Mahalanobis distances to  centers（Mahalanobis 与中心的距离）</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-22"><span class="calibre4">Birch</span></a></td>
<td class="label1">branching factor（分支因子）, threshold（阈值）, optional global clusterer（可选全局簇）.</td>
<td class="label1">大的 <code class="docutils"><span class="calibre4">n_clusters</span></code> 和 <code class="docutils"><span class="calibre4">n_samples</span></code></td>
<td class="label1">Large dataset, outlier removal, data reduction.（大数据集，异常值去除，数据简化）</td>
<td class="label1">Euclidean distance between points（点之间的欧式距离）</td>
</tr>
</tbody>
</table>
<p class="calibre10">当 clusters （簇）具有 specific shape （特殊的形状），即 non-flat manifold（非平面 manifold），并且标准欧几里得距离不是正确的 metric （度量标准）时，Non-flat geometry clustering （非平面几何聚类）是非常有用的。这种情况出现在上图的两个顶行中。</p>
<p class="calibre10">用于 clustering （聚类）的 Gaussian mixture models （高斯混合模型），专用于 mixture models （混合模型）描述在 <a class="calibre3 pcalibre" href="mixture.html#mixture"><span class="calibre4">文档的另一章节</span></a> 。可以将 KMeans 视为具有 equal covariance per component （每个分量相等协方差）的 Gaussian mixture model （高斯混合模型）的特殊情况。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-15">
<span id="calibre_link-568" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.2. K-means</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="docutils"><span class="calibre4">KMeans</span></code></a> 算法通过试图分离 n groups of equal variance（n 个相等方差组）的样本来聚集数据，minimizing （最小化）称为 <a class="calibre3 pcalibre" href="inertia">inertia</a> 或者 within-cluster sum-of-squares （簇内和平方）的 criterion （标准）。
该算法需要指定 number of clusters （簇的数量）。它可以很好地 scales （扩展）到 large number of samples（大量样本），并已经被广泛应用于许多不同领域的应用领域。</p>
<p class="calibre10">k-means 算法将一组 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 划分成 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 不相交的 clusters （簇） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" />, 每个都用 cluster （该簇）中的样本的均值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000239.jpg" alt="\mu_j" /> 描述。
这个 means （均值）通常被称为 cluster（簇）的 “centroids（质心）”; 注意，它们一般不是从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 中挑选出的点，虽然它们是处在同一个 space（空间）。
K-means（K-均值）算法旨在选择最小化 <em class="calibre13">inertia（惯性）</em> 或  within-cluster sum of squared（簇内和的平方和）的标准的 centroids（质心）:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000794.jpg" alt="\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_j - \mu_i||^2)" class="math" /></p>
</div>
<p class="calibre10">Inertia（惯性）, 或 the within-cluster sum of squares（簇内和平方差） criterion（标准）,可以被认为是 internally coherent clusters （内部想干聚类）的 measure （度量）。
它有各种缺点:</p>
<ul class="calibre6">
<li class="toctree-l">Inertia（惯性）假设 clusters （簇）是 convex（凸）的和 isotropic （各项同性），这并不是总是这样。它对 elongated clusters （细长的簇）或具有不规则形状的 manifolds 反应不佳。</li>
<li class="toctree-l">Inertia（惯性）不是一个 normalized metric（归一化度量）: 我们只知道 lower values （较低的值）是更好的，并且 零 是最优的。但是在 very high-dimensional spaces （非常高维的空间）中，欧几里得距离往往会变得 inflated （膨胀）（这就是所谓的 “curse of dimensionality （维度诅咒/维度惩罚）”）。在 k-means 聚类之前运行诸如 <a class="calibre3 pcalibre" href="PCA">PCA</a> 之类的 dimensionality reduction algorithm （降维算法）可以减轻这个问题并加快计算速度。</li>
</ul>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_kmeans_assumptions.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kmeans_assumptions_0011.png" class="align-center" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000657.jpg" /></a>
<p class="calibre10">K-means 通常被称为 Lloyd’s algorithm（劳埃德算法）。在基本术语中，算法有三个步骤。、
第一步是选择 initial centroids （初始质心），最基本的方法是从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 数据集中选择 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个样本。初始化完成后，K-means 由两个其他步骤之间的循环组成。
第一步将每个样本分配到其 nearest centroid （最近的质心）。第二步通过取分配给每个先前质心的所有样本的平均值来创建新的质心。计算旧的和新的质心之间的差异，并且算法重复这些最后的两个步骤，直到该值小于阈值。换句话说，算法重复这个步骤，直到质心不再显著移动。</p>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_kmeans_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kmeans_digits_0011.png" class="align-right" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000770.jpg" /></a>
<p class="calibre10">K-means 相当于具有 small, all-equal, diagonal covariance matrix （小的全对称协方差矩阵）的 expectation-maximization algorithm （期望最大化算法）。</p>
<p class="calibre10">该算法也可以通过 <a href="#calibre_link-23" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-569">`Voronoi diagrams（Voronoi图）&lt;https://en.wikipedia.org/wiki/Voronoi_diagram&gt;`_</span></a> 的概念来理解。首先使用 current centroids （当前质心）计算点的 Voronoi 图。
Voronoi 图中的每个 segment （段）都成为一个 separate cluster （单独的簇）。其次，centroids（质心）被更新为每个 segment （段）的 mean（平均值）。然后，该算法重复此操作，直到满足停止条件。
通常情况下，当 iterations （迭代）之间的 objective function （目标函数）的相对减小小于给定的 tolerance value （公差值）时，算法停止。在此实现中不是这样: 当质心移动小于 tolerance （公差）时，迭代停止。</p>
<p class="calibre10">给定足够的时间，K-means 将总是收敛的，但这可能是 local minimum （局部最小）的。这很大程度上取决于 initialization of the centroids （质心的初始化）。
因此，通常会进行几次 different initializations of the centroids （初始化不同质心）的计算。帮助解决这个问题的一种方法是 k-means++ 初始化方案，它已经在 scikit-learn 中实现（使用 <code class="docutils"><span class="calibre4">init='k-means++'</span></code> 参数）。
这将初始化 centroids （质心）（通常）彼此远离，导致比随机初始化更好的结果，如参考文献所示。</p>
<p class="calibre10">可以给出一个参数，以允许 K-means 并行运行，称为 <code class="docutils"><span class="calibre4">n_jobs</span></code>。给这个参数一个正值使用许多处理器（默认值: 1）。值 -1 使用所有可用的处理器，-2 使用一个，等等。Parallelization （并行化）通常以 cost of memory（内存的代价）加速计算（在这种情况下，需要存储多个质心副本，每个作业使用一个）。</p>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">当 <cite class="calibre13">numpy</cite> 使用 <cite class="calibre13">Accelerate</cite> 框架时，K-Means 的并行版本在 OS X 上损坏。这是 expected behavior （预期的行为）: <cite class="calibre13">Accelerate</cite> 可以在 fork 之后调用，但是您需要使用 Python binary（二进制）（该多进程在 posix 下不执行）来执行子进程。</p>
</div>
<p class="calibre10">K-means 可用于 vector quantization （矢量量化）。这是使用以下类型的 trained model （训练模型）的变换方法实现的 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="docutils"><span class="calibre4">KMeans</span></code></a> 。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py"><span class="calibre4">Demonstration of k-means assumptions</span></a>: 演示 k-means 是否 performs intuitively （直观执行），何时不执行</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"><span class="calibre4">A demo of K-Means clustering on the handwritten digits data</span></a>: 聚类手写数字</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">“k-means++: The advantages of careful seeding”</a>
Arthur, David, and Sergei Vassilvitskii,
<em class="calibre13">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms</em>, Society for Industrial and Applied Mathematics (2007)</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-16">
<span id="calibre_link-570" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.2.1. 小批量 K-Means</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="docutils"><span class="calibre4">MiniBatchKMeans</span></code></a> 是 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="docutils"><span class="calibre4">KMeans</span></code></a> 算法的一个变体，它使用 mini-batches （小批量）来减少计算时间，同时仍然尝试优化相同的 objective function （目标函数）。
Mini-batches（小批量）是输入数据的子集，在每次 training iteration （训练迭代）中 randomly sampled （随机抽样）。这些小批量大大减少了融合到本地解决方案所需的计算量。
与其他降低 k-means 收敛时间的算法相反，mini-batch k-means 产生的结果通常只比标准算法略差。</p>
<p class="calibre10">该算法在两个主要步骤之间进行迭代，类似于 vanilla k-means 。
在第一步， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000303.jpg" alt="b" /> 样本是从数据集中随机抽取的，形成一个 mini-batch （小批量）。然后将它们分配到最近的 centroid（质心）。
在第二步，centroids （质心）被更新。与 k-means 相反，这是在每个样本的基础上完成的。对于 mini-batch （小批量）中的每个样本，通过取样本的 streaming average （流平均值）和分配给该质心的所有先前样本来更新分配的质心。
这具有随时间降低 centroid （质心）的 rate （变化率）的效果。执行这些步骤直到达到收敛或达到预定次数的迭代。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="docutils"><span class="calibre4">MiniBatchKMeans</span></code></a> 收敛速度比 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="docutils"><span class="calibre4">KMeans</span></code></a> ，但是结果的质量会降低。在实践中，质量差异可能相当小，如示例和引用的参考。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_mini_batch_kmeans.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mini_batch_kmeans_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000894.jpg" class="calibre43" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py"><span class="calibre4">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</span></a>: KMeans 与 MiniBatchKMeans 的比较</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"><span class="calibre4">Clustering text documents using k-means</span></a>: 使用 sparse MiniBatchKMeans （稀疏 MiniBatchKMeans）的文档聚类</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="calibre4">Online learning of a dictionary of parts of faces</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">“Web Scale K-Means clustering”</a>
D. Sculley, <em class="calibre13">Proceedings of the 19th international conference on World
wide web</em> (2010)</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-17">
<span id="calibre_link-571" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.3. Affinity Propagation</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code class="docutils"><span class="calibre4">AffinityPropagation</span></code></a> AP聚类是通过在样本对之间发送消息直到收敛来创建聚类。然后使用少量示例样本作为聚类中心来描述数据集，
聚类中心是数据集中最能代表一类数据的样本。在样本对之间发送的消息表示一个样本作为另一个样本的示例样本的
适合程度，适合程度值在根据通信的反馈不断更新。更新迭代直到收敛，完成聚类中心的选取，因此也给出了最终聚类。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_affinity_propagation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_affinity_propagation_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000437.jpg" class="calibre11" /></a>
</div>
<dl class="calibre10">
<dt class="calibre18">Affinity Propagation 算法比较有趣的是可以根据提供的数据决定聚类的数目。 因此有两个比较重要的参数</dt>
<dd class="calibre19"><em class="calibre13">preference</em>, 决定使用多少个示例样本  <a href="#calibre_link-24" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-572">*</span></a>damping factor*（阻尼因子） 减少吸引信息和归属信息以防止
更新减少吸引度和归属度信息时数据振荡。</dd>
</dl>
<p class="calibre10">AP聚类算法主要的缺点是算法的复杂度. AP聚类算法的时间复杂度是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000447.jpg" alt="O(N^2 T)" />, 其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" />
是样本的个数 ， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000900.jpg" alt="T" /> 是收敛之前迭代的次数. 如果使用密集的相似性矩阵空间复杂度是
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000893.jpg" alt="O(N^2)" /> 如果使用稀疏的相似性矩阵空间复杂度可以降低。 这使得AP聚类最适合中小型数据集。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py"><span class="calibre4">Demo of affinity propagation clustering algorithm</span></a>: Affinity
Propagation on a synthetic 2D datasets with 3 classes.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py"><span class="calibre4">Visualizing the stock market structure</span></a> Affinity Propagation on
Financial time series to find groups of companies</li>
</ul>
</div>
<p class="calibre10"><strong class="calibre14">Algorithm description(算法描述):</strong>
样本之间传递的信息有两种。 第一种是 responsibility(吸引信息) <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000074.jpg" alt="r(i, k)" />, 样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 适合作为样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 的聚类中心的程度。</p>
<p class="calibre10">第二种是 availability(归属信息) <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000306.jpg" alt="a(i, k)" /> 样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 选择样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 作为聚类中心的适合程度,并且考虑其他所有样本选取 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 做为聚类中心的合适程度。
通过这个方法，选取示例样本作为聚类中心如果 (1) 该样本与其许多样本相似，并且 (2) 被许多样本选取
为它们自己的示例样本。</p>
<p class="calibre10">样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 对样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 吸引度计算公式:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000146.jpg" alt="r(i, k) \leftarrow s(i, k) - max [ a(i, k&apos;) + s(i, k&apos;) \forall k&apos; \neq k ]" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000708.jpg" alt="s(i, k)" /> 是样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 和样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 之间的相似度。
样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 作为样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 的示例样本的合适程度:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000580.jpg" alt="a(i, k) \leftarrow min [0, r(k, k) + \sum_{i&apos;~s.t.~i&apos; \notin \{i, k\}}{r(i&apos;, k)}]" class="math" /></p>
</div>
<p class="calibre10">算法开始时 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000550.jpg" alt="r" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000307.jpg" alt="a" /> 都被置 0,然后开始迭代计算直到收敛。
为了防止更新数据时出现数据振荡，在迭代过程中引入阻尼因子 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000719.jpg" alt="\lambda" /> :</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000544.jpg" alt="r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000350.jpg" alt="a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000180.jpg" alt="t" /> 迭代的次数。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-18">
<span id="calibre_link-573" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.4. Mean Shift</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><code class="docutils"><span class="calibre4">MeanShift</span></code></a> 算法旨在于发现一个样本密度平滑的 <em class="calibre13">blobs</em> 。
均值漂移算法是基于质心的算法，通过更新质心的候选位置为所选定区域的偏移均值。
然后，这些候选者在后处理阶段被过滤以消除近似重复，从而形成最终质心集合。</p>
<p class="calibre10">给定第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000180.jpg" alt="t" /> 次迭代中的候选质心 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> , 候选质心的位置将被安装如下公式更新:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000712.jpg" alt="x_i^{t+1} = x_i^t + m(x_i^t)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000859.jpg" alt="N(x_i)" /> 是围绕 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> 周围一个给定距离范围内的样本空间
and <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 是  <em class="calibre13">mean shift</em> vector（均值偏移向量） 是所有质心中指向
点密度增加最多的区域的偏移向量。使用以下等式计算，有效地将质心更新为其邻域内样本的平均值:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000458.jpg" alt="m(x_i) = \frac{\sum_{x_j \in N(x_i)}K(x_j - x_i)x_j}{\sum_{x_j \in N(x_i)}K(x_j - x_i)}" class="math" /></p>
</div>
<p class="calibre10">算法自动设定聚类的数目，取代依赖参数 <code class="docutils"><span class="calibre4">bandwidth``（带宽）,带宽是决定搜索区域的size的参数。</span>
<span class="calibre4">这个参数可以手动设置，但是如果没有设置，可以使用提供的评估函数</span> <span class="calibre4">``estimate_bandwidth</span></code> 进行评估。</p>
<p class="calibre10">该算法不是高度可扩展的，因为在执行算法期间需要执行多个最近邻搜索。 该算法保证收敛，但是当
质心的变化较小时，算法将停止迭代。</p>
<p class="calibre10">通过找到给定样本的最近质心来给新样本打上标签。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_mean_shift.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mean_shift_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000227.jpg" class="calibre11" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py"><span class="calibre4">A demo of the mean-shift clustering algorithm</span></a>: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&amp;rep=rep1&amp;type=pdf">“Mean shift: A robust approach toward feature space analysis.”</a>
D. Comaniciu and P. Meer, <em class="calibre13">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2002)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-19">
<span id="calibre_link-574" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.5. Spectral clustering</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="docutils"><span class="calibre4">SpectralClustering</span></code></a> 是在样本之间进行亲和力矩阵的低维度嵌入，其实是低维空间中的 KMeans。
如果亲和度矩阵稀疏，则这是非常有效的并且 <a class="calibre3 pcalibre" href="http://pyamg.org/">pyamg</a> module 以及安装好。
SpectralClustering 需要指定聚类数。这个算法适用于聚类数少时，在聚类数多是不建议使用。</p>
<p class="calibre10">对于两个聚类，它解决了相似图上的 <a class="calibre3 pcalibre" href="http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf">normalised cuts</a> 问题:
将图形切割成两个，使得切割的边缘的重量比每个簇内的边缘的权重小。在图像处理时，这个标准是特别有趣的:
图像的顶点是像素，相似图的边缘是图像的渐变函数。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="noisy_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000061.jpg" class="calibre44" /></a> <a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="segmented_img" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000128.jpg" class="calibre44" /></a></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">Transforming distance to well-behaved similarities</p>
<p class="calibre10">请注意，如果你的相似矩阵的值分布不均匀，例如:存在负值或者距离矩阵并不表示相似性
spectral problem 将会变得奇异，并且不能解决。
在这种情况下，建议对矩阵的 entries 进行转换。比如在符号距离有符号的情况下通常使用 heat kernel:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">similarity</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">exp</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">beta</span> <span class="calibre4">*</span> <span class="calibre4">distance</span> <span class="calibre4">/</span> <span class="calibre4">distance</span><span class="calibre4">.</span><span class="calibre4">std</span><span class="calibre4">())</span>
</pre>
</div>
</div>
<p class="calibre10">请看这样一个应用程序的例子。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py"><span class="calibre4">Spectral clustering for image segmentation</span></a>: Segmenting objects
from a noisy background using spectral clustering.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-segmentation-py"><span class="calibre4">Segmenting the picture of a raccoon face in regions</span></a>: Spectral clustering
to split the image of the raccoon face in regions.</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-575">
<h3 class="sigil_not_in_toc1">2.3.5.1. 不同的标记分配策略</h3>
<p class="calibre2">可以使用不同的分配策略, 对应于 <code class="docutils"><span class="calibre4">assign_labels</span></code> 参数 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="docutils"><span class="calibre4">SpectralClustering</span></code></a>。
<code class="docutils"><span class="calibre4">"kmeans"</span></code> 可以匹配更精细的数据细节，但是可能更加不稳定。 特别是，除非你设置
<code class="docutils"><span class="calibre4">random_state</span></code> 否则可能无法复现运行的结果 ，因为它取决于随机初始化。另一方，
使用 <code class="docutils"><span class="calibre4">"discretize"</span></code> 策略是 100% 可以复现的，但是它往往会产生相当均匀的几何形状的边缘。</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head"><code class="docutils"><span class="calibre4">assign_labels="kmeans"</span></code></th>
<th class="head"><code class="docutils"><span class="calibre4">assign_labels="discretize"</span></code></th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_segmentation.html"><img alt="face_kmeans" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000428.jpg" class="calibre45" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_segmentation.html"><img alt="face_discretize" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000422.jpg" class="calibre45" /></a></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323">“A Tutorial on Spectral Clustering”</a>
Ulrike von Luxburg, 2007</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324">“Normalized cuts and image segmentation”</a>
Jianbo Shi, Jitendra Malik, 2000</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501">“A Random Walks View of Spectral Segmentation”</a>
Marina Meila, Jianbo Shi, 2001</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100">“On Spectral Clustering: Analysis and an algorithm”</a>
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-20">
<span id="calibre_link-576" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.6. 层次聚类</h2>
<p class="calibre2">Hierarchical clustering 是一个常用的聚类算法，它通过不断的合并或者分割来构建聚类。
聚类的层次被表示成树（或者 dendrogram（树形图））。树根是拥有所有样本的唯一聚类，叶子是仅有一个样本的聚类。
请参照 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Hierarchical_clustering">Wikipedia page</a> 查看更多细节。</p>
<p class="calibre10">The <a class="calibre3 pcalibre" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="docutils"><span class="calibre4">AgglomerativeClustering</span></code></a> 使用自下而上的方法进行层次聚类:开始是每一个对象是一个聚类，
并且聚类别相继合并在一起。 linkage criteria 确定用于合并的策略的度量:</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Ward</strong> 最小化所有聚类内的平方差总和。这是一种 variance-minimizing （方差最小化）的优化方向，
这是与k-means 的目标函数相似的优化方法，但是用 agglomerative hierarchical（聚类分层）的方法处理。</li>
<li class="toctree-l"><strong class="calibre14">Maximum</strong> 或 <strong class="calibre14">complete linkage</strong> 最小化聚类对两个样本之间的最大距离。</li>
<li class="toctree-l"><strong class="calibre14">Average linkage</strong> 最小化聚类两个聚类中样本距离的平均值。</li>
</ul>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="docutils"><span class="calibre4">AgglomerativeClustering</span></code></a> 在于连接矩阵联合使用时，也可以扩大到大量的样本，但是
在样本之间没有添加连接约束时，计算代价很大:每一个步骤都要考虑所有可能的合并。</p>
<div class="toctree-wrapper">
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="docutils"><span class="calibre4">FeatureAgglomeration</span></code></a></p>
<p class="calibre10">The <a class="calibre3 pcalibre" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="docutils"><span class="calibre4">FeatureAgglomeration</span></code></a> 使用 agglomerative clustering 将看上去相似的
特征组合在一起，从而减少特征的数量。这是一个降维工具, 请参照 <a class="calibre3 pcalibre" href="unsupervised_reduction.html#data-reduction"><span class="calibre4">无监督降维</span></a>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-577">
<h3 class="sigil_not_in_toc1">2.3.6.1. Different linkage type: Ward, complete and average linkage</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="docutils"><span class="calibre4">AgglomerativeClustering</span></code></a> 支持 Ward, average, and complete
linkage 策略.</p>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000642.jpg" class="calibre46" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000305.jpg" class="calibre46" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_digits_linkage.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_linkage_0031.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000255.jpg" class="calibre46" /></a>
<p class="calibre10">Agglomerative cluster 存在 “rich get richer” 现象导致聚类大小不均匀。这方面 complete linkage
是最坏的策略，Ward 给出了最规则的大小。然而，在 Ward 中 affinity (or distance used in clustering)
不能被改变，对于 non Euclidean metrics 来说 average linkage 是一个好的选择。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py"><span class="calibre4">Various Agglomerative Clustering on a 2D embedding of digits</span></a>: exploration of the
different linkage strategies in a real dataset.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-578">
<h3 class="sigil_not_in_toc1">2.3.6.2. 添加连接约束</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="docutils"><span class="calibre4">AgglomerativeClustering</span></code></a> 中一个有趣的特点是可以使用 connectivity matrix（连接矩阵）
将连接约束添加到算法中（只有相邻的聚类可以合并到一起），连接矩阵为每一个样本给定了相邻的样本。
例如，在 swiss-roll 的例子中，连接约束禁止在不相邻的 swiss roll 上合并，从而防止形成在 roll 上
重复折叠的聚类。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="unstructured" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000053.jpg" class="calibre47" /></a> <a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="structured" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000415.jpg" class="calibre47" /></a></strong></p>
<p class="calibre10">这些约束对于强加一定的局部结构是很有用的，但是这也使得算法更快，特别是当样本数量巨大时。</p>
<p class="calibre10">连通性的限制是通过连接矩阵来实现的:一个 scipy sparse matrix（稀疏矩阵），仅在一行和
一列的交集处具有应该连接在一起的数据集的索引。这个矩阵可以通过 a-priori information （先验信息）
构建:例如，你可能通过仅仅将从一个连接指向另一个的链接合并页面来聚类页面。也可以从数据中学习到,</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">例如使用 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code class="docutils"><span class="calibre4">sklearn.neighbors.kneighbors_graph</span></code></a> 限制与最临近的合并 :ref:<a href="#calibre_link-25" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-579">`</span></a>this example</div>
</blockquote>
<dl class="calibre10">
<dt class="calibre18">&lt;sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py&gt;`, 或者使用</dt>
<dd class="calibre19"><a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code class="docutils"><span class="calibre4">sklearn.feature_extraction.image.grid_to_graph</span></code></a> 仅合并图像上相邻的像素点，
例如 <a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py"><span class="calibre4">raccoon face</span></a> 。</dd>
</dl>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py"><span class="calibre4">A demo of structured Ward hierarchical clustering on a raccoon face image</span></a>: Ward clustering
to split the image of a raccoon face in regions.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py"><span class="calibre4">Hierarchical clustering: structured vs unstructured ward</span></a>: Example of
Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="calibre4">Feature agglomeration vs. univariate selection</span></a>:
Example of dimensionality reduction with feature agglomeration based on
Ward hierarchical clustering.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="calibre4">Agglomerative clustering with and without structure</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10"><strong class="calibre14">Connectivity constraints with average and complete linkage</strong></p>
<p class="calibre10">Connectivity constraints 和 complete or average linkage 可以增强 agglomerative clustering 中的
‘rich getting richer’ 现象。特别是，如果建立的是 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code class="docutils"><span class="calibre4">sklearn.neighbors.kneighbors_graph</span></code></a>。
在少量聚类的限制中, 更倾向于给出一些 macroscopically occupied clusters 并且几乎是空的 (讨论内容请查看
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="calibre4">Agglomerative clustering with and without structure</span></a>)。</p>
</div>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000367.jpg" class="calibre48" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000257.jpg" class="calibre48" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0031.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000414.jpg" class="calibre48" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_0041.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000744.jpg" class="calibre48" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-580">
<h3 class="sigil_not_in_toc1">2.3.6.3. Varying the metric</h3>
<p class="calibre2">Average and complete linkage 可以使用各种距离 (or affinities), 特别是 Euclidean distance (<em class="calibre13">l2</em>),
Manhattan distance（曼哈顿距离）(or Cityblock（城市区块距离）, or <em class="calibre13">l1</em>), cosine distance(余弦距离),</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">或者任何预先计算的 affinity matrix（亲和度矩阵）.</div>
</blockquote>
<ul class="calibre6">
<li class="toctree-l"><em class="calibre13">l1</em> distance 有利于稀疏特征或者稀疏噪声: 例如很多特征都是0，就想在文本挖掘中使用 rare words 一样。</li>
<li class="toctree-l"><em class="calibre13">cosine</em> distance 非常有趣因为它对全局放缩是一样的。</li>
</ul>
<p class="calibre10">选择度量标准的方针是使得不同类样本之间距离最大化，并且最小化同类样本之间的距离。</p>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000472.jpg" class="calibre49" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000177.jpg" class="calibre49" /></a>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000853.jpg" class="calibre49" /></a>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py"><span class="calibre4">Agglomerative clustering with different metrics</span></a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-21">
<span id="calibre_link-581" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.7. DBSCAN</h2>
<p class="calibre2">The <a class="calibre3 pcalibre" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="docutils"><span class="calibre4">DBSCAN</span></code></a> 算法将聚类视为被低密度区域分隔的高密度区域。由于这个相当普遍的观点，
DBSCAN发现的聚类可以是任何形状的，与假设聚类是 convex shaped 的 K-means 相反。
DBSCAN 的核心概念是 <em class="calibre13">core samples</em>, 是指位于高密度区域的样本。
因此一个聚类是一组核心样本，每个核心样本彼此靠近（通过一定距离度量测量）
和一组接近核心样本的非核心样本（但本身不是核心样本）。算法中的两个参数, <code class="docutils"><span class="calibre4">min_samples</span></code>
和 <code class="docutils"><span class="calibre4">eps</span></code>,正式的定义了我们所说的 <a href="#calibre_link-26" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-582">*</span></a>dense*（稠密）。较高的 <code class="docutils"><span class="calibre4">min_samples</span></code> 或者</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">较低的 <a href="#calibre_link-27" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-583">``</span></a>eps``表示形成聚类所需的较高密度。</div>
</blockquote>
<p class="calibre10">更正式的,我们定义核心样本是指数据集中的一个样本，存在 <code class="docutils"><span class="calibre4">min_samples</span></code> 个其他样本在 <code class="docutils"><span class="calibre4">eps</span></code>
距离范围内，这些样本被定为为核心样本的邻居 <em class="calibre13">neighbors</em> 。这告诉我们核心样本在向量空间稠密的区域。
一个聚类是一个核心样本的集合，可以通过递归来构建，选取一个核心样本，查找它所有的 neighbors （邻居样本）
中的核心样本，然后查找 <em class="calibre13">their</em> （新获取的核心样本）的 neighbors （邻居样本）中的核心样本，递归这个过程。
聚类中还具有一组非核心样本，它们是集群中核心样本的邻居的样本，但本身并不是核心样本。
显然，这些样本位于聚类的边缘。</p>
<dl class="calibre10">
<dt class="calibre18">根据定义，任何核心样本都是聚类的一部分，任何不是核心样本并且和任意一个核心样本距离都大于</dt>
<dd class="calibre19"><code class="docutils"><span class="calibre4">eps</span></code> 的样本将被视为异常值。</dd>
</dl>
<p class="calibre10">在下图中，颜色表示聚类成员属性，大圆圈表示算法发现的核心样本。 较小的圈子是仍然是群集的
一部分的非核心样本。 此外，异常值由下面的黑点表示。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_dbscan.html"><img alt="dbscan_results" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000029.jpg" class="calibre11" /></a></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"><span class="calibre4">Demo of DBSCAN clustering algorithm</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">实现</p>
<p class="calibre10">DBSCAN 算法是具有确定性的，当以相同的顺序给出相同的数据时总是形成相同的聚类。
然而，当以不同的顺序提供数据时聚类的结果可能不相同。首先，即使核心样本总是被
分配给相同的聚类，这些集群的标签将取决于数据中遇到这些样本的顺序。第二个更重
要的是，非核心样本的聚类可能因数据顺序而有所不同。
当一个非核心样本距离两个核心样本的距离都小于 <code class="docutils"><span class="calibre4">eps</span></code> 时，就会发生这种情况。
通过三角不等式可知，这两个核心样本距离一定大于 <code class="docutils"><span class="calibre4">eps</span></code> 或者处于同一个聚类中。
非核心样本将被非配到首先查找到改样本的类别，因此结果将取决于数据的顺序。</p>
<p class="calibre10">当前版本使用 ball trees 和 kd-trees 来确定领域，这样避免了计算全部的距离矩阵
（0.14 之前的 scikit-learn 版本计算全部的距离矩阵）。保留使用 custom metrics
（自定义指标）的可能性。细节请参照 <code class="docutils"><span class="calibre4">NearestNeighbors</span></code>。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">大量样本的内存消耗</p>
<p class="calibre10">默认的实现方式并没有充分利用内存，因为在不使用 kd-trees 或者 ball-trees 的情况下构建一个
完整的相似度矩阵（e.g. 使用稀疏矩阵）。这个矩阵将消耗 n^2 个浮点数。
解决这个问题的几种机制:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first">A sparse radius neighborhood graph （稀疏半径邻域图）(其中缺少条目被假定为距离超出eps)
可以以高效的方式预先编译，并且可以使用 <code class="docutils"><span class="calibre4">metric='precomputed'</span></code> 来运行 dbscan。</p>
</li>
<li class="toctree-l"><p class="first">数据可以压缩，当数据中存在准确的重复时，可以删除这些重复的数据，或者使用BIRCH。
任何。然后仅需要使用相对少量的样本来表示大量的点。当训练DBSCAN时，可以提供一个</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10"><code class="docutils"><span class="calibre4">sample_weight</span></code> 参数。</p>
</div>
</blockquote>
</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">引用:</p>
<ul class="calibre6">
<li class="toctree-l">“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
with Noise”
Ester, M., H. P. Kriegel, J. Sander, and X. Xu,
In Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226&ndash;231. 1996</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-22">
<span id="calibre_link-584" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.8. Birch</h2>
<p class="calibre2">The <a class="calibre3 pcalibre" href="generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" title="sklearn.cluster.Birch"><code class="docutils"><span class="calibre4">Birch</span></code></a> 为提供的数据构建一颗 Characteristic Feature Tree (CFT，聚类特征树)。
数据实质上是被有损压缩成一组 Characteristic Feature nodes (CF Nodes，聚类特征节点)。
CF Nodes 中有一部分子聚类被称为 Characteristic Feature subclusters (CF Subclusters)，
并且这些位于非终端位置的CF Subclusters 可以拥有 CF Nodes 作为孩子节点。</p>
<p class="calibre10">CF Subclusters 保存用于聚类的必要信息，防止将整个输入数据保存在内存中。
这些信息包括:</p>
<ul class="calibre6">
<li class="toctree-l">Number of samples in a subcluster（子聚类中样本数）.</li>
<li class="toctree-l">Linear Sum - A n-dimensional vector holding the sum of all samples（保存所有样本和的n维向量）</li>
<li class="toctree-l">Squared Sum - Sum of the squared L2 norm of all samples（所有样本的L2 norm的平方和）.</li>
<li class="toctree-l">Centroids - To avoid recalculation linear sum / n_samples（为了防止重复计算 linear sum / n_samples）.</li>
<li class="toctree-l">Squared norm of the centroids（质心的 Squared norm ）.</li>
</ul>
<p class="calibre10">Birch 算法有两个参数，即 threshold （阈值）和 branching factor 分支因子。Branching factor （分支因子）
限制了一个节点中的子集群的数量 ，threshold （簇半径阈值）限制了新加入的样本和存在与现有子集群中样本的最大距离。</p>
<p class="calibre10">该算法可以视为将一个实例或者数据简化的方法，因为它将输入的数据简化到可以直接从CFT的叶子结点中获取的一组子聚类。
这种简化的数据可以通过将其馈送到global clusterer（全局聚类）来进一步处理。Global clusterer（全局聚类）可以
通过 <a href="#calibre_link-28" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-585">``</span></a>n_clusters``来设置。</p>
<p class="calibre10">如果 <code class="docutils"><span class="calibre4">n_clusters</span></code> 被设置为 None，将直接读取叶子结点中的子聚类，否则，global clustering（全局聚类）
将逐步标记他的 subclusters 到 global clusters (labels) 中，样本将被映射到 距离最近的子聚类的global label中。</p>
<p class="calibre10"><strong class="calibre14">算法描述:</strong></p>
<ul class="calibre6">
<li class="toctree-l">一个新的样本作为一个CF Node 被插入到 CF Tree 的根节点。然后将其合并到根节点的子聚类中去，使得合并后子聚类
拥有最小的半径，子聚类的选取受 threshold 和 branching factor 的约束。如果子聚类也拥有孩子节点，则重复执
行这个步骤直到到达叶子结点。在叶子结点中找到最近的子聚类以后，递归的更新这个子聚类及其父聚类的属性。</li>
<li class="toctree-l">如果合并了新样本和最近的子聚类获得的子聚类半径大约square of the threshold（阈值的平方），
并且子聚类的数量大于branching factor（分支因子），则将为这个样本分配一个临时空间。
最远的两个子聚类被选取，剩下的子聚类按照之间的距离分为两组作为被选取的两个子聚类的孩子节点。</li>
<li class="toctree-l">If this split node has a parent subcluster and there is room
for a new subcluster, then the parent is split into two. If there is no room,
then this node is again split into two and the process is continued
recursively, till it reaches the root.
如果拆分的节点有一个 parent subcluster ，并且有一个容纳一个新的子聚类的空间，那么父聚类拆分成两个。
如果没有空间容纳一个新的聚类，那么这个节点将被再次拆分，依次向上检查父节点是否需要分裂，
如果需要按叶子节点方式相同分裂。</li>
</ul>
<p class="calibre10"><strong class="calibre14">Birch or MiniBatchKMeans?</strong></p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">Birch 在高维数据上表现不好。一条经验法则，如果 <code class="docutils"><span class="calibre4">n_features</span></code> 大于20，通常使用 MiniBatchKMeans 更好。</li>
<li class="toctree-l">如果需要减少数据实例的数量，或者如果需要大量的子聚类作为预处理步骤或者其他， Birch 比 MiniBatchKMeans 更有用。</li>
</ul>
</div>
</blockquote>
<p class="calibre10"><strong class="calibre14">How to use partial_fit?</strong></p>
<p class="calibre10">为了避免对 global clustering 的计算，每次调用建议使用  <code class="docutils"><span class="calibre4">partial_fit</span></code> 。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">初始化 <code class="docutils"><span class="calibre4">n_clusters=None</span></code> 。</li>
<li class="toctree-l">通过多次调用 partial_fit 训练所以的数据。</li>
<li class="toctree-l">设置 <code class="docutils"><span class="calibre4">n_clusters</span></code> 为所需值，通过使用 <code class="docutils"><span class="calibre4">brc.set_params(n_clusters=n_clusters)</span></code> 。</li>
<li class="toctree-l">最后不需要参数调用 <code class="docutils"><span class="calibre4">partial_fit</span></code> , 例如 <code class="docutils"><span class="calibre4">brc.partial_fit()</span></code> 执行全局聚类。</li>
</ol>
</div>
</blockquote>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_birch_vs_minibatchkmeans_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000628.jpg" class="math" /></a>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
<a class="calibre3 pcalibre" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li class="toctree-l">Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
<a class="calibre3 pcalibre" href="https://code.google.com/archive/p/jbirch">https://code.google.com/archive/p/jbirch</a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-586">
<span id="calibre_link-587" class="calibre4"></span><h2 class="sigil_not_in_toc">2.3.9. 聚类性能度量</h2>
<p class="calibre2">度量聚类算法的性能不是简单的统计错误的数量或计算监督分类算法中的 precision （准确率）和 recall （召回率）。
特别地，任何 evaluation metric （度量指标）不应该考虑到 cluster labels （簇标签）的绝对值，而是如果这个簇定义类似于某些 ground truth set of classes 或者满足某些假设，使得属于同一个类的成员更类似于根据某些 similarity metric （相似性度量）的不同类的成员。</p>
<div class="toctree-wrapper" id="calibre_link-588">
<span id="calibre_link-589" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.1. 调整后的 Rand 指数</h3>
<p class="calibre2">考虑到 the ground truth class 赋值 <code class="docutils"><span class="calibre4">labels_true</span></code> 和相同样本 <code class="docutils"><span class="calibre4">labels_pred</span></code> 的聚类算法分配的知识，<strong class="calibre14">adjusted Rand index</strong> 是一个函数，用于测量两个 assignments （任务）的 <strong class="calibre14">similarity（相似度）</strong> ，忽略 permutations （排列）和 <strong class="calibre14">with chance normalization（使用机会规范化）</strong>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_rand_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.24...</span>
</pre>
</div>
</div>
<p class="calibre10">可以在预测的标签中 permute （排列） 0 和 1，重命名为 2 到 3， 得到相同的分数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_rand_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.24...</span>
</pre>
</div>
</div>
<p class="calibre10">另外， <a class="calibre3 pcalibre" href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="docutils"><span class="calibre4">adjusted_rand_score</span></code></a> 是 <strong class="calibre14">symmetric（对称的）</strong> : 交换参数不会改变 score （得分）。它可以作为 <strong class="calibre14">consensus measure（共识度量）</strong>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_rand_score</span><span class="calibre4">(</span><span class="calibre4">labels_pred</span><span class="calibre4">,</span> <span class="calibre4">labels_true</span><span class="calibre4">)</span>  
<span class="calibre4">0.24...</span>
</pre>
</div>
</div>
<p class="calibre10">完美的标签得分为 1.0</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">labels_true</span><span class="calibre4">[:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_rand_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>
</pre>
</div>
</div>
<p class="calibre10">坏 (e.g. independent labelings（独立标签）) 有负数 或 接近于 0.0 分:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_rand_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">-0.12...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-590">
<h4 class="sigil_not_in_toc1">2.3.9.1.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Random (uniform) label assignments have a ARI score close to 0.0（随机（统一）标签分配的 ARI 评分接近于 0.0）</strong>
对于 <code class="docutils"><span class="calibre4">n_clusters</span></code> 和 <code class="docutils"><span class="calibre4">n_samples</span></code> 的任何值（这不是原始的 Rand index 或者 V-measure 的情况）。</li>
<li class="toctree-l"><strong class="calibre14">Bounded range（范围是有界的） [-1, 1]</strong>: negative values （负值）是坏的 (独立性标签), 类似的聚类有一个 positive ARI （正的 ARI）， 1.0 是完美的匹配得分。</li>
<li class="toctree-l"><strong class="calibre14">No assumption is made on the cluster structure（对簇的结构不需作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms（谱聚类算法）的结果。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-591">
<h4 class="sigil_not_in_toc1">2.3.9.1.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l"><p class="first">与 inertia 相反，<strong class="calibre14">ARI requires knowledge of the ground truth classes（ARI 需要 ground truth classes 的相关知识）</strong> ，而在实践中几乎不可用，或者需要人工标注者手动分配（如在监督学习环境中）。</p>
<p class="calibre10">然而，ARI 还可以在 purely unsupervised setting （纯粹无监督的设置中）作为可用于 聚类模型选择（TODO）的共识索引的构建块。</p>
</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="calibre4">Adjustment for chance in clustering performance evaluation</span></a>: 分析数据集大小对随机分配聚类度量值的影响。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-592">
<h4 class="sigil_not_in_toc1">2.3.9.1.3. 数学表达</h4>
<p class="calibre2">如果 C 是一个 ground truth class assignment（任务）， K 是簇的个数，我们定义 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000307.jpg" alt="a" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000303.jpg" alt="b" /> 如:</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000307.jpg" alt="a" />, 在 C 中的相同集合的与 K 中的相同集合中的元素的对数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000303.jpg" alt="b" />, 在 C 中的不同集合与 K 中的不同集合中的元素的对数</li>
</ul>
<p class="calibre10">原始（未经调整）的 Rand index 则由下式给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000368.jpg" alt="\text{RI} = \frac{a + b}{C_2^{n_{samples}}}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000066.jpg" alt="C_2^{n_{samples}}" /> 是数据集中可能的 pairs （数据对）的总数（不排序）。</p>
<p class="calibre10">然而，RI 评分不能保证 random label assignments （随机标签任务）将获得接近零的值（特别是如果簇的数量与采样数量相同的数量级）。</p>
<p class="calibre10">为了抵消这种影响，我们可以通过定义 adjusted Rand index （调整后的 Rand index）来 discount（折现） 随机标签的预期 RI <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000086.jpg" alt="E[\text{RI}]" /> ,如下所示:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000254.jpg" alt="\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://link.springer.com/article/10.1007%2FBF01908075">Comparing Partitions</a>
L. Hubert and P. Arabie, Journal of Classification 1985</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">Wikipedia entry for the adjusted Rand index</a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-593">
<span id="calibre_link-594" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.2. 基于 Mutual Information （互信息）的分数</h3>
<p class="calibre2">考虑到 ground truth class assignments （标定过的真实数据类分配） <code class="docutils"><span class="calibre4">labels_true</span></code> 的知识和相同样本 <code class="docutils"><span class="calibre4">labels_pred</span></code> 的聚类算法分配， <strong class="calibre14">Mutual Information</strong> 是测量两者 <strong class="calibre14">agreement</strong> 分配的函数，忽略 permutations（排列）。
这种测量方案的两个不同的标准化版本可用，<strong class="calibre14">Normalized Mutual Information(NMI)</strong> 和 <strong class="calibre14">Adjusted Mutual Information(AMI)</strong>。NMI 经常在文献中使用，而 AMI 最近被提出，并且 <strong class="calibre14">normalized against chance</strong>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.22504...</span>
</pre>
</div>
</div>
<p class="calibre10">可以在 predicted labels （预测的标签）中 permute （排列） 0 和 1, 重命名为 2 到 3 并得到相同的得分:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.22504...</span>
</pre>
</div>
</div>
<p class="calibre10">全部的，<a class="calibre3 pcalibre" href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code class="docutils"><span class="calibre4">mutual_info_score</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="docutils"><span class="calibre4">adjusted_mutual_info_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="docutils"><span class="calibre4">normalized_mutual_info_score</span></code></a> 是 symmetric（对称的）: 交换参数不会更改分数。因此，它们可以用作 <strong class="calibre14">consensus measure</strong>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_pred</span><span class="calibre4">,</span> <span class="calibre4">labels_true</span><span class="calibre4">)</span>  
<span class="calibre4">0.22504...</span>
</pre>
</div>
</div>
<p class="calibre10">完美标签得分是 1.0:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">labels_true</span><span class="calibre4">[:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">normalized_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>
</pre>
</div>
</div>
<p class="calibre10">这对于 <code class="docutils"><span class="calibre4">mutual_info_score</span></code> 是不正确的，因此更难判断:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.69...</span>
</pre>
</div>
</div>
<p class="calibre10">坏的 (例如 independent labelings（独立标签）) 具有非正分数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">adjusted_mutual_info_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">-0.10526...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-595">
<h4 class="sigil_not_in_toc1">2.3.9.2.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Random (uniform) label assignments have a AMI score close to 0.0（随机（统一）标签分配的AMI评分接近0.0）</strong>
对于 <code class="docutils"><span class="calibre4">n_clusters</span></code> 和 <code class="docutils"><span class="calibre4">n_samples</span></code> 的任何值（这不是原始 Mutual Information 或者 V-measure 的情况）。</li>
<li class="toctree-l"><strong class="calibre14">Bounded range（有界范围） [0, 1]</strong>:  接近 0 的值表示两个主要独立的标签分配，而接近 1 的值表示重要的一致性。此外，正好 0 的值表示 <strong class="calibre14">purely（纯粹）</strong> 独立标签分配，正好为 1 的 AMI 表示两个标签分配相等（有或者没有 permutation）。</li>
<li class="toctree-l"><strong class="calibre14">No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms （频谱聚类算法）的结果。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-596">
<h4 class="sigil_not_in_toc1">2.3.9.2.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l"><p class="first">与 inertia 相反，<strong class="calibre14">MI-based measures require the knowledge of the ground truth classes（MI-based measures 需要了解 ground truth classes）</strong> ，而在实践中几乎不可用，或者需要人工标注或手动分配（如在监督学习环境中）。</p>
<p class="calibre10">然而，基于 MI-based measures （基于 MI 的测量方式）也可用于纯无人监控的设置，作为可用于聚类模型选择的 Consensus Index （共识索引）的构建块。</p>
</li>
<li class="toctree-l"><p class="first">NMI 和 MI 没有调整机会。</p>
</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="calibre4">Adjustment for chance in clustering performance evaluation</span></a>: 分析数据集大小对随机分配聚类度量值的影响。 此示例还包括 Adjusted Rand Index。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-597">
<h4 class="sigil_not_in_toc1">2.3.9.2.3. 数学公式</h4>
<p class="calibre2">假设两个标签分配（相同的 N 个对象），<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000736.jpg" alt="U" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000291.jpg" alt="V" />。
它们的 entropy （熵）是一个 partition set （分区集合）的不确定性量，定义如下:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000338.jpg" alt="H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000854.jpg" alt="P(i) = |U_i| / N" /> 是从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000736.jpg" alt="U" /> 中随机选取的对象到类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000601.jpg" alt="U_i" /> 的概率。同样对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000291.jpg" alt="V" />:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000761.jpg" alt="H(V) = - \sum_{j=1}^{|V|}P&apos;(j)\log(P&apos;(j))" class="math" /></p>
</div>
<p class="calibre10">使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000246.jpg" alt="P&apos;(j) = |V_j| / N" />. <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000736.jpg" alt="U" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000291.jpg" alt="V" /> 之间的 mutual information (MI) 由下式计算:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000236.jpg" alt="\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P&apos;(j)}\right)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000861.jpg" alt="P(i, j) = |U_i \cap V_j| / N" /> 是随机选择的对象落入两个类的概率 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000601.jpg" alt="U_i" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000221.jpg" alt="V_j" /> 。</p>
<p class="calibre10">也可以用设定的基数表达式表示:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000595.jpg" alt="\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)" class="math" /></p>
</div>
<p class="calibre10">normalized (归一化) mutual information 被定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000461.jpg" alt="\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\sqrt{H(U)H(V)}}" class="math" /></p>
</div>
<p class="calibre10">mutual information 的价值以及 normalized variant （标准化变量）的值不会因 chance （机会）而被调整，随着不同标签（clusters（簇））的数量的增加，不管标签分配之间的 “mutual information” 的实际数量如何，都会趋向于增加。</p>
<p class="calibre10">mutual information 的期望值可以用 Vinh, Epps 和 Bailey,(2009) 的以下公式来计算。在这个方程式中,
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000197.jpg" alt="a_i = |U_i|" /> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000601.jpg" alt="U_i" /> 中元素的数量) 和
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000055.jpg" alt="b_j = |V_j|" /> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000221.jpg" alt="V_j" /> 中元素的数量).</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000322.jpg" alt="E[\text{MI}(U,V)]=\sum_{i=1}^|U| \sum_{j=1}^|V| \sum_{n_{ij}=(a_i+b_j-N)^+ }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right) \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})! (N-a_i-b_j+n_{ij})!}" class="math" /></p>
</div>
<p class="calibre10">使用期望值, 然后可以使用与 adjusted Rand index 相似的形式来计算调整后的 mutual information:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000556.jpg" alt="\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\max(H(U), H(V)) - E[\text{MI}]}" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l">Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles &ndash; a
knowledge reuse framework for combining multiple partitions”. Journal of
Machine Learning Research 3: 583&ndash;617.
<a class="calibre3 pcalibre" href="http://strehl.com/download/strehl-jmlr02.pdf">doi:10.1162/153244303321897735</a>.</li>
<li class="toctree-l">Vinh, Epps, and Bailey, (2009). “Information theoretic measures
for clusterings comparison”. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML ‘09.
<a class="calibre3 pcalibre" href="https://dl.acm.org/citation.cfm?doid=1553374.1553511">doi:10.1145/1553374.1553511</a>.
ISBN 9781605585161.</li>
<li class="toctree-l">Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance, JMLR
<a class="calibre3 pcalibre" href="http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf">http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Mutual_Information">Wikipedia entry for the (normalized) Mutual Information</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Adjusted_Mutual_Information">Wikipedia entry for the Adjusted Mutual Information</a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-598">
<span id="calibre_link-599" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.3. 同质性，完整性和 V-measure</h3>
<p class="calibre2">鉴于样本的 ground truth class assignments （标定过的真实数据类分配）的知识，可以使用 conditional entropy （条件熵）分析来定义一些 intuitive metric（直观的度量）。</p>
<p class="calibre10">特别是 Rosenberg 和 Hirschberg (2007) 为任何 cluster （簇）分配定义了以下两个理想的目标:</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">homogeneity(同质性)</strong>: 每个簇只包含一个类的成员</li>
<li class="toctree-l"><strong class="calibre14">completeness(完整性)</strong>: 给定类的所有成员都分配给同一个簇。</li>
</ul>
<p class="calibre10">我们可以把这些概念作为分数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="docutils"><span class="calibre4">homogeneity_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="docutils"><span class="calibre4">completeness_score</span></code></a> 。两者均在 0.0 以下 和 1.0 以上（越高越好）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">homogeneity_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.66...</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">completeness_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span> 
<span class="calibre4">0.42...</span>
</pre>
</div>
</div>
<p class="calibre10">称为 <strong class="calibre14">V-measure</strong> 的 harmonic mean 由以下函数计算 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="docutils"><span class="calibre4">v_measure_score</span></code></a>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">v_measure_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>    
<span class="calibre4">0.51...</span>
</pre>
</div>
</div>
<p class="calibre10">V-measure 实际上等于上面讨论的 mutual information (NMI) 由 label entropies <a class="calibre3 pcalibre" href="#calibre_link-29" id="calibre_link-30">[B2011]</a> （标准熵 <a class="calibre3 pcalibre" href="#calibre_link-29" id="calibre_link-31">[B2011]</a>） 的总和 normalized （归一化）。</p>
<p class="calibre10">Homogeneity（同质性）, completeness（完整性） and V-measure 可以立即计算 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure" title="sklearn.metrics.homogeneity_completeness_v_measure"><code class="docutils"><span class="calibre4">homogeneity_completeness_v_measure</span></code></a> 如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">homogeneity_completeness_v_measure</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                     
<span class="calibre4">(0.66..., 0.42..., 0.51...)</span>
</pre>
</div>
</div>
<p class="calibre10">以下聚类分配稍微好一些，因为它是同构但不完整的:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">homogeneity_completeness_v_measure</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                     
<span class="calibre4">(1.0, 0.68..., 0.81...)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="docutils"><span class="calibre4">v_measure_score</span></code></a> 是 <strong class="calibre14">symmetric（对称的）</strong>: 它可以用于评估同一数据集上两个 independent assignments （独立赋值）的 <strong class="calibre14">agreement（协议）</strong>。</p>
<p class="calibre10">这不是这样的 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="docutils"><span class="calibre4">completeness_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="docutils"><span class="calibre4">homogeneity_score</span></code></a>: 两者的关系是被这样约束着:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">homogeneity_score</span><span class="calibre4">(</span><span class="calibre4">a</span><span class="calibre4">,</span> <span class="calibre4">b</span><span class="calibre4">)</span> <span class="calibre4">==</span> <span class="calibre4">completeness_score</span><span class="calibre4">(</span><span class="calibre4">b</span><span class="calibre4">,</span> <span class="calibre4">a</span><span class="calibre4">)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-600">
<h4 class="sigil_not_in_toc1">2.3.9.3.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Bounded scores（分数是有界的）</strong>: 0.0 是最坏的, 1.0 是一个完美的分数.</li>
<li class="toctree-l">Intuitive interpretation（直观解释）: 具有不良 V-measure 的聚类可以在 <strong class="calibre14">qualitatively analyzed in terms of homogeneity and completeness（在同质性和完整性方面进行定性分析）</strong> 以更好地感知到作业完成的错误类型。</li>
<li class="toctree-l"><strong class="calibre14">No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较聚类算法，例如 k-means ，其假定 isotropic blob shapes 与可以找到具有 “folded” shapes 的聚类的 spectral clustering algorithms （频谱聚类算法）的结果。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-601">
<h4 class="sigil_not_in_toc1">2.3.9.3.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l"><p class="first">以前引入的 metrics （度量标准）**not normalized with regards to random labeling（并不是随机标记的标准化的）**: 这意味着，根据 number of samples （样本数量），clusters （簇）和 ground truth classes （标定过的真实数据类），完全随机的标签并不总是产生 homogeneity （同质性），completeness（完整性）和 hence v-measure 的相同值。特别是 <strong class="calibre14">random labeling won’t yield zero scores especially when the number of clusters is large（随机标记不会产生零分，特别是当集群数量大时）</strong>。</p>
<p class="calibre10">当样本数量超过 1000，簇的数量小于 10 时，可以安全地忽略此问题。<strong class="calibre14">For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)（对于较小的样本数量或者较大数量的簇，使用 adjusted index 例如 Adjusted Rand Index (ARI)）</strong>。</p>
</li>
</ul>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_adjusted_for_chance_measures_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000098.jpg" class="calibre50" /></a>
</div>
<ul class="calibre6">
<li class="toctree-l">这些 metrics （指标） <strong class="calibre14">require the knowledge of the ground truth classes（需要标定过的真实数据类的知识）</strong>，而在实践中几乎不可用，或需要人工标注来人工分配（如在受监督的学习环境中）。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="calibre4">Adjustment for chance in clustering performance evaluation</span></a>: 分析数据集大小对随机分配聚类度量值的影响。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-602">
<h4 class="sigil_not_in_toc1">2.3.9.3.3. 数学表达</h4>
<p class="calibre2">Homogeneity（同质性） 和 completeness（完整性） 的得分由下面公式给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000559.jpg" alt="h = 1 - \frac{H(C|K)}{H(C)}" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000225.jpg" alt="c = 1 - \frac{H(K|C)}{H(K)}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000679.jpg" alt="H(C|K)" /> 是 <strong class="calibre14">给定簇分配的类的 conditional entropy （条件熵）</strong> ，由下式给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000204.jpg" alt="H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n} \cdot \log\left(\frac{n_{c,k}}{n_k}\right)" class="math" /></p>
</div>
<p class="calibre10">并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000787.jpg" alt="H(C)" /> 是 <strong class="calibre14">entropy of the classes（类的熵）</strong>，并且由下式给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000621.jpg" alt="H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个样本总数， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000214.jpg" alt="n_c" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000487.jpg" alt="n_k" /> 分别属于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000577.jpg" alt="c" /> 类和簇 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的样本数，最后 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000614.jpg" alt="n_{c,k}" /> 分配给簇 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的类 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000577.jpg" alt="c" /> 的样本数。</p>
<p class="calibre10"><strong class="calibre14">conditional entropy of clusters given class（给定类的条件熵）</strong> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000618.jpg" alt="H(K|C)" /> 和 <strong class="calibre14">entropy of clusters（类的熵）</strong> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000182.jpg" alt="H(K)" /> 以 symmetric manner （对称方式）定义。</p>
<p class="calibre10">Rosenberg 和 Hirschberg 进一步定义 <strong class="calibre14">V-measure</strong> 作为 <strong class="calibre14">harmonic mean of homogeneity and completeness（同质性和完整性的 harmonic mean）</strong>:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000292.jpg" alt="v = 2 \cdot \frac{h \cdot c}{h + c}" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://aclweb.org/anthology/D/D07/D07-1043.pdf">V-Measure: A conditional entropy-based external cluster evaluation
measure</a>
Andrew Rosenberg and Julia Hirschberg, 2007</li>
</ul>
<table class="docutils1" frame="void" id="calibre_link-29" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[B2011]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-30">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-31">2</a>)</em> <a class="calibre3 pcalibre" href="http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf">Identication and Characterization of Events in Social Media</a>, Hila
Becker, PhD Thesis.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-603">
<span id="calibre_link-604" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.4. Fowlkes-Mallows 分数</h3>
<p class="calibre2">当样本的已标定的真实数据的类别分配已知时，可以使用 Fowlkes-Mallows index （Fowlkes-Mallows 指数）(<a class="calibre3 pcalibre" href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code class="docutils"><span class="calibre4">sklearn.metrics.fowlkes_mallows_score</span></code></a>) 。Fowlkes-Mallows 分数 FMI 被定义为 geometric mean of the pairwise precision （成对的准确率）和 recall （召回率）的几何平均值:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000834.jpg" alt="\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}" class="math" /></p>
</div>
<p class="calibre10">其中的 <code class="docutils"><span class="calibre4">TP</span></code> 是 <strong class="calibre14">True Positive（真正例）</strong> 的数量（即，真实标签和预测标签中属于相同簇的点对数），<code class="docutils"><span class="calibre4">FP</span></code> 是 <strong class="calibre14">False Positive（假正例）</strong> （即，在真实标签中属于同一簇的点对数，而不在预测标签中），<code class="docutils"><span class="calibre4">FN</span></code> 是 <strong class="calibre14">False Negative（假负例）</strong> 的数量（即，预测标签中属于同一簇的点对数，而不在真实标签中）。</p>
<p class="calibre10">score （分数）范围为 0 到 1。较高的值表示两个簇之间的良好相似性。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fowlkes_mallows_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.47140...</span>
</pre>
</div>
</div>
<p class="calibre10">可以在 predicted labels （预测的标签）中 permute （排列） 0 和 1 ，重命名为 2 到 3 并得到相同的得分:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fowlkes_mallows_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.47140...</span>
</pre>
</div>
</div>
<p class="calibre10">完美的标签得分是 1.0:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">labels_true</span><span class="calibre4">[:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fowlkes_mallows_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">1.0</span>
</pre>
</div>
</div>
<p class="calibre10">坏的（例如 independent labelings （独立标签））的标签得分为 0:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fowlkes_mallows_score</span><span class="calibre4">(</span><span class="calibre4">labels_true</span><span class="calibre4">,</span> <span class="calibre4">labels_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.0</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-605">
<h4 class="sigil_not_in_toc1">2.3.9.4.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Random (uniform) label assignments have a FMI score close to 0.0（随机（统一）标签分配 FMI 得分接近于 0.0）</strong> 对于 <code class="docutils"><span class="calibre4">n_clusters</span></code> 和 <code class="docutils"><span class="calibre4">n_samples</span></code> 的任何值（对于原始 Mutual Information 或例如 V-measure 而言）。</li>
<li class="toctree-l"><strong class="calibre14">Bounded range（有界范围） [0, 1]</strong>:  接近于 0 的值表示两个标签分配在很大程度上是独立的，而接近于 1 的值表示 significant agreement 。此外，正好为 0 的值表示 <strong class="calibre14">purely</strong> 独立标签分配，正好为 1 的 AMI 表示两个标签分配相等（有或者没有 permutation （排列））。</li>
<li class="toctree-l"><strong class="calibre14">No assumption is made on the cluster structure（对簇的结构没有作出任何假设）</strong>: 可以用于比较诸如 k-means 的聚类算法，其将假设 isotropic blob shapes 与能够找到具有 “folded” shapes 的簇的 spectral clustering algorithms （频谱聚类算法）的结果相结合。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-606">
<h4 class="sigil_not_in_toc1">2.3.9.4.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l">与 inertia（习惯）相反，<strong class="calibre14">FMI-based measures require the knowledge of the ground truth classes（基于 FMI 的测量方案需要了解已标注的真是数据的类）</strong> ，而几乎不用于实践和需要人工标注者的手动任务（如在监督学习的学习环境中）。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l">E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical Association.
<a class="calibre3 pcalibre" href="http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf">http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Fowlkes-Mallows_index">Wikipedia entry for the Fowlkes-Mallows Index</a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-607">
<span id="calibre_link-608" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.5. Silhouette 系数</h3>
<p class="calibre2">如果标注过的真实数据的标签不知道，则必须使用模型本身进行度量。Silhouette Coefficient (<a class="calibre3 pcalibre" href="generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" title="sklearn.metrics.silhouette_score"><code class="docutils"><span class="calibre4">sklearn.metrics.silhouette_score</span></code></a>) 是一个这样的评估的例子，其中较高的 Silhouette Coefficient 得分与具有更好定义的聚类的模型相关。Silhouette Coefficient 是为每个样本定义的，由两个得分组成:</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">a</strong>: 样本与同一类别中所有其他点之间的平均距离。</li>
<li class="toctree-l"><strong class="calibre14">b</strong>: 样本与 <em class="calibre13">下一个距离最近的簇</em> 中的所有其他点之间的平均距离。</li>
</ul>
<p class="calibre10">然后将单个样本的 Silhouette 系数 <em class="calibre13">s</em> 给出为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000278.jpg" alt="s = \frac{b - a}{max(a, b)}" class="math" /></p>
</div>
<p class="calibre10">给定一组样本的 Silhouette 系数作为每个样本的 Silhouette 系数的平均值。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">pairwise_distances</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dataset</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">dataset</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">dataset</span><span class="calibre4">.</span><span class="calibre4">target</span>
</pre>
</div>
</div>
<p class="calibre10">在正常使用情况下，将 Silhouette 系数应用于聚类分析的结果。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.cluster</span> <span class="calibre4">import</span> <span class="calibre4">KMeans</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kmeans_model</span> <span class="calibre4">=</span> <span class="calibre4">KMeans</span><span class="calibre4">(</span><span class="calibre4">n_clusters</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels</span> <span class="calibre4">=</span> <span class="calibre4">kmeans_model</span><span class="calibre4">.</span><span class="calibre4">labels_</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">silhouette_score</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">,</span> <span class="calibre4">metric</span><span class="calibre4">=</span><span class="calibre4">'euclidean'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                     
<span class="calibre4">0.55...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l">Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”. Computational
and Applied Mathematics 20: 53&ndash;65.
<a class="calibre3 pcalibre" href="http://dx.doi.org/10.1016/0377-0427(87)90125-7">doi:10.1016/0377-0427(87)90125-7</a>.</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-609">
<h4 class="sigil_not_in_toc1">2.3.9.5.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l">对于不正确的 clustering （聚类），分数为 -1 ， highly dense clustering （高密度聚类）为 +1 。零点附近的分数表示 overlapping clusters （重叠的聚类）。</li>
<li class="toctree-l">当 clusters （簇）密集且分离较好时，分数更高，这与 cluster （簇）的标准概念有关。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-610">
<h4 class="sigil_not_in_toc1">2.3.9.5.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l">convex clusters（凸的簇）的 Silhouette Coefficient 通常比其他类型的 cluster （簇）更高，例如通过 DBSCAN 获得的基于密度的 cluster（簇）。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"><span class="calibre4">Selecting the number of clusters with silhouette analysis on KMeans clustering</span></a> : 在这个例子中，silhouette 分析用于为 n_clusters 选择最佳值.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-611">
<span id="calibre_link-612" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.3.9.6. Calinski-Harabaz 指数</h3>
<p class="calibre2">如果不知道真实数据的类别标签，则可以使用 Calinski-Harabaz 指数 (<a class="calibre3 pcalibre" href="generated/sklearn.metrics.calinski_harabaz_score.html#sklearn.metrics.calinski_harabaz_score" title="sklearn.metrics.calinski_harabaz_score"><code class="docutils"><span class="calibre4">sklearn.metrics.calinski_harabaz_score</span></code></a>) 来评估模型，其中较高的 Calinski-Harabaz 的得分与具有更好定义的聚类的模型相关。</p>
<p class="calibre10">对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 簇，Calinski-Harabaz 得分 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000242.jpg" alt="s" /> 是作为 between-clusters dispersion mean （簇间色散平均值）与 within-cluster dispersion（群内色散之间）的比值给出的:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000741.jpg" alt="s(k) = \frac{\mathrm{Tr}(B_k)}{\mathrm{Tr}(W_k)} \times \frac{N - k}{k - 1}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000795.jpg" alt="B_K" /> 是 between group dispersion matrix （组间色散矩阵）， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000394.jpg" alt="W_K" /> 是由以下定义的 within-cluster dispersion matrix （群内色散矩阵）:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000855.jpg" alt="W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000265.jpg" alt="B_k = \sum_q n_q (c_q - c) (c_q - c)^T" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 为数据中的点数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000381.jpg" alt="C_q" /> 为 cluster （簇） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> 中的点集， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000464.jpg" alt="c_q" /> 为 cluster（簇） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> 的中心， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000577.jpg" alt="c" /> 为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000190.jpg" alt="E" /> 的中心， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000856.jpg" alt="n_q" /> 为 cluster（簇） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> 中的点数。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">pairwise_distances</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">dataset</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">dataset</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">dataset</span><span class="calibre4">.</span><span class="calibre4">target</span>
</pre>
</div>
</div>
<p class="calibre10">在正常使用情况下，将 Calinski-Harabaz index （Calinski-Harabaz 指数）应用于 cluster analysis （聚类分析）的结果。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.cluster</span> <span class="calibre4">import</span> <span class="calibre4">KMeans</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kmeans_model</span> <span class="calibre4">=</span> <span class="calibre4">KMeans</span><span class="calibre4">(</span><span class="calibre4">n_clusters</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels</span> <span class="calibre4">=</span> <span class="calibre4">kmeans_model</span><span class="calibre4">.</span><span class="calibre4">labels_</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">calinski_harabaz_score</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">)</span>  
<span class="calibre4">560.39...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-613">
<h4 class="sigil_not_in_toc1">2.3.9.6.1. 优点</h4>
<ul class="calibre6">
<li class="toctree-l">当 cluster （簇）密集且分离较好时，分数更高，这与一个标准的 cluster（簇）有关。</li>
<li class="toctree-l">得分计算很快</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-614">
<h4 class="sigil_not_in_toc1">2.3.9.6.2. 缺点</h4>
<ul class="calibre6">
<li class="toctree-l">凸的簇的 Calinski-Harabaz index（Calinski-Harabaz 指数）通常高于其他类型的 cluster（簇），例如通过 DBSCAN 获得的基于密度的 cluster（簇）。</li>
</ul>
<div class="toctree-wrapper">
<p class="calibre10">参考</p>
<ul class="calibre6">
<li class="toctree-l">Caliński, T., &amp; Harabasz, J. (1974). “A dendrite method for cluster
analysis”. Communications in Statistics-theory and Methods 3: 1-27.
<a class="calibre3 pcalibre" href="http://dx.doi.org/10.1080/03610926.2011.560741">doi:10.1080/03610926.2011.560741</a>.</li>
</ul>
</div>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-136">
<span id="calibre_link-615" class="calibre4"></span><h1 class="calibre5">2.4. 双聚类</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@udy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
    </div>
<p class="calibre10">Biclustering 可以使用
<a class="calibre3 pcalibre" href="classes.html#module-sklearn.cluster.bicluster" title="sklearn.cluster.bicluster"><code class="docutils"><span class="calibre4">sklearn.cluster.bicluster</span></code></a> 模块。 Biclustering 算法对数据矩阵的行列同时进行聚类。 同时对行列进行聚类称之为
biclusters。 每一次聚类都会通过原始数据矩阵的一些属性确定一个子矩阵。</p>
<p class="calibre10">例如, 一个矩阵 <code class="docutils"><span class="calibre4">(10,</span> <span class="calibre4">10)</span></code> , 一个 bicluster 聚类，有三列二行，就是一个子矩阵 <code class="docutils"><span class="calibre4">(3,</span> <span class="calibre4">2)</span></code></p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">data</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">100</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rows</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">])[:,</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">newaxis</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">columns</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">data</span><span class="calibre4">[</span><span class="calibre4">rows</span><span class="calibre4">,</span> <span class="calibre4">columns</span><span class="calibre4">]</span>
<span class="calibre4">array([[ 1,  2],</span>
<span class="calibre4">       [21, 22],</span>
<span class="calibre4">       [31, 32]])</span>
</pre>
</div>
</div>
<p class="calibre10">为了可视化， 给定一个 bicluster 聚类，数据矩阵的行列可以重新分配，使得 bi-cluster 是连续的。</p>
<p class="calibre10">算法在如何定义 bicluster 方面有一些不同，常见类型包括：</p>
<ul class="calibre6">
<li class="toctree-l">不变的 values , 不变的 rows, 或者不变的 columns。</li>
<li class="toctree-l">异常高的或者低的值。</li>
<li class="toctree-l">低方差的子矩阵。</li>
<li class="toctree-l">相关的 rows 或者 columns。</li>
</ul>
<p class="calibre10">算法在分配给 bicluster 行列的方式不同, 会导致不同的 bicluster 结构。 当行和列分成分区时，会发生对角线或者棋盘结构。</p>
<p class="calibre10">如果每一行和每一列同属于一种 bicluster ,就重新排列数据矩阵的行和列,会使得 bicluster 呈现对角线。 下面是一个例子，此结构的biclusters 具有比其他行列更高的平均值:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/bicluster/images/sphx_glr_plot_spectral_coclustering_003.png"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_spectral_coclustering_0031.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000423.jpg" class="calibre44" /></a>
</div>
<p class="calibre10">在棋盘结构的例子中, 每一行属于所有的列类别, 每一列属于所有的行类别。 下面是一个例子，每个 bicluster 中的值差异较小:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/bicluster/images/sphx_glr_plot_spectral_biclustering_003.png"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_spectral_biclustering_0031.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000155.jpg" class="calibre44" /></a>
</div>
<p class="calibre10">在拟合模型之后， 可以在 <code class="docutils"><span class="calibre4">rows_</span></code> 和 <code class="docutils"><span class="calibre4">columns_</span></code> 属性中找到行列 cluster membership 。
<code class="docutils"><span class="calibre4">rows_[i]</span></code> 是一个二进制的向量，
就是属于 bicluster <code class="docutils"><span class="calibre4">i</span></code> 的一行。
同样的, <code class="docutils"><span class="calibre4">columns_[i]</span></code> 就表示属于 bicluster <code class="docutils"><span class="calibre4">i</span></code> 的列。</p>
<p class="calibre10">一些模块也有 <code class="docutils"><span class="calibre4">row_labels_</span></code> 何 <code class="docutils"><span class="calibre4">column_labels_</span></code> 属性。
这些模块对行列进行分区, 例如对角线或者棋盘 bicluster 结构。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">Biclustering 在不同的领域有很多其他名称，包括 co-clustering, two-mode clustering, two-way clustering, block
clustering, coupled two-way clustering 等.有一些算法的名称，比如 Spectral Co-Clustering algorithm, 反应了这些备用名称。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-616">
<span id="calibre_link-617" class="calibre4"></span><h2 class="sigil_not_in_toc">2.4.1. Spectral Co-Clustering</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.bicluster.SpectralCoclustering.html#sklearn.cluster.bicluster.SpectralCoclustering" title="sklearn.cluster.bicluster.SpectralCoclustering"><code class="docutils"><span class="calibre4">SpectralCoclustering</span></code></a> 算法找到的 bicluster 的值比相应的其他行和列更高。</div>
</blockquote>
<p class="calibre10">每一个行和列都只属于一个 bicluster, 所以重新分配行和列，使得分区连续显示对角线上的 high value:</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">算法将输入的数据矩阵看做成二分图：该矩阵的行和列对应于两组顶点，每个条目对应于行和列之间的边，该算法近似的进行归一化，对图进行切割，找到更重的子图。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-618">
<h3 class="sigil_not_in_toc1">2.4.1.1. 数学公式</h3>
<p class="calibre2">找到最优归一化剪切的近似解，可以通过图形的 Laplacian 的广义特征值分解。
通常这意味着直接使用 Laplacian 矩阵. 如果原始数据矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 有形状 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000205.jpg" alt="m \times n" />, 则对应的 bipartite 图的 Laplacian 矩阵具有形状 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000406.jpg" alt="(m + n) \times (m + n)" />。
但是, 在这种情况直接使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> , 因为它更小，更有作用。</p>
<p class="calibre10">输入矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 被预处理如下:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000852.jpg" alt="A_n = R^{-1/2} A C^{-1/2}" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000120.jpg" alt="R" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 对角线矩阵，和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000773.jpg" alt="\sum_{j} A_{ij}" /> 相同，  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000457.jpg" alt="j" /> 的对角吸纳矩阵，等同于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000274.jpg" alt="\sum_{i} A_{ij}" />。</p>
<p class="calibre10">奇异值分解, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000539.jpg" alt="A_n = U \Sigma V^\top" /> , 提供了 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 行列的分区. 左边的奇异值向量给予行分区，右边的奇异值向量给予列分区。</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000490.jpg" alt="\ell = \lceil \log_2 k \rceil" /> 奇异值向量从第二个开始, 提供所需的分区信息。 这些用于形成矩阵 :<cite class="calibre13">Z</cite>:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000609.jpg" alt="Z = \begin{bmatrix} R^{-1/2} U \\\\                     C^{-1/2} V       \end{bmatrix}" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000736.jpg" alt="U" /> 的列是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000409.jpg" alt="u_2, \dots, u_{\ell +1}" />, 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000291.jpg" alt="V" /> 相似 。</p>
<p class="calibre10">然后 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000233.jpg" alt="Z" /> 的 rows 通过使用 <a class="calibre3 pcalibre" href="clustering.html#k-means"><span class="calibre4">k-means</span></a> 进行聚类. <code class="docutils"><span class="calibre4">n_rows</span></code> 标签提供行分区,
剩下的 <code class="docutils"><span class="calibre4">n_columns</span></code> 标签 提供 列分区。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/bicluster/plot_spectral_coclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-coclustering-py"><span class="calibre4">A demo of the Spectral Co-Clustering algorithm</span></a>: 如何用 bicluster 数据矩阵并应用。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py"><span class="calibre4">Biclustering documents with the Spectral Co-clustering algorithm</span></a>:一个在 20 个新闻组数据集中发现 biclusters 的例子</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Dhillon, Inderjit S, 2001. <a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011">Co-clustering documents and words using
bipartite spectral graph partitioning</a>.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-619">
<span id="calibre_link-620" class="calibre4"></span><h2 class="sigil_not_in_toc">2.4.2. Spectral Biclustering</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.bicluster.SpectralBiclustering.html#sklearn.cluster.bicluster.SpectralBiclustering" title="sklearn.cluster.bicluster.SpectralBiclustering"><code class="docutils"><span class="calibre4">SpectralBiclustering</span></code></a> 算法假设输入的数据矩阵具有隐藏的棋盘结构。 具有这种结构的矩阵的行列
可能被分区，使得在笛卡尔积中的 大部分 biclusters 的 row clusters 和 column cluster 是近似恒定的。</div>
</blockquote>
<p class="calibre10">例如，如果有两个row 分区和三个列分区，每一行属于三个 bicluster ，每一列属于两个 bicluster。</p>
<p class="calibre10">这个算法划分矩阵的行和列，以至于提供一个相应的块状不变的棋盘矩阵，近似于原始矩阵。</p>
<div class="toctree-wrapper" id="calibre_link-621">
<h3 class="sigil_not_in_toc1">2.4.2.1. 数学表示</h3>
<p class="calibre2">输入矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 先归一化，使得棋盘模式更明显。有三种方法:</p>
<ol class="arabic">
<li class="toctree-l"><em class="calibre13">独立的行和列归一化</em>, as in Spectral
Co-Clustering. 这个方法使得行和一个常数相加，列和变量相加。</li>
<li class="toctree-l"><strong class="calibre14">Bistochastization</strong>: 重复行和列归一化直到收敛。该方法使得行和列都相加</li>
</ol>
<p class="calibre10">&nbsp;&nbsp;&nbsp;相同的常数。</p>
<ol class="arabic" start="3">
<li class="toctree-l"><strong class="calibre14">Log 归一化</strong>: 计算数据矩阵的对数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000652.jpg" alt="L = \log A" />. 列就是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000619.jpg" alt="\overline{L_{i \cdot}}" />, 行就是
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000613.jpg" alt="\overline{L_{\cdot j}}" />, 总体上来看 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000035.jpg" alt="\overline{L_{\cdot \cdot}}" /> of <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> 被计算的. 最后矩阵通过下面的公式计算</li>
</ol>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000790.jpg" alt="K_{ij} = L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot j}} + \overline{L_{\cdot \cdot}}" class="math" /></p>
</div>
<p class="calibre10">归一化后，首先少量的奇异值向量被计算，只是在 Spectral Co-Clustering 算法中。</p>
<p class="calibre10">如果使用 log 归一化，则所有的奇异向量都是有意义的。但是, 如果是独立的归一化或双曲线化
被使用，第一个奇异矢量, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000171.jpg" alt="u_1" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000127.jpg" alt="v_1" />。
会被丢弃。 从现在开始,  “first” 奇异值向量与
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000115.jpg" alt="u_2 \dots u_{p+1}" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000694.jpg" alt="v_2 \dots v_{p+1}" /> 相关，除了日志归一化的情况。</p>
<p class="calibre10">给定这些奇异值向量， 将他们排序，通过分段常数向量保证最佳近似。 使用一维 k-means 找到每个向量的近似值
并使用欧几里得距离得分。 Some subset of 最好的左右奇异值向量的子集被选择。 下一步,
数据预计到这个最佳子集的奇异向量和聚类。</p>
<p class="calibre10">例如，如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" />  奇异值向量被计算，最好按照描述找到
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> ， 其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000829.jpg" alt="q&lt;p" />。
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000736.jpg" alt="U" /> 列为，the <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> 最佳左奇异向量的矩阵,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000291.jpg" alt="V" /> 对于右边是类似的. 要划分行,
将 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" />  的 投影到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000636.jpg" alt="q" /> 维空间:
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000696.jpg" alt="A * V" />。  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000244.jpg" alt="m" /> 行 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000838.jpg" alt="m \times q" />
矩阵的行作为采样和使用 k-means 的聚类处理产生行标签。
类似地，将列投影到 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000699.jpg" alt="A^{\top} * U" /> ，并且对
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000622.jpg" alt="n \times q" /> 矩阵进行聚类得到列标签。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/bicluster/plot_spectral_biclustering.html#sphx-glr-auto-examples-bicluster-plot-spectral-biclustering-py"><span class="calibre4">A demo of the Spectral Biclustering algorithm</span></a>: 一个简单的例子
显示如何生成棋盘矩阵和 bicluster</li>
</ul>
</div>
<p class="calibre10">.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Kluger, Yuval, et. al., 2003. <a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608">Spectral biclustering of microarray
data: coclustering genes and conditions</a>.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-622">
<span id="calibre_link-623" class="calibre4"></span><h2 class="sigil_not_in_toc">2.4.3. Biclustering 评测</h2>
<p class="calibre2">有两种评估双组分结果的方法：内部和外部。
诸如群集稳定性等内部措施只依赖于数据和结果本身。
目前在scikit-learn中没有内部的二集群措施。外部措施是指外部信息来源，例如真正的解决方案。
当使用真实数据时，真正的解决方案通常是未知的，但是，由于真正的解决方案是已知的，因此人造数据的双重分析可能对于评估算法非常有用。</p>
<p class="calibre10">为了将一组已发现的双组分与一组真正的双组分进行比较，
需要两个相似性度量：单个双色团体的相似性度量，以及将这些个体相似度结合到总分中的方法。</p>
<p class="calibre10">为了比较单个双核，已经采用了几种措施。现在，只有Jaccard索引被实现：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000109.jpg" alt="J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}" class="math" /></p>
</div>
<dl class="calibre10">
<dt class="calibre18"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000385.jpg" alt="A" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000783.jpg" alt="B" /> 是 biclusters, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000045.jpg" alt="|A \cap B|" /> 是交叉点的元素的数量。</dt>
<dd class="calibre19">Jaccard 索引 达到最小值0，当 biclusters 不重叠的时候，并且当他们相同干的时候，最大值为1。</dd>
</dl>
<p class="calibre10">有些方法已经开发出来，用来比较两个 biclusters 的数据集。
从现在开始 之后 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.consensus_score.html#sklearn.metrics.consensus_score" title="sklearn.metrics.consensus_score"><code class="docutils"><span class="calibre4">consensus_score</span></code></a> (Hochreiter et. al., 2010) 是可以用:</p>
<ol class="arabic">
<li class="toctree-l">使用 Jaccard 索引或类似措施，计算 biclusters 的 bicluster 相似性。</li>
<li class="toctree-l">以一对一的方式将 bicluster 分从一组分配给另一组，以最大化其相似性的总和。该步骤使用匈牙利算法执行。</li>
<li class="toctree-l">相似性的最终总和除以较大集合的大小。</li>
</ol>
<p class="calibre10">最小共识得分为0，发生在所有 biclusters 完全不相似时。当两组 biclusters 相同时，最大分数为1。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Hochreiter, Bodenhofer, et. al., 2010. <a class="calibre3 pcalibre" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/">FABIA: factor analysis
for bicluster acquisition</a>.</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-32">
<span id="calibre_link-624" class="calibre4"></span><h1 class="calibre5">2.5. 分解成分中的信号（矩阵分解问题）</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@武器大师一个挑俩</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@png</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@柠檬</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />    
    </div>
<div class="toctree-wrapper" id="calibre_link-625">
<span id="calibre_link-626" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.1. 主成分分析（PCA）</h2>
<div class="toctree-wrapper" id="calibre_link-627">
<h3 class="sigil_not_in_toc1">2.5.1.1. 准确的PCA和概率解释（Exact PCA and probabilistic interpretation）</h3>
<p class="calibre2">PCA 用于对一组连续正交分量中的多变量数据集进行方差最大方向的分解。
在 scikit-learn 中， <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 被实现为一个变换对象， 通过 <code class="docutils"><span class="calibre4">fit</span></code> 方法可以降维成 <cite class="calibre13">n</cite> 个成分，
并且可以将新的数据投影(project, 亦可理解为分解)到这些成分中。</p>
<p class="calibre10">可选参数 <code class="docutils"><span class="calibre4">whiten=True</span></code> 使得可以将数据投影到奇异（singular）空间上，同时将每个成分缩放到单位方差。
如果下游模型对信号的各向同性作出强烈的假设，这通常是有用的，例如，使用RBF内核的 SVM 算法和 K-Means 聚类算法。</p>
<p class="calibre10">以下是iris数据集的一个示例，该数据集包含4个特征， 通过PCA降维后投影到方差最大的二维空间上：</p>
<div class="toctree-wrapper" id="calibre_link-628">

<p class="calibre10"><span class="calibre4"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 对象还提供了 PCA 的概率解释， 其可以基于其解释的方差量给出数据的可能性。</span></p>
</div>
<p class="calibre10">可以通过在交叉验证（cross-validation）中使用 <cite class="calibre13">score</cite> 方法来实现：</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000360.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="calibre4">Comparison of LDA and PCA 2D projection of Iris dataset</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="calibre4">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-629">
<span id="calibre_link-630" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.1.2. 增量PCA (Incremental PCA)</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 对象非常有用, 但对大型数据集有一定的限制。
最大的限制是 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 仅支持批处理，这意味着所有要处理的数据必须适合主内存。
<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="docutils"><span class="calibre4">IncrementalPCA</span></code></a> 对象使用不同的处理形式使之允许部分计算，
这一形式几乎和 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 以小型批处理方式处理数据的方法完全匹配。
<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="docutils"><span class="calibre4">IncrementalPCA</span></code></a> 可以通过以下方式实现核外（out-of-core）主成分分析：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">使用 <code class="docutils"><span class="calibre4">partial_fit</span></code> 方法从本地硬盘或网络数据库中以此获取数据块。</li>
<li class="toctree-l">通过 <code class="docutils"><span class="calibre4">numpy.memmap</span></code> 在一个 memory mapped file 上使用 fit 方法。</li>
</ul>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="docutils"><span class="calibre4">IncrementalPCA</span></code></a> 仅存储成分和噪声方差的估计值，并按顺序递增地更新解释方差比（<a href="#calibre_link-33" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-631">explained_variance_ratio_</span></a>）。</p>
</div>
</blockquote>
<p class="calibre10">这就是为什么内存使用取决于每个批次的样本数，而不是数据集中要处理的样本数。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000241.jpg" class="align-center" /></a>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_incremental_pca_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000217.jpg" class="align-center" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Examples:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="calibre4">Incremental PCA</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-632">
<span id="calibre_link-633" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.1.3. PCA 使用随机SVD</h3>
<p class="calibre2">通过丢弃具有较低奇异值的奇异向量成分，将数据降维到低维空间并保留大部分方差是非常有意义的。</p>
<p class="calibre10">例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096，
在这样大的数据上训练含RBF内核的支持向量机是很慢的。
此外我们知道数据本质上的维度远低于4096，因为人脸的所有照片都看起来有点相似。
样本位于许多的很低维度（例如约200维）。PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分方差。</p>
<p class="calibre10">在这种情况下，使用可选参数 <code class="docutils"><span class="calibre4">svd_solver='randomized'</span></code> 的 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 是非常有用的。
因为我们将要丢弃大部分奇异值，所以对我们将保留并实际执行变换的奇异向量进行近似估计的有限的计算更有效。</p>
<p class="calibre10">例如：以下显示了来自 Olivetti 数据集的 16 个样本肖像（以 0.0 为中心）。
右侧是前 16 个奇异向量重画为肖像。因为我们只需要使用大小为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000319.jpg" alt="n_{samples} = 400" />
和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000840.jpg" alt="n_{features} = 64 \times 64 = 4096" /> 的数据集的前 16 个奇异向量, 使得计算时间小于 1 秒。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<p class="calibre10">注意：使用可选参数 <code class="docutils"><span class="calibre4">svd_solver='randomized'</span></code> ，在 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 中我们还需要给出输入低维空间大小 <code class="docutils"><span class="calibre4">n_components</span></code> 。</p>
<p class="calibre10">如果我们注意到： <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000692.jpg" alt="n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})" /> 且
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000819.jpg" alt="n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})" />,
对于PCA中实施的确切方式，随机 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 的时间复杂度是：<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000396.jpg" alt="O(n_{\max}^2 \cdot n_{\mathrm{components}})" /> ，
而不是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000165.jpg" alt="O(n_{\max}^2 \cdot n_{\min})" /> 。</p>
<p class="calibre10">对于确切的方式，随机 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 的内存占用量正比于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000162.jpg" alt="2 \cdot n_{\max} \cdot n_{\mathrm{components}}" /> ，
而不是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000899.jpg" alt="n_{\max}\cdot n_{\min}" /></p>
<p class="calibre10">注意：选择参数 <code class="docutils"><span class="calibre4">svd_solver='randomized'</span></code> 的 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a>，在执行 <code class="docutils"><span class="calibre4">inverse_transform</span></code> 时，
并不是 <code class="docutils"><span class="calibre4">transform</span></code> 的确切的逆变换操作（即使 参数设置为默认的 <code class="docutils"><span class="calibre4">whiten=False</span></code>）</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="calibre4">Faces recognition example using eigenfaces and SVMs</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="calibre4">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://arxiv.org/abs/0909.4061">“Finding structure with randomness: Stochastic algorithms for
constructing approximate matrix decompositions”</a>
Halko, et al., 2009</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-634">
<span id="calibre_link-635" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.1.4. 核 PCA</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="docutils"><span class="calibre4">KernelPCA</span></code></a> 是 PCA 的扩展，通过使用核方法实现非线性降维（dimensionality reduction） (参阅 <a class="calibre3 pcalibre" href="metrics.html#metrics"><span class="calibre4">成对的矩阵, 类别和核函数</span></a>)。
它具有许多应用，包括去噪, 压缩和结构化预测（ structured prediction ） (kernel dependency estimation（内核依赖估计）)。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="docutils"><span class="calibre4">KernelPCA</span></code></a> 支持 <code class="docutils"><span class="calibre4">transform</span></code> 和 <code class="docutils"><span class="calibre4">inverse_transform</span></code> 。</div>
</blockquote>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_kernel_pca.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_pca_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000060.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="calibre4">Kernel PCA</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-636">
<span id="calibre_link-637" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.1.5. 稀疏主成分分析 ( SparsePCA 和 MiniBatchSparsePCA )</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="docutils"><span class="calibre4">SparsePCA</span></code></a> 是 PCA 的一个变体，目的是提取能最好地重建数据的稀疏组分集合。</p>
<p class="calibre10">小批量稀疏 PCA ( <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="docutils"><span class="calibre4">MiniBatchSparsePCA</span></code></a> ) 是一个 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="docutils"><span class="calibre4">SparsePCA</span></code></a> 的变种，它速度更快但准确度有所降低。对于给定的迭代次数，通过迭代该组特征的小块来达到速度的增加。</p>
<p class="calibre10">Principal component analysis（主成分分析） (<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a>) 的缺点在于，通过该方法提取的成分具有唯一的密度表达式，即当表示为原始变量的线性组合时，它们具有非零系数，使之难以解释。在许多情况下，真正的基础组件可以更自然地想象为稀疏向量; 例如在面部识别中，每个组件可能自然地映射到面部的某个部分。</p>
<p class="calibre10">稀疏的主成分产生更简洁、可解释的表达式，明确强调了样本之间的差异性来自哪些原始特征。</p>
<p class="calibre10">以下示例说明了使用稀疏 PCA 提取 Olivetti 人脸数据集中的 16 个组分。可以看出正则化项产生了许多零。此外，数据的自然结构导致了非零系数垂直相邻 （vertically adjacent）。该模型不会在数学上强制执行: 每个组分都是一个向量  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000660.jpg" alt="h \in \mathbf{R}^{4096}" />,除非人性化地的可视化为 64x64 像素的图像，否则没有垂直相邻性的概念。
下面显示的组分看起来局部化（appear local)是数据的内在结构的影响，这种局部模式使重建误差最小化。有一种考虑到邻接性和不同结构类型的导致稀疏的规范（sparsity-inducing norms）,参见 <a class="calibre3 pcalibre" href="#calibre_link-34" id="calibre_link-37">[Jen09]</a> 对这种方法进行了解。
有关如何使用稀疏 PCA 的更多详细信息，请参阅下面的示例部分。
更多关于 Sparse PCA 使用的内容，参见示例部分，如下：</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<p class="calibre10">请注意，有多种不同的计算稀疏PCA 问题的公式。 这里使用的方法基于 <a class="calibre3 pcalibre" href="#calibre_link-35" id="calibre_link-36">[Mrl09]</a> 。优化问题的解决是一个带有惩罚项（L1范数的） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> 的一个 PCA 问题（dictionary learning（字典学习））:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000298.jpg" alt="(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}              ||X-UV||_2^2+\alpha||V||_1 \\              \text{subject to\,} &amp; ||U_k||_2 = 1 \text{ for all }              0 \leq k &lt; n_{components}" class="math" /></p>
</div>
<p class="calibre10">导致稀疏（sparsity-inducing）的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000427.jpg" alt="\ell_1" /> 规范也可以避免当训练样本很少时从噪声中学习成分。可以通过超参数 <code class="docutils"><span class="calibre4">alpha</span></code> 来调整惩罚程度（从而减少稀疏度）。值较小会导致温和的正则化因式分解，而较大的值将许多系数缩小到零。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">虽然本着在线算法的精神， <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="docutils"><span class="calibre4">MiniBatchSparsePCA</span></code></a> 类不实现 <code class="docutils"><span class="calibre4">partial_fit</span></code> , 因为在线算法沿特征方向，而不是样本方向。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="calibre4">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-35" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-36">[Mrl09]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online Dictionary Learning for Sparse Coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-34" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-37">[Jen09]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf">“Structured Sparse Principal Component Analysis”</a>
R. Jenatton, G. Obozinski, F. Bach, 2009</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-638">
<span id="calibre_link-639" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.2. 截断奇异值分解和隐语义分析</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="docutils"><span class="calibre4">TruncatedSVD</span></code></a> 实现了一个奇异值分解（SVD）的变体，它只计算 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个最大的奇异值，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 是用户指定的参数。</p>
<p class="calibre10">当截断的 SVD被应用于 term-document矩阵（由 <code class="docutils"><span class="calibre4">CountVectorizer</span></code> 或 <code class="docutils"><span class="calibre4">TfidfVectorizer</span></code> 返回）时，这种转换被称为 <a class="calibre3 pcalibre" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">latent semantic analysis</a> (LSA), 因为它将这样的矩阵转换为低纬度的 “semantic（语义）” 空间。
特别地是 LSA 能够抵抗同义词和多义词的影响（两者大致意味着每个单词有多重含义），这导致 term-document 矩阵过度稀疏，并且在诸如余弦相似性的度量下表现出差的相似性。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">LSA 也被称为隐语义索引 LSI，尽管严格地说它是指在持久索引（persistent indexes）中用于信息检索的目的。</p>
</div>
<p class="calibre10">数学表示中， 训练样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 用截断的SVD产生一个低秩的（ low-rank）近似值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> :</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000209.jpg" alt="X \approx X_k = U_k \Sigma_k V_k^\top" class="math" /></p>
</div>
<p class="calibre10">在这个操作之后，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000832.jpg" alt="U_k \Sigma_k^\top" /> 是转换后的训练集，其中包括 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个特征（在 API 中被称为 <code class="docutils"><span class="calibre4">n_components</span></code> ）。</p>
<p class="calibre10">还需要转换一个测试集 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" />, 我们乘以 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000833.jpg" alt="V_k" />:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000234.jpg" alt="X&apos; = X V_k" class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">自然语言处理(NLP) 和信息检索(IR) 文献中的 LSA 的大多数处理方式是交换矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 的坐标轴,使其具有 <code class="docutils"><span class="calibre4">n_features</span></code> × <code class="docutils"><span class="calibre4">n_samples</span></code> 的形状。
我们以 scikit-learn API 相匹配的不同方式呈现 LSA, 但是找到的奇异值是相同的。</p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="docutils"><span class="calibre4">TruncatedSVD</span></code></a> 非常类似于 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a>, 但不同之处在于它工作在样本矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 而不是它们的协方差矩阵。
当从特征值中减去 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 的每列（每个特征per-feature）的均值时，在得到的矩阵上应用 truncated SVD 相当于 PCA 。
实际上，这意味着 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="docutils"><span class="calibre4">TruncatedSVD</span></code></a> 转换器（transformer）接受 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 矩阵，而不需要对它们进行密集（density），因为即使对于中型大小文档的集合，密集化 （densifying）也可能填满内存。</p>
<p class="calibre10">虽然 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="docutils"><span class="calibre4">TruncatedSVD</span></code></a> 转换器（transformer）可以在任何（稀疏的）特征矩阵上工作，但还是建议在 LSA/document 处理设置中，在 tf&ndash;idf 矩阵上的原始频率计数使用它。
特别地，应该打开子线性缩放（sublinear scaling）和逆文档频率（inverse document frequency） (<code class="docutils"><span class="calibre4">sublinear_tf=True,</span> <span class="calibre4">use_idf=True</span></code>) 以使特征值更接近于高斯分布，补偿 LSA 对文本数据的错误假设。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"><span class="calibre4">Clustering text documents using k-means</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),
<em class="calibre13">Introduction to Information Retrieval</em>, Cambridge University Press,
chapter 18: <a class="calibre3 pcalibre" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">Matrix decompositions &amp; latent semantic indexing</a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-640">
<span id="calibre_link-641" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.3. 词典学习</h2>
<div class="toctree-wrapper" id="calibre_link-38">
<span id="calibre_link-642" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.3.1. 带有预计算词典的稀疏编码</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder" title="sklearn.decomposition.SparseCoder"><code class="docutils"><span class="calibre4">SparseCoder</span></code></a> 对象是一个估计器 （estimator），可以用来将信号转换成一个固定的预计算的词典内原子（atoms）的稀疏线性组合（sparse linear combination），如离散小波基（ discrete wavelet basis ） 。
因此，该对象不实现 <code class="docutils"><span class="calibre4">fit</span></code> 方法。该转换相当于一个稀疏编码问题: 将数据的表示为尽可能少的词典原子的线性组合。
词典学习的所有变体实现以下变换方法，可以通过 <code class="docutils"><span class="calibre4">transform_method</span></code> 初始化参数进行控制:</p>
<ul class="calibre6">
<li class="toctree-l">Orthogonal matching pursuit(追求正交匹配) (<a class="calibre3 pcalibre" href="linear_model.html#omp"><span class="calibre4">正交匹配追踪法（OMP）</span></a>)</li>
<li class="toctree-l">Least-angle regression (最小角度回归)(<a class="calibre3 pcalibre" href="linear_model.html#least-angle-regression"><span class="calibre4">最小角回归</span></a>)</li>
<li class="toctree-l">Lasso computed by least-angle regression(最小角度回归的Lasso 计算)</li>
<li class="toctree-l">Lasso using coordinate descent ( 使用坐标下降的Lasso)(<a class="calibre3 pcalibre" href="linear_model.html#lasso"><span class="calibre4">Lasso</span></a>)</li>
<li class="toctree-l">Thresholding(阈值)</li>
</ul>
<p class="calibre10">阈值方法速度非常快，但是不能产生精确的重建。
它们在分类任务的文献中已被证明是有用的。对于图像重建任务，追求正交匹配可以产生最精确、无偏的重建。</p>
<p class="calibre10">词典学习对象通过 <code class="docutils"><span class="calibre4">split_code</span></code> 参数提供稀疏编码结果中的正值和负值分离的可能性。当使用词典学习来提取将用于监督学习的特征时，这是有用的，因为它允许学习算法将不同的权重从正加载（loading）分配给相应的负加载的特定原子。</p>
<p class="calibre10">单个样本的分割编码具有长度 <code class="docutils"><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">n_components</span></code> ，并使用以下规则构造: 首先，计算长度为 <code class="docutils"><span class="calibre4">n_components</span></code> 的常规编码。然后， <code class="docutils"><span class="calibre4">split_code</span></code> 的第一个 <code class="docutils"><span class="calibre4">n_components</span></code> 条目将用正常编码向量的正部分填充。分割编码的第二部分用编码向量的负部分填充，只有一个正号。因此， split_code 是非负的。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py"><span class="calibre4">Sparse coding with a precomputed dictionary</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-643">
<h3 class="sigil_not_in_toc1">2.5.3.2. 通用词典学习</h3>
<p class="calibre2">词典学习( <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning" title="sklearn.decomposition.DictionaryLearning"><code class="docutils"><span class="calibre4">DictionaryLearning</span></code></a> ) 是一个矩阵因式分解问题，相当于找到一个在拟合数据的稀疏编码中表现良好的（通常是过完备的（overcomplete））词典。</p>
<p class="calibre10">将数据表示为来自过完备词典的原子的稀疏组合被认为是哺乳动物初级视觉皮层的工作方式。
因此，应用于图像补丁的词典学习已被证明在诸如图像完成、修复和去噪，以及有监督的识别图像处理任务中表现良好的结果。</p>
<p class="calibre10">词典学习是通过交替更新稀疏编码来解决的优化问题，作为解决多个 Lasso 问题的一个解决方案，考虑到字典固定，然后更新字典以最好地适合稀疏编码。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000693.jpg" alt="(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}              ||X-UV||_2^2+\alpha||U||_1 \\              \text{subject to\,} &amp; ||V_k||_2 = 1 \text{ for all }              0 \leq k &lt; n_{\mathrm{atoms}}" class="math" /></p>
</div>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<p class="calibre10">在使用这样一个过程来拟合词典之后，变换只是一个稀疏的编码步骤，与所有的词典学习对象共享相同的实现。(参见 <a class="calibre3 pcalibre" href="#calibre_link-38"><span class="calibre4">带有预计算词典的稀疏编码</span></a>)。</p>
<p class="calibre10">以下图像显示了字典学习是如何从浣熊脸部的部分图像中提取的4x4像素图像补丁中进行词典学习的。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_image_denoising_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000364.jpg" class="calibre51" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py"><span class="calibre4">Image denoising using dictionary learning</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online dictionary learning for sparse coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-644">
<span id="calibre_link-645" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.5.3.3. 小批量字典学习</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="docutils"><span class="calibre4">MiniBatchDictionaryLearning</span></code></a> 实现了更快、更适合大型数据集的字典学习算法，其运行速度更快，但准确度有所降低。</p>
<p class="calibre10">默认情况下，<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="docutils"><span class="calibre4">MiniBatchDictionaryLearning</span></code></a> 将数据分成小批量，并通过在指定次数的迭代中循环使用小批量，以在线方式进行优化。但是，目前它没有实现停止条件。</p>
<p class="calibre10">估计器还实现了  <code class="docutils"><span class="calibre4">partial_fit</span></code>, 它通过在一个小批处理中仅迭代一次来更新字典。 当在线学习的数据从一开始就不容易获得，或者数据超出内存时，可以使用这种迭代方法。</p>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_dict_face_patches.html"></a>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">字典学习聚类</strong></p>
<p class="calibre10">注意，当使用字典学习来提取表示（例如，用于稀疏编码）时，聚类可以是学习字典的良好中间方法。
例如，<a class="calibre3 pcalibre" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="docutils"><span class="calibre4">MiniBatchKMeans</span></code></a> 估计器能高效计算并使用 <code class="docutils"><span class="calibre4">partial_fit</span></code> 方法实现在线学习。</p>
<p class="calibre10">示例: 在线学习面部部分的字典 <a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="calibre4">Online learning of a dictionary of parts of faces</span></a></p>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-646">
<span id="calibre_link-647" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.4. 因子分析</h2>
<p class="calibre2">在无监督的学习中，我们只有一个数据集 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000739.jpg" alt="X = \{x_1, x_2, \dots, x_n\}" />.
这个数据集如何在数学上描述？ <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 的一个非常简单的连续隐变量模型</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000083.jpg" alt="x_i = W h_i + \mu + \epsilon" class="math" /></p>
</div>
<p class="calibre10">矢量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000449.jpg" alt="h_i" /> 被称为 “隐性的”，因为它是不可观察的。
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000516.jpg" alt="\epsilon" /> 被认为是符合高斯分布的噪声项，平均值为 0，协方差为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000634.jpg" alt="\Psi" /> （即 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000297.jpg" alt="\epsilon \sim \mathcal{N}(0, \Psi)" />），
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000764.jpg" alt="\mu" /> 是偏移向量。 这样一个模型被称为 “生成的”，因为它描述了如何从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000449.jpg" alt="h_i" /> 生成 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> 。
如果我们使用所有的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> 作为列来形成一个矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000082.jpg" alt="\mathbf{X}" /> ，并将所有的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000449.jpg" alt="h_i" /> 作为矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000797.jpg" alt="\mathbf{H}" /> 的列，
那么我们可以写（适当定义的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000157.jpg" alt="\mathbf{M}" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000012.jpg" alt="\mathbf{E}" /> ）:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000193.jpg" alt="\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}" class="math" /></p>
</div>
<p class="calibre10">换句话说，我们 <em class="calibre13">分解</em> 矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000082.jpg" alt="\mathbf{X}" />.
如果给出 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000449.jpg" alt="h_i" />，上述方程自动地表示以下概率解释：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000715.jpg" alt="p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)" class="math" /></p>
</div>
<p class="calibre10">对于一个完整的概率模型，我们还需要隐变量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 的先验分布。
最直接的假设（基于高斯分布的良好性质）是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000014.jpg" alt="h \sim \mathcal{N}(0, \mathbf{I})" />. 这产生一个高斯分布作为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 的边际分布:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000763.jpg" alt="p(x) = \mathcal{N}(\mu, WW^T + \Psi)" class="math" /></p>
</div>
<p class="calibre10">现在，在没有任何进一步假设的前提下，隐变量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 是多余的 &ndash; <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 完全可以用均值和协方差来建模。
我们需要对这两个参数之一进行更具体的构造。 一个简单的附加假设是将误差协方差 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000634.jpg" alt="\Psi" /> 构造成如下:</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000848.jpg" alt="\Psi = \sigma^2 \mathbf{I}" />: 这个假设能推导出 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 的概率模型。</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000281.jpg" alt="\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)" />: 这个模型称为 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis" title="sklearn.decomposition.FactorAnalysis"><code class="docutils"><span class="calibre4">FactorAnalysis</span></code></a>, 一个经典的统计模型。 矩阵W有时称为 “因子加载矩阵”。</li>
</ul>
<p class="calibre10">两个模型基都基于高斯分布是低阶协方差矩阵的假设。
因为这两个模型都是概率性的，所以它们可以集成到更复杂的模型中，
例如因子分析器的混合。如果隐变量基于非高斯分布，则得到完全不同的模型（例如， <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="docutils"><span class="calibre4">FastICA</span></code></a> ）。</p>
<p class="calibre10">因子分析 <em class="calibre13">可以</em> 产生与 :class:<a href="#calibre_link-39" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-648">`</span></a>PCA`类似的成分（例如其加载矩阵的列）。
然而，这些成分没有通用的性质（例如它们是否是正交的）:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<p class="calibre10">因子分析( <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> ) 的主要优点是可以独立地对输入空间的每个方向（异方差噪声）的方差建模:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a>
</div>
<p class="calibre10">在异方差噪声存在的情况下，这可以比概率 PCA 作出更好的模型选择:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_pca_vs_fa_model_selection_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000594.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="calibre4">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-649">
<span id="calibre_link-650" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.5. 独立成分分析（ICA）</h2>
<p class="calibre2">独立分量分析将多变量信号分解为独立性最强的加性子组件。
它通过 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="docutils"><span class="calibre4">Fast</span> <span class="calibre4">ICA</span></code></a> 算法在 scikit-learn 中实现。
ICA 通常不用于降低维度，而是用于分离叠加信号。
由于 ICA 模型不包括噪声项，因此要使模型正确，必须使用白化。
这可以在内部调节白化参数或手动使用 PCA 的一种变体。</p>
<p class="calibre10">ICA 通常用于分离混合信号（称为 <em class="calibre13">盲源分离</em> 的问题），如下例所示:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ica_blind_source_separation_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000038.jpg" class="calibre41" /></a>
</div>
<p class="calibre10">ICA 也可以用于具有稀疏子成分的非线性分解:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="calibre4">Blind source separation using FastICA</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="calibre4">FastICA on 2D point clouds</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="calibre4">Faces dataset decompositions</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-651">
<span id="calibre_link-652" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.6. 非负矩阵分解(NMF 或 NNMF)</h2>
<div class="toctree-wrapper" id="calibre_link-653">
<h3 class="sigil_not_in_toc1">2.5.6.1. NMF 与 Frobenius 范数</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> <a class="calibre3 pcalibre" href="#calibre_link-40" id="calibre_link-45">[1]</a> 是在数据和分量是非负情况下的另一种降维方法。
在数据矩阵不包含负值的情况下，可以插入 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 而不是 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 或其变体。
通过优化 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 与矩阵乘积 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000441.jpg" alt="WH" /> 之间的距离 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> ，可以将样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" /> 分解为两个非负矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000229.jpg" alt="W" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000684.jpg" alt="H" />。
最广泛使用的距离函数是 Frobenius 平方范数，它是欧几里德范数到矩阵的推广:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000713.jpg" alt="d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2" class="math" /></p>
</div>
<p class="calibre10">与 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a> 不同，通过叠加分量而不减去，以加法方式获得向量的表示。这种加性模型对于表示图像和文本是有效的。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">[Hoyer, 2004] <a class="calibre3 pcalibre" href="#calibre_link-41" id="calibre_link-46">[2]</a> 研究表明，当处于一定约束时，<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 可以产生数据集基于某子部分的表示，从而获得可解释的模型。</div>
</blockquote>
<p class="calibre10">以下示例展示了与 PCA 特征面相比， <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 从 Olivetti 面部数据集中的图像中发现的16个稀疏组件。</p>
<p class="calibre10">Unlike <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">PCA</span></code></a>, the representation of a vector is obtained in an additive
fashion, by superimposing the components, without subtracting. Such additive
models are efficient for representing images and text.</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a> <a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html"></a></strong></p>
<p class="calibre10"><code class="docutils"><span class="calibre4">init</span></code> 属性确定了应用的初始化方法，这对方法的性能有很大的影响。
<a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 实现了非负双奇异值分解方法。NNDSVD <a class="calibre3 pcalibre" href="#calibre_link-42" id="calibre_link-47">[4]</a> 基于两个 SVD 过程，一个近似数据矩阵，
使用单位秩矩阵的代数性质，得到的部分SVD因子的其他近似正部分。
基本的 NNDSVD 算法更适合稀疏分解。其变体 NNDSVDa（全部零值替换为所有元素的平均值）和
NNDSVDar（零值替换为比数据平均值除以100小的随机扰动）在稠密情况时推荐使用。</p>
<p class="calibre10">请注意，乘法更新 (‘mu’) 求解器无法更新初始化中存在的零，因此当与引入大量零的基本 NNDSVD 算法联合使用时，
会导致较差的结果; 在这种情况下，应优先使用 NNDSVDa 或 NNDSVDar。</p>
<p class="calibre10">也可以通过设置 <code class="docutils"><span class="calibre4">init="random"</span></code>，使用正确缩放的随机非负矩阵初始化 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 。
整数种子或 <code class="docutils"><span class="calibre4">RandomState</span></code> 也可以传递给 <code class="docutils"><span class="calibre4">random_state</span></code> 以控制重现性。</p>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 中，L1 和 L2 先验可以被添加到损失函数中以使模型正规化。
L2 先验使用 Frobenius 范数，而L1 先验使用 L1 范数。与 <code class="docutils"><span class="calibre4">ElasticNet</span></code> 一样，
我们通过 <code class="docutils"><span class="calibre4">l1_ratio</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000286.jpg" alt="\rho" />) 参数和正则化强度参数 <code class="docutils"><span class="calibre4">alpha</span></code> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" />) 来控制 L1 和 L2 的组合。那么先验项是:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000799.jpg" alt="\alpha \rho ||W||_1 + \alpha \rho ||H||_1 + \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2 + \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2" class="math" /></p>
</div>
<p class="calibre10">正则化目标函数为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000471.jpg" alt="d_{\mathrm{Fro}}(X, WH) + \alpha \rho ||W||_1 + \alpha \rho ||H||_1 + \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2 + \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2" class="math" /></p>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 正则化 W 和 H . 公共函数 <code class="docutils"><span class="calibre4">non_negative_factorization</span></code> 允许通过 <code class="docutils"><span class="calibre4">regularization</span></code> 属性进行更精细的控制，将 仅W ，仅H 或两者正规化。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-654">
<h3 class="sigil_not_in_toc1">2.5.6.2. 具有 beta-divergence 的 NMF</h3>
<p class="calibre2">如前所述，最广泛使用的距离函数是平方 Frobenius 范数，这是欧几里得范数到矩阵的推广:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000678.jpg" alt="d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2" class="math" /></p>
</div>
<p class="calibre10">其他距离函数可用于 NMF，例如（广义） Kullback-Leibler(KL) 散度，也称为 I-divergence:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000616.jpg" alt="d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})" class="math" /></p>
</div>
<p class="calibre10">或者， Itakura-Saito(IS) divergence:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000721.jpg" alt="d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)" class="math" /></p>
</div>
<p class="calibre10">这三个距离函数是 beta-divergence 函数族的特殊情况，其参数分别为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000870.jpg" alt="\beta = 2, 1, 0" /> <a class="calibre3 pcalibre" href="#calibre_link-43" id="calibre_link-49">[6]</a> 。 beta-divergence 定义如下:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000379.jpg" alt="d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})" class="math" /></p>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_beta_divergence.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_beta_divergence_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000159.jpg" class="calibre27" /></a>
</div>
<p class="calibre10">请注意，在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000754.jpg" alt="\beta \in (0; 1)" /> 上定义无效，仅仅在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000178.jpg" alt="d_{KL}" />
和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000732.jpg" alt="d_{IS}" /> 的上可以分别连续扩展。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="docutils"><span class="calibre4">NMF</span></code></a> 使用 Coordinate Descent (‘cd’) <a class="calibre3 pcalibre" href="#calibre_link-44" id="calibre_link-48">[5]</a> 和乘法更新 (‘mu’) <a class="calibre3 pcalibre" href="#calibre_link-43" id="calibre_link-50">[6]</a> 来实现两个求解器。
‘mu’ 求解器可以优化每个 beta-divergence，包括 Frobenius 范数 (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000176.jpg" alt="\beta=2" />) ，
（广义） Kullback-Leibler divergence (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000871.jpg" alt="\beta=1" />) 和Itakura-Saito divergence（beta = 0） ）。
请注意，对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000064.jpg" alt="\beta \in (1; 2)" />，’mu’ 求解器明显快于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000092.jpg" alt="\beta" /> 的其他值。
还要注意，使用负数（或0，即 ‘itakura-saito’ ） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000092.jpg" alt="\beta" />，输入矩阵不能包含零值。</p>
<p class="calibre10">‘cd’ 求解器只能优化 Frobenius 范数。由于 NMF 的潜在非凸性，即使优化相同的距离函数，
不同的求解器也可能会收敛到不同的最小值。</p>
<p class="calibre10">NMF最适用于 <code class="docutils"><span class="calibre4">fit_transform</span></code> 方法，该方法返回矩阵W.矩阵 H 被 <code class="docutils"><span class="calibre4">components_</span></code> 属性中存储到拟合模型中;
方法 <code class="docutils"><span class="calibre4">transform</span></code> 将基于这些存储的组件分解新的矩阵 X_new:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">1.2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">0.8</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">6</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">NMF</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">NMF</span><span class="calibre4">(</span><span class="calibre4">n_components</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">init</span><span class="calibre4">=</span><span class="calibre4">'random'</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">W</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">H</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">components_</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">6.1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3.2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">W_new</span> <span class="calibre4">=</span> <span class="calibre4">model</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_new</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="calibre4">Faces dataset decompositions</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="calibre4">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/decomposition/plot_beta_divergence.html#sphx-glr-auto-examples-decomposition-plot-beta-divergence-py"><span class="calibre4">Beta-divergence loss functions</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-40" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-45">[1]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf">“Learning the parts of objects by non-negative matrix factorization”</a>
D. Lee, S. Seung, 1999</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-41" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-46">[2]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">“Non-negative Matrix Factorization with Sparseness Constraints”</a>
P. Hoyer, 2004</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-42" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-47">[4]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">“SVD based initialization: A head start for nonnegative
matrix factorization”</a>
C. Boutsidis, E. Gallopoulos, 2008</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-44" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-48">[5]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf">“Fast local algorithms for large scale nonnegative matrix and tensor
factorizations.”</a>
A. Cichocki, P. Anh-Huy, 2009</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-43" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[6]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-49">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-50">2</a>)</em> <a class="calibre3 pcalibre" href="http://http://arxiv.org/pdf/1010.1763v3.pdf">“Algorithms for nonnegative matrix factorization with the beta-divergence”</a>
C. Fevotte, J. Idier, 2011</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-655">
<span id="calibre_link-656" class="calibre4"></span><h2 class="sigil_not_in_toc">2.5.7. 隐 Dirichlet 分配（LDA）</h2>
<p class="calibre2">隐 Dirichlet 分配是离散数据集（如文本语料库）的集合的生成概率模型。
它也是一个主题模型，用于从文档集合中发现抽象主题。</p>
<p class="calibre10">LDA 的图形模型是一个三层贝叶斯模型:</p>
<img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/lda_model_graph.png" class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000057.jpg" />
<p class="calibre10">当建模文本语料库时，该模型假设具有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000271.jpg" alt="D" /> 文档和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 主题的语料库的以下生成过程:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">对于每个主题 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" />，绘制 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000518.jpg" alt="\beta_k \sim \mathrm{Dirichlet}(\eta),\: k =1...K" /></li>
<li class="toctree-l">对于每个文档 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" />，绘制 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000686.jpg" alt="\theta_d \sim \mathrm{Dirichlet}(\alpha), \: d=1...D" /></li>
<li class="toctree-l">对于文档 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" /> 中的每个单词 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />:</li>
</ol>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">绘制主题索引 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000118.jpg" alt="z_{di} \sim \mathrm{Multinomial}(\theta_d)" /></li>
<li class="toctree-l">绘制观察词 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000777.jpg" alt="w_{ij} \sim \mathrm{Multinomial}(beta_{z_{di}}.)" /></li>
</ol>
</div>
</blockquote>
</div>
</blockquote>
<p class="calibre10">对于参数估计，后验分布为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000211.jpg" alt="p(z, \theta, \beta |w, \alpha, \eta) =   \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}" class="math" /></p>
</div>
<p class="calibre10">由于后验分布难以处理，变体贝叶斯方法使用更简单的分布 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000299.jpg" alt="q(z,\theta,\beta | \lambda, \phi, \gamma)" /> 近似，
并且优化了这些变体参数  <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000719.jpg" alt="\lambda" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000566.jpg" alt="\gamma" /> 最大化Evidence Lower Bound (ELBO):</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000551.jpg" alt="\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}   E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]" class="math" /></p>
</div>
<p class="calibre10">最大化 ELBO 相当于最小化 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000475.jpg" alt="q(z,\theta,\beta)" /> 和后验 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000683.jpg" alt="p(z, \theta, \beta |w, \alpha, \eta)" /> 之间的 Kullback-Leibler(KL) 散度。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="docutils"><span class="calibre4">LatentDirichletAllocation</span></code></a> 实现在线变体贝叶斯算法，支持在线和批量更新方法。
批处理方法在每次完全传递数据后更新变分变量，在线方法从小批量数据点中更新变体变量。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">虽然在线方法保证收敛到局部最优点，最优点的质量和收敛速度可能取决于与小批量大小和学习率相关的属性。</p>
</div>
<p class="calibre10">当 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="docutils"><span class="calibre4">LatentDirichletAllocation</span></code></a> 应用于 “document-term” 矩阵时，矩阵将被分解为 “topic-term” 矩阵和 “document-topic” 矩阵。
虽然 “topic-term” 矩阵在模型中被存储为 <code class="docutils"><span class="calibre4">components_</span></code> ，但是可以通过 <code class="docutils"><span class="calibre4">transform</span></code> 方法计算 “document-topic” 矩阵。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="docutils"><span class="calibre4">LatentDirichletAllocation</span></code></a> 还实现了  <code class="docutils"><span class="calibre4">partial_fit</span></code> 方法。这可用于当数据被顺序提取时.</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="calibre4">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">“Latent Dirichlet Allocation”</a>
D. Blei, A. Ng, M. Jordan, 2003</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf">“Online Learning for Latent Dirichlet Allocation”</a>
M. Hoffman, D. Blei, F. Bach, 2010</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">“Stochastic Variational Inference”</a>
M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-234">
<div class="toctree-wrapper" role="main">
            
  
<div class="toctree-wrapper" id="calibre_link-657">
<h1 class="calibre5">2.6. 协方差估计</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@李昊伟</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@柠檬</a><br class="calibre9" />    
    </div>
<p class="calibre10">许多统计问题在某一时刻需要估计一个总体的协方差矩阵，这可以看作是对数据集散点图形状的估计。
大多数情况下，基于样本的估计（基于其属性，如尺寸，结构，均匀性），
对估计质量有很大影响。 <cite class="calibre13">sklearn.covariance</cite> 方法的目的是
提供一个能在各种设置下准确估计总体协方差矩阵的工具。</p>
<p class="calibre10">我们假设观察是独立的，相同分布的 (i.i.d.)。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-658">
<h1 id="calibre_link-659" class="calibre5">2.7. 经验协方差</h1>
<p class="calibre2">已知数据集的协方差矩阵与经典 <em class="calibre13">maximum likelihood estimator(最大似然估计)</em> （或 “经验协方差”）
很好地近似，条件是与特征数量（描述观测值的变量）相比，观测数量足够大。
更准确地说，样本的最大似然估计是相应的总体协方差矩阵的无偏估计。</p>
<p class="calibre10">样本的经验协方差矩阵可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.empirical_covariance.html#sklearn.covariance.empirical_covariance" title="sklearn.covariance.empirical_covariance"><code class="docutils"><span class="calibre4">empirical_covariance</span></code></a> 包的函数计算 ，
或者通过 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code class="docutils"><span class="calibre4">EmpiricalCovariance</span></code></a> 使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance.fit" title="sklearn.covariance.EmpiricalCovariance.fit"><code class="docutils"><span class="calibre4">EmpiricalCovariance.fit</span></code></a>
方法将对象与数据样本拟合 。
要注意，取决于数据是否居中，结果会有所不同，所以可能需要准确使用参数 <code class="docutils"><span class="calibre4">assume_centered</span></code>。
如果使用 <code class="docutils"><span class="calibre4">assume_centered=False</span></code> ，则结果更准确。且测试集应该具有与训练集相同的均值向量。
如果不是这样，两者都应该使用中心值， <code class="docutils"><span class="calibre4">assume_centered=True</span></code> 应该使用。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py"><span class="calibre4">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</span></a> for
an example on how to fit an <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code class="docutils"><span class="calibre4">EmpiricalCovariance</span></code></a> object
to data.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-660">
<span id="calibre_link-661" class="calibre4"></span><h1 id="calibre_link-662" class="calibre5">2.8. 收敛协方差</h1>
<div class="toctree-wrapper" id="calibre_link-663">
<h2 class="sigil_not_in_toc">2.8.1. 基本收敛</h2>
<p class="calibre2">尽管是协方差矩阵的无偏估计，
最大似然估计不是协方差矩阵的特征值的一个很好的估计，
所以从反演得到的精度矩阵是不准确的。
有时，甚至出现数学原因，经验协方差矩阵不能反转。
为了避免这样的反演问题，引入了经验协方差矩阵的一种变换方式：<code class="docutils"><span class="calibre4">shrinkage</span></code> 。</p>
<p class="calibre10">在 scikit-learn 中，该变换（具有用户定义的收缩系数）
可以直接应用于使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.shrunk_covariance.html#sklearn.covariance.shrunk_covariance" title="sklearn.covariance.shrunk_covariance"><code class="docutils"><span class="calibre4">shrunk_covariance</span></code></a> 方法预先计算协方差。
此外，协方差的收缩估计可以用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance" title="sklearn.covariance.ShrunkCovariance"><code class="docutils"><span class="calibre4">ShrunkCovariance</span></code></a> 对象
及其 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance.fit" title="sklearn.covariance.ShrunkCovariance.fit"><code class="docutils"><span class="calibre4">ShrunkCovariance.fit</span></code></a> 方法拟合到数据中。
再次，根据数据是否居中，结果会不同，所以可能要准确使用参数 <code class="docutils"><span class="calibre4">assume_centered</span></code> 。</p>
<p class="calibre10">在数学上，这种收缩在于减少经验协方差矩阵的最小和最大特征值之间的比率。
可以通过简单地根据给定的偏移量移动每个特征值来完成，
这相当于找到协方差矩阵的l2惩罚的最大似然估计器（l2-penalized Maximum
Likelihood Estimator）。在实践中，收缩归结为简单的凸变换：
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000501.jpg" alt="\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id" />.</p>
<p class="calibre10">选择收缩量， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> 相当于设置偏差/方差权衡，下面将讨论。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py"><span class="calibre4">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</span></a> for
an example on how to fit a <a class="calibre3 pcalibre" href="generated/sklearn.covariance.ShrunkCovariance.html#sklearn.covariance.ShrunkCovariance" title="sklearn.covariance.ShrunkCovariance"><code class="docutils"><span class="calibre4">ShrunkCovariance</span></code></a> object
to data.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-664">
<h2 class="sigil_not_in_toc">2.8.2. Ledoit-Wolf 收敛</h2>
<p class="calibre2">在他们的 2004 年的论文 <a class="calibre3 pcalibre" href="#calibre_link-235" id="calibre_link-236">[1]</a> 中， O.Ledoit 和 M.Wolf 提出了一个公式，
用来计算优化的收敛系数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000212.jpg" alt="\alpha" /> ，
它使得估计协方差和实际协方差矩阵之间的均方差进行最小化。</p>
<p class="calibre10">在 <cite class="calibre13">sklearn.covariance</cite> 包中，可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.ledoit_wolf.html#sklearn.covariance.ledoit_wolf" title="sklearn.covariance.ledoit_wolf"><code class="docutils"><span class="calibre4">ledoit_wolf</span></code></a> 函数来计算样本的
基于 Ledoit-Wolf estimator 的协方差， 或者可以针对同样的样本
通过拟合 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.LedoitWolf.html#sklearn.covariance.LedoitWolf" title="sklearn.covariance.LedoitWolf"><code class="docutils"><span class="calibre4">LedoitWolf</span></code></a> 对象来获得。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py"><span class="calibre4">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</span></a>
关于如何将 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.LedoitWolf.html#sklearn.covariance.LedoitWolf" title="sklearn.covariance.LedoitWolf"><code class="docutils"><span class="calibre4">LedoitWolf</span></code></a> 对象与数据拟合，
并将 Ledoit-Wolf 估计器的性能进行可视化的示例。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-235" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-236">[1]</a></td>
<td class="label1">O. Ledoit and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional
Covariance Matrices”, Journal of Multivariate Analysis, Volume 88, Issue 2,
February 2004, pages 365-411.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-665">
<span id="calibre_link-666" class="calibre4"></span><h2 class="sigil_not_in_toc">2.8.3. Oracle 近似收缩</h2>
<p class="calibre2">在数据为高斯分布的假设下，Chen et al. 等 <a class="calibre3 pcalibre" href="#calibre_link-237" id="calibre_link-238">[2]</a>  推导出了一个公式，旨在
产生比 Ledoit 和 Wolf 公式具有更小均方差的收敛系数。
所得到的估计器被称为协方差的 Oracle 收缩近似估计器。</p>
<p class="calibre10">在 <cite class="calibre13">sklearn.covariance</cite> 包中， OAS 估计的协方差可以使用函数 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.oas.html#sklearn.covariance.oas" title="sklearn.covariance.oas"><code class="docutils"><span class="calibre4">oas</span></code></a> 对样本进行计算，或者可以通过将 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code class="docutils"><span class="calibre4">OAS</span></code></a> 对象拟合到相同的样本来获得。</p>
<div class="toctree-wrapper" id="calibre_link-667">
<a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_covariance_estimation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_covariance_estimation_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000383.jpg" class="calibre52" /></a>
<p class="calibre10"><span class="calibre4">设定收缩时的偏差方差权衡：比较 Ledoit-Wolf 和 OAS 估计量的选择</span></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-237" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-238">[2]</a></td>
<td class="label1">Chen et al., “Shrinkage Algorithms for MMSE Covariance Estimation”,
IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_covariance_estimation.html#sphx-glr-auto-examples-covariance-plot-covariance-estimation-py"><span class="calibre4">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</span></a> for
an example on how to fit an <a class="calibre3 pcalibre" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code class="docutils"><span class="calibre4">OAS</span></code></a> object
to data.</li>
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_lw_vs_oas.html#sphx-glr-auto-examples-covariance-plot-lw-vs-oas-py"><span class="calibre4">Ledoit-Wolf vs OAS estimation</span></a> to visualize the
Mean Squared Error difference between a <a class="calibre3 pcalibre" href="generated/sklearn.covariance.LedoitWolf.html#sklearn.covariance.LedoitWolf" title="sklearn.covariance.LedoitWolf"><code class="docutils"><span class="calibre4">LedoitWolf</span></code></a> and
an <a class="calibre3 pcalibre" href="generated/sklearn.covariance.OAS.html#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code class="docutils"><span class="calibre4">OAS</span></code></a> estimator of the covariance.</li>
</ul>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_lw_vs_oas.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lw_vs_oas_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000108.jpg" class="calibre27" /></a>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-668">
<span id="calibre_link-669" class="calibre4"></span><h1 id="calibre_link-670" class="calibre5">2.9. 稀疏逆协方差</h1>
<p class="calibre2">协方差矩阵的逆矩阵，通常称为精度矩阵（precision matrix），它与部分相关矩阵（partial correlation matrix）成正比。
它给出部分独立性关系。换句话说，如果两个特征在其他特征上有条件地独立，
则精度矩阵中的对应系数将为零。这就是为什么估计一个稀疏精度矩阵是有道理的：
通过从数据中学习独立关系，协方差矩阵的估计能更好处理。这被称为协方差选择。</p>
<p class="calibre10">在小样本的情况，即 <code class="docutils"><span class="calibre4">n_samples</span></code> 是数量级 <code class="docutils"><span class="calibre4">n_features</span></code> 或更小，
稀疏的逆协方差估计往往比收敛的协方差估计更好。
然而，在相反的情况下，或者对于非常相关的数据，它们可能在数值上不稳定。
此外，与收敛估算不同，稀疏估计器能够恢复非对角线结构 （off-diagonal structure）。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.covariance.GraphLasso.html#sklearn.covariance.GraphLasso" title="sklearn.covariance.GraphLasso"><code class="docutils"><span class="calibre4">GraphLasso</span></code></a> 估计器使用 L1 惩罚执行关于精度矩阵的稀疏性：
<code class="docutils"><span class="calibre4">alpha</span></code> 参数越高，精度矩阵的稀疏性越大。
相应的 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.GraphLassoCV.html#sklearn.covariance.GraphLassoCV" title="sklearn.covariance.GraphLassoCV"><code class="docutils"><span class="calibre4">GraphLassoCV</span></code></a> 对象使用交叉验证来自动设置 <code class="docutils"><span class="calibre4">alpha</span></code> 参数。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_sparse_cov.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_sparse_cov_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000270.jpg" class="calibre53" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><strong class="calibre14">结构恢复</strong></p>
<p class="calibre10">从数据中的相关性恢复图形结构是一个具有挑战性的事情。如果您对这种恢复感兴趣，请记住：</p>
<ul class="calibre6">
<li class="toctree-l">相关矩阵的恢复比协方差矩阵更容易：在运行 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.GraphLasso.html#sklearn.covariance.GraphLasso" title="sklearn.covariance.GraphLasso"><code class="docutils"><span class="calibre4">GraphLasso</span></code></a> 前先标准化观察值</li>
<li class="toctree-l">如果底层图具有比平均节点更多的连接节点，则算法将错过其中一些连接。</li>
<li class="toctree-l">如果您的观察次数与底层图形中的边数相比不大，则不会恢复。</li>
<li class="toctree-l">即使您具有良好的恢复条件，通过交叉验证（例如使用GraphLassoCV对象）选择的 Alpha 参数将导致选择太多边。
然而，相关边缘将具有比不相关边缘更重的权重。</li>
</ul>
</div>
<p class="calibre10">数学公式如下：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000417.jpg" alt="\hat{K} = \mathrm{argmin}_K \big(             \mathrm{tr} S K - \mathrm{log} \mathrm{det} K             + \alpha \|K\|_1             \big)" class="math" /></p>
</div>
<p class="calibre10">其中：<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 是要估计的精度矩阵（precision matrix）， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000248.jpg" alt="S" /> 是样本的协方差矩阵。
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000771.jpg" alt="\|K\|_1" /> 是非对角系数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> （off-diagonal coefficients）的绝对值之和。
用于解决这个问题的算法是来自 Friedman 2008 Biostatistics 论文的 GLasso 算法。
它与 R 语言 <code class="docutils"><span class="calibre4">glasso</span></code> 包中的算法相同。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_sparse_cov.html#sphx-glr-auto-examples-covariance-plot-sparse-cov-py"><span class="calibre4">Sparse inverse covariance estimation</span></a>:</li>
</ul>
<p class="calibre10">合成数据示例，显示结构的一些恢复，并与其他协方差估计器进行比较。</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py"><span class="calibre4">Visualizing the stock market structure</span></a>:
真实股票市场数据示例，查找哪些信号最相关。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Friedman et al, <a class="calibre3 pcalibre" href="http://biostatistics.oxfordjournals.org/content/9/3/432.short">“Sparse inverse covariance estimation with the
graphical lasso”</a>,
Biostatistics 9, pp 432, 2008</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-671">
<span id="calibre_link-672" class="calibre4"></span><h1 id="calibre_link-673" class="calibre5">2.10. Robust 协方差估计</h1>
<p class="calibre2">实际数据集通常是会有测量或记录错误。合格但不常见的观察也可能出于各种原因。
每个不常见的观察称为异常值。
上面提出的经验协方差估计器和收缩协方差估计器对数据中异常观察值非常敏感。
因此，应该使用更好的协方差估计（robust covariance estimators）来估算其真实数据集的协方差。
或者，可以使用更好的协方差估计器（robust covariance estimators）来执行异常值检测，
并根据数据的进一步处理，丢弃/降低某些观察值。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">sklearn.covariance</span></code> 包实现了 robust estimator of covariance， 即 Minimum Covariance Determinant <a class="calibre3 pcalibre" href="#calibre_link-239" id="calibre_link-241">[3]</a> 。</p>
<div class="toctree-wrapper" id="calibre_link-674">
<h2 class="sigil_not_in_toc">2.10.1. 最小协方差决定</h2>
<p class="calibre2">最小协方差决定（Minimum Covariance Determinant）估计器是
由 P.J. Rousseeuw 在 <a class="calibre3 pcalibre" href="#calibre_link-239" id="calibre_link-242">[3]</a> 中引入的数据集协方差的鲁棒估计 (robust estimator)。
这个想法是找出一个给定比例（h）的 “好” 观察值，它们不是离群值，
且可以计算其经验协方差矩阵。
然后将该经验协方差矩阵重新缩放以补偿所执行的观察选择（”consistency step(一致性步骤)”）。
计算最小协方差决定估计器后，可以根据其马氏距离（Mahalanobis distance）给出观测值的权重，
这导致数据集的协方差矩阵的重新加权估计（”reweighting step(重新加权步骤)”）。</p>
<p class="calibre10">Rousseeuw 和 Van Driessen  <a class="calibre3 pcalibre" href="#calibre_link-240" id="calibre_link-243">[4]</a> 开发了 FastMCD 算法，以计算最小协方差决定因子（Minimum Covariance Determinant）。
在 scikit-learn 中，该算法在将 MCD 对象拟合到数据时应用。FastMCD 算法同时计算数据集位置的鲁棒估计。</p>
<p class="calibre10">Raw估计可通过 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code class="docutils"><span class="calibre4">MinCovDet</span></code></a> 对象的 <code class="docutils"><span class="calibre4">raw_location_</span></code> 和 <code class="docutils"><span class="calibre4">raw_covariance_</span></code> 属性获得。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<table class="docutils1" frame="void" id="calibre_link-239" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[3]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-241">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-242">2</a>)</em> P. J. Rousseeuw. Least median of squares regression.
J. Am Stat Ass, 79:871, 1984.</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-240" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-243">[4]</a></td>
<td class="label1">A Fast Algorithm for the Minimum Covariance Determinant Estimator,
1999, American Statistical Association and the American Society
for Quality, TECHNOMETRICS.</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_robust_vs_empirical_covariance.html#sphx-glr-auto-examples-covariance-plot-robust-vs-empirical-covariance-py"><span class="calibre4">Robust vs Empirical covariance estimate</span></a>
关于如何将对象 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code class="docutils"><span class="calibre4">MinCovDet</span></code></a> 与数据拟合的示例， 尽管存在异常值，但估计结果仍然比较准确。</li>
<li class="toctree-l">See <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py"><span class="calibre4">Robust covariance estimation and Mahalanobis distances relevance</span></a>
马氏距离（Mahalanobis distance），针对协方差估计器 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code class="docutils"><span class="calibre4">EmpiricalCovariance</span></code></a> 和
<a class="calibre3 pcalibre" href="generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code class="docutils"><span class="calibre4">MinCovDet</span></code></a> 之间的差异进行可视化。（所以我们得到了精度矩阵的更好估计）</li>
</ul>
</div>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">Influence of outliers on location and covariance estimates</th>
<th class="head">Separating inliers from outliers using a Mahalanobis distance</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_robust_vs_empirical_covariance.html"><img alt="robust_vs_emp" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000324.jpg" class="calibre47" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_mahalanobis_distances.html"><img alt="mahalanobis" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000002.jpg" class="calibre47" /></a></td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
</div>


<div class="calibre" id="calibre_link-2">
<span id="calibre_link-675" class="calibre4"></span><h1 class="calibre5">2.11. 新奇和异常值检测</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/RyanZhiNie" class="calibre3 pcalibre">@RyanZhiNie</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@羊三</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@羊三</a><br class="calibre9" />
    </div>
<p class="calibre10">许多应用需要能够对新观测进行判断，判断其是否与现有观测服从同一分布（即新观测为内围值），相反则被认为不服从同一分布（即新观测为异常值）。 通常，这种能力被用于清理实际的数据集。必须做出两种重要区分：</p>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">新奇检测:</th>
<td class="label1">训练数据未被异常值污染，我们对于新观测中的异常情况有兴趣检测。</td>
</tr>
<tr class="row-odd"><th class="head">异常值检测:</th>
<td class="label1">训练数据包含异常值，我们需要拟合出训练数据的中心模式，以忽略有偏差的观测。</td>
</tr>
</tbody>
</table>
<p class="calibre10">scikit-learn项目提供了一套可用于新奇或异常值检测的机器学习工具。 该策略是以无监督的方式学习数据中的对象来实现的:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">然后可以使用 <cite class="calibre13">predict</cite> 方法将新观测归为内围值或异常值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">内围值被标记为1，而异常值被标记为-1。</p>
<div class="toctree-wrapper" id="calibre_link-676">
<h2 class="sigil_not_in_toc">2.11.1. Novelty Detection（新奇检测）</h2>
<p class="calibre2">考虑一个来自同一分布的数据集，以 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" /> 个特征描述、有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个观测。 现在考虑我们再往该数据集中添加一个观测。 如果新观测与原有观测有很大差异，我们就可以怀疑它是否是内围值吗？ （即是否来自同一分布？）或者相反，如果新观测与原有观测很相似，我们就无法将其与原有观测区分开吗？ 这就是新奇检测工具和方法所解决的问题。</p>
<p class="calibre10">一般来说，它将要学习出一个粗略且紧密的边界，界定出初始观测分布的轮廓，绘制在相互嵌入的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" /> 维空间中。 那么，如果后续的观测在边界划分的子空间内，则它们被认为来自与初始观测相同的总体。 否则，如果它们在边界之外，我们可以说就我们评估中给定的置信度而言，它们是异常值。</p>
<p class="calibre10">One-Class SVM（一类支持向量机）已经由 Schölkopf 等人采用以实现新奇检测，并在 <a class="calibre3 pcalibre" href="svm.html#svm"><span class="calibre4">支持向量机</span></a> 模块的 <a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 对象中实现。
需要选择 kernel 和 scalar 参数来定义边界。 通常选择 RBF kernel，即使没有确切的公式或算法来设置其带宽参数。 这是 scikit-learn 实现中的默认值。 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000702.jpg" alt="\nu" /> 参数，也称为一类支持向量机的边沿，对应于在边界之外找到新的但内围的观测的概率。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://dl.acm.org/citation.cfm?id=1119749">Estimating the support of a high-dimensional distribution</a> Schölkopf,
Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py"><span class="calibre4">One-class SVM with non-linear kernel (RBF)</span></a> ，通过 <a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 对象学习一些数据来将边界可视化。</li>
</ul>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/svm/plot_oneclass.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_oneclass_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000466.jpg" class="calibre27" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-677">
<h2 class="sigil_not_in_toc">2.11.2. Outlier Detection（异常值检测）</h2>
<p class="calibre2">异常值检测类似于新奇检测，其目的是将内围观测的中心与一些被称为 “异常值” 的污染数据进行分离。 然而，在异常值检测的情况下，我们没有干净且适用于训练任何工具的数据集来代表内围观测的总体。</p>
<div class="toctree-wrapper" id="calibre_link-678">
<h3 class="sigil_not_in_toc1">2.11.2.1. Fitting an elliptic envelope（椭圆模型拟合）</h3>
<p class="calibre2">实现异常值检测的一种常见方式是假设内围数据来自已知分布（例如，数据服从高斯分布）。 从这个假设来看，我们通常试图定义数据的 “形状”，并且可以将异常观测定义为足够远离拟合形状的观测。</p>
<p class="calibre10">scikit-learn 提供了 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 对象，它能拟合出数据的稳健协方差估计，从而为中心数据点拟合出一个椭圆，忽略中心模式之外的点。</p>
<p class="calibre10">例如，假设内围数据服从高斯分布，它将稳健地（即不受异常值的影响）估计内围位置和协方差。 从该估计得到的马氏距离用于得出异常度量。 该策略如下图所示。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_mahalanobis_distances.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_mahalanobis_distances_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000002.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_mahalanobis_distances.html#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py"><span class="calibre4">Robust covariance estimation and Mahalanobis distances relevance</span></a> 说明对位置和协方差使用标准估计
(<a class="calibre3 pcalibre" href="generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code class="docutils"><span class="calibre4">covariance.EmpiricalCovariance</span></code></a>) 或稳健估计
(<a class="calibre3 pcalibre" href="generated/sklearn.covariance.MinCovDet.html#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code class="docutils"><span class="calibre4">covariance.MinCovDet</span></code></a>) 来评估观测的异常程度的差异。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum
covariance determinant estimator” Technometrics 41(3), 212 (1999)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-679">
<span id="calibre_link-680" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.11.2.2. Isolation Forest（隔离森林）</h3>
<dl class="calibre10">
<dt class="calibre18">在高维数据集中实现异常值检测的一种有效方法是使用随机森林。</dt>
<dd class="calibre19"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a> 通过随机选择特征然后随机选择所选特征的最大值和最小值之间的分割值来隔离观测。</dd>
</dl>
<p class="calibre10">由于递归划分可以由树形结构表示，因此隔离样本所需的分割次数等同于从根节点到终止节点的路径长度。</p>
<p class="calibre10">在这样的随机树的森林中取平均的路径长度是数据正态性和我们的决策功能的量度。</p>
<p class="calibre10">随机划分能为异常观测产生明显的较短路径。 因此，当随机树的森林共同为特定样本产生较短的路径长度时，这些样本就很有可能是异常观测。</p>
<p class="calibre10">该策略如下图所示。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_isolation_forest.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_isolation_forest_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000032.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py"><span class="calibre4">IsolationForest example</span></a>  说明隔离森林的用法。</li>
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py"><span class="calibre4">Outlier detection with several methods.</span></a> 比较 <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a> 与
<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a>,
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> (调整为执行类似异常值检测的方法）和基于协方差使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a>
进行异常值检测。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM‘08. Eighth IEEE International Conference on.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-681">
<h3 class="sigil_not_in_toc1">2.11.2.3. Local Outlier Factor（局部异常系数）</h3>
<p class="calibre2">对中等高维数据集实现异常值检测的另一种有效方法是使用局部异常系数（LOF）算法。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a> （LOF）算法计算出反映观测异常程度的得分（称为局部异常系数）。 它测量给定数据点相对于其邻近点的局部密度偏差。 算法思想是检测出具有比其邻近点明显更低密度的样本。</div>
</blockquote>
<p class="calibre10">实际上，局部密度从 k 个最近邻得到。 观测数据的 LOF 得分等于其 k 个最近邻的平均局部密度与其本身密度的比值：正常情况预期具有与其近邻类似的局部密度，而异常数据 则预计比局部密度要小得多。</p>
<p class="calibre10">考虑的k个近邻数（别名参数 n_neighbors ）通常选择 1) 大于聚类必须包含的对象最小数量，以便其它对象相对于该聚类成为局部异常值，并且 2) 小于可能成为局部异常值对象的最大数量。 在实践中，这样的信息通常不可用，并且使 n_neighbors = 20 似乎通常都能很好地工作。 当异常值的比例较高时（即大于 10% 时，如下面的例子），n_neighbors 应该较大（在下面的例子中，n_neighbors = 35）。</p>
<p class="calibre10">LOF 算法的优点是考虑到数据集的局部和全局属性：即使在异常样本具有不同潜在密度的数据集中，它也能够表现得很好。 问题不在于样本是如何被分离的，而是样本与周围近邻的分离程度有多大。</p>
<p class="calibre10">该策略如下图所示。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_lof.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_lof_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000593.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_lof.html#sphx-glr-auto-examples-neighbors-plot-lof-py"><span class="calibre4">Anomaly detection with Local Outlier Factor (LOF)</span></a> 的  <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a> 使用说明。</li>
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py"><span class="calibre4">Outlier detection with several methods.</span></a> 与其它异常检测方法进行比较。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Breunig, Kriegel, Ng, and Sander (2000)
<a class="calibre3 pcalibre" href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf">LOF: identifying density-based local outliers.</a>
Proc. ACM SIGMOD</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-682">
<h3 class="sigil_not_in_toc1">2.11.2.4. 一类支持向量机与椭圆模型与隔离森林与局部异常系数</h3>
<p class="calibre2">严格来说， One-class SVM （一类支持向量机）不是异常值检测方法，而是一种新奇检测方法：其训练集不应该被异常值污染，因为算法可能将它们拟合。 也就是说，高维度的异常值检测或对数据分布不做任何假设是非常具有挑战性的， 而一类支持向量机在这些情况下产生出有用的结果。</p>
<p class="calibre10">下面的例子说明了当数据越来越不单峰时，
<a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 的表现越来越差。
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 在具有多种模式的数据上表现得更好， <a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a> 和
<a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a> 在每种情况下都表现良好。</p>
<table border="1" class="docutils1" id="calibre_link-683">
<caption class="calibre54"><span class="calibre4"><strong class="calibre14">Comparing One-class SVM, Isolation Forest, LOF, and Elliptic Envelope</strong></span></caption>
<colgroup class="calibre21">
<col width="40%" class="label"></col>
<col width="60%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">对于中心化和椭圆形的内围模式，
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 不适用于内围总体的旋转对称性。 此外，它拟合了一小部分存在于训练集中的异常值。 相反，基于拟合
<a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 的决策规则学习出一个椭圆，对于内围分布拟合良好。
<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a>
和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a> 也表现良好。</td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html"><img alt="outlier1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000537.jpg" class="calibre55" /></a></td>
</tr>
<tr class="row-odd"><td class="label1">由于内围分布变为双峰，所以
<a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 对内围数据拟合得不好。 但是，我们可以看到
<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a>,
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a>
难以检测双峰，而且 <a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> 往往会过拟合：因为它没有内围模型， 便随机地把一些异常值当做内围值聚类在某个区域。</td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html"><img alt="outlier2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000620.jpg" class="calibre55" /></a></td>
</tr>
<tr class="calibre23"><td class="label1">如果内围分布极度不正态，则
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a>,
<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a>
和 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a> 能构造出合理的近似结果,
而 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 完全失败。</td>
<td class="label1"><a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html"><img alt="outlier3" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000139.jpg" class="calibre55" /></a></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参见 <a class="calibre3 pcalibre" href="../auto_examples/covariance/plot_outlier_detection.html#sphx-glr-auto-examples-covariance-plot-outlier-detection-py"><span class="calibre4">Outlier detection with several methods.</span></a> 比较
<a class="calibre3 pcalibre" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="docutils"><span class="calibre4">svm.OneClassSVM</span></code></a> (调整为执行类似异常值检测的方法）,
<a class="calibre3 pcalibre" href="generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code class="docutils"><span class="calibre4">ensemble.IsolationForest</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="docutils"><span class="calibre4">neighbors.LocalOutlierFactor</span></code></a>
和基于协方差使用 <a class="calibre3 pcalibre" href="generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="docutils"><span class="calibre4">covariance.EllipticEnvelope</span></code></a> 进行异常值检测。</li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-4">
<span id="calibre_link-684" class="calibre4"></span><h1 class="calibre5">2.12. 密度估计</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@不将就</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Xi</a><br class="calibre9" />
    </div>
<p class="calibre10">密度估计在无监督学习，特征工程和数据建模之间划分了界线。一些最流行和最有用的密度估计方法是混合模型，如高斯混合( <a class="calibre3 pcalibre" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="docutils"><span class="calibre4">sklearn.mixture.GaussianMixture</span></code></a> ), 和基于邻近的方法( <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="docutils"><span class="calibre4">sklearn.neighbors.KernelDensity</span></code></a> )，如核密度估计。 <a class="calibre3 pcalibre" href="clustering.html#clustering"><span class="calibre4">clustering</span></a> 一节中更充分地讨论了高斯混合，因为此方法也用作为一种无监督聚类方案。</p>
<p class="calibre10">密度估计是一个非常简单的概念，大多数人已经熟悉了其中一种常用的密度估计技术：直方图。</p>
<div class="toctree-wrapper" id="calibre_link-685">
<h2 class="sigil_not_in_toc">2.12.1. 密度估计: 直方图</h2>
<p class="calibre2">直方图是一种简单的数据可视化方法，其中定义了组( bins )，并且统计了每个组( bin )中的数据点的数量。在下图的左上角中可以看到一个直方图的例子:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="hist_to_kde" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000048.jpg" class="calibre26" /></a></strong></p>
<p class="calibre10">然而，直方图的一个主要问题是组( binning )的选择可能会对得到的可视化结果造成不相称的影响。考虑上图中右上角的图, 它显示了相同数据下组( bins )向右移动后的直方图。这两个可视化的结果看起来完全不同，可能会导致对数据作出不同的解释。</p>
<p class="calibre10">直观地说，你也可以把一个直方图看成由一堆块组成，每个点上放一个块，通过在合适的网格空间中堆积这些块，我们就可以得到直方图。但是，如果不是把这些块堆叠在一个规则的网格上，而是把每个块的中心定位在它所代表的点上，然后把每个位置的总高度相加呢?这样可以得到如上图左
下角所示的可视化.它可能不像直方图那样整洁，但是由数据决定块的位置意味着它能更好地表示基本的数据。</p>
<p class="calibre10">这个可视化是核密度估计的一个例子，该例中用的是一种”顶帽”核（即每个点上的方块）。我们可以通过使用一种更平滑的核来得到一个更平滑的分布。上图右下角展示了一个高斯核密度估计，其中每个点都给总的分布贡献一条高斯曲线。结果是从数据中得到了一个平滑的密度估计，并且可作为一个强大的非参数模型用来估计这些点的分布。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-686">
<span id="calibre_link-687" class="calibre4"></span><h2 class="sigil_not_in_toc">2.12.2. 核密度估计</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="docutils"><span class="calibre4">sklearn.neighbors.KernelDensity</span></code></a> 实现了 scikit-learn 中的核密度估计，它使用 Ball Tree 或 KD Tree 来进行高效查询（有关这些讨论请参见 <a class="calibre3 pcalibre" href="neighbors.html#neighbors"><span class="calibre4">最近邻</span></a> ）。尽管为了简单起见上述示例采用的是一维数据集，但实际上核密度估计能够用在任意维度上, 不过在实际应用中,维数灾难会导致其在高维上的性能降低。</p>
<p class="calibre10">如下图所示, 从双峰分布中绘制了100个点，并展示了选用三个不同核的核密度估计:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="kde_1d_distribution" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000722.jpg" class="calibre26" /></a></strong></p>
<p class="calibre10">图中可以很明显地看到核的形状如何影响结果分布的平滑度. 使用 scikit-learn 核密度估计的方法如下所示：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors.kde</span> <span class="calibre4">import</span> <span class="calibre4">KernelDensity</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kde</span> <span class="calibre4">=</span> <span class="calibre4">KernelDensity</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'gaussian'</span><span class="calibre4">,</span> <span class="calibre4">bandwidth</span><span class="calibre4">=</span><span class="calibre4">0.2</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kde</span><span class="calibre4">.</span><span class="calibre4">score_samples</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span>
<span class="calibre4">       -0.41076071])</span>
</pre>
</div>
</div>
<p class="calibre10">如上所示,这里我们选用的是高斯核 <code class="docutils"><span class="calibre4">kernel='gaussian'</span></code> .在数学上，核是由带宽参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 控制的正值函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000460.jpg" alt="K(x;h)" /> . 给定核的形状后,在一组点 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000007.jpg" alt="x_i; i=1\cdots N" /> 内的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 点处的密度估计由下式给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000059.jpg" alt="\rho_K(y) = \sum_{i=1}^{N} K((y - x_i) / h)" class="math" /></p>
</div>
<p class="calibre10">这里的带宽作为平滑参数，用来平衡结果中偏差和方差的值。 大的带宽会导致非常平滑（即高偏差）密度分布,而小的带宽则导致不平滑（即高方差）密度分布。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="docutils"><span class="calibre4">sklearn.neighbors.KernelDensity</span></code></a> 实现了一些常见形状的核, 如下图所示:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="kde_kernels" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000579.jpg" class="calibre26" /></a></strong></p>
<p class="calibre10">这些核的形式如下所示:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first">Gaussian kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'gaussian'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000133.jpg" alt="K(x; h) \propto \exp(- \frac{x^2}{2h^2} )" /></p>
</li>
<li class="toctree-l"><p class="first">Tophat kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'tophat'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000429.jpg" alt="K(x; h) \propto 1" /> if <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000641.jpg" alt="x &lt; h" /></p>
</li>
<li class="toctree-l"><p class="first">Epanechnikov kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'epanechnikov'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000240.jpg" alt="K(x; h) \propto 1 - \frac{x^2}{h^2}" /></p>
</li>
<li class="toctree-l"><p class="first">Exponential kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'exponential'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000765.jpg" alt="K(x; h) \propto \exp(-x/h)" /></p>
</li>
<li class="toctree-l"><p class="first">Linear kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'linear'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000478.jpg" alt="K(x; h) \propto 1 - x/h" /> if <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000641.jpg" alt="x &lt; h" /></p>
</li>
<li class="toctree-l"><p class="first">Cosine kernel (<code class="docutils"><span class="calibre4">kernel</span> <span class="calibre4">=</span> <span class="calibre4">'cosine'</span></code>)</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000529.jpg" alt="K(x; h) \propto \cos(\frac{\pi x}{2h})" /> 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000641.jpg" alt="x &lt; h" /></p>
</li>
</ul>
<p class="calibre10">核密度估计可以与任何有效的距离度量一起使用（可用度量列表请参见 <a class="calibre3 pcalibre" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="docutils"><span class="calibre4">sklearn.neighbors.DistanceMetric</span></code></a> ），
但其结果被适当地归一化处理,仅适用于欧几里德度量标准。 一个特别有用的度量是测量球体上的点与点之间角距离
的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Haversine_formula">Haversine distance</a> 。
下面是使用核密度估计来对地理空间数据进行可视化的示例，本例中南美大陆两种不同物种的观测分布如图:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_species_kde.html"></a></strong></p>
<p class="calibre10">核密度估计的另一个有用的应用是从数据集中学习出一个非参数生成模型，以便有效地从该生成模型中绘制新的样本。 以下是使用此过程创建一组新的手写数字的示例，使用的是高斯核对数据的 PCA 投影进行学习：</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_digits_kde_sampling.html"><img alt="digits_kde" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000841.jpg" class="calibre26" /></a></strong></p>
<p class="calibre10">“新”数据由输入数据线性组合而成，其权重根据 KDE 模型按概率给出。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_kde_1d.html#sphx-glr-auto-examples-neighbors-plot-kde-1d-py"><span class="calibre4">Simple 1D Kernel Density Estimation</span></a>: 一维简单核密度估计的计算。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_digits_kde_sampling.html#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py"><span class="calibre4">Kernel Density Estimation</span></a>: 使用核密度估计来学习手写数字数据生成模型，以及使用该模型绘制新样本的示例</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py"><span class="calibre4">Kernel Density Estimate of Species Distributions</span></a>: 使用Haversine距离度量来显示地理空间数据的核密度估计示例.</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-53">
<span id="calibre_link-688" class="calibre4"></span><h1 class="calibre5">2.13. 神经网络模型（无监督）</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@不将就</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@夜神月</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper" id="calibre_link-689">
<span id="calibre_link-690" class="calibre4"></span><h2 class="sigil_not_in_toc">2.13.1. 限制波尔兹曼机</h2>
<p class="calibre2">Restricted Boltzmann machines (RBM)（限制玻尔兹曼机）是基于概率模型的无监督非线性特征学习器。当用 RBM 或 RBMs 中的层次结构提取的特征在馈入线性分类器（如线性支持向量机或感知机）时通常会获得良好的结果。</p>
<p class="calibre10">该模型对输入的分布作出假设。目前，scikit-learn 只提供了 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code class="docutils"><span class="calibre4">BernoulliRBM</span></code></a>，它假定输入是二值的，或者是 0 到 1 之间的值，每个值都编码特定特征被激活的概率。</p>
<p class="calibre10">RBM 尝试使用特定图形模型最大化数据的可能性。所使用的参数学习算法（ <a class="calibre3 pcalibre" href="#calibre_link-54"><span class="calibre4">Stochastic Maximum Likelihood</span></a> （随机最大似然））要防止特征表示偏离输入数据，这使得它们能学习到有趣的特征，但使得该模型对于小数据集不太有用且通常对于密度估计无效。</p>
<p class="calibre10">该方法随着独立RBM的权重初始化深层神经网络而普及。这种方法被称为无监督的预训练（unsupervised pre-training）。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_rbm_logistic_classification.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_rbm_logistic_classification_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000408.jpg" class="calibre56" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py"><span class="calibre4">Restricted Boltzmann Machine features for digit classification</span></a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-691">
<h3 class="sigil_not_in_toc1">2.13.1.1. 图形模型和参数化</h3>
<p class="calibre2">RBM 的图形模型是一个全连接的二分图（fully-connected bipartite graph）。</p>
<img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/rbm_graph.png" class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000637.jpg" />
<p class="calibre10">节点是随机变量，其状态取决于它连接到的其他节点的状态。 因此，为了简单起见，模型被参数化为连接的权重以及每个可见和隐藏单元的一个偏置项。
我们用能量函数衡量联合概率分布的质量:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000076.jpg" alt="E(\mathbf{v}, \mathbf{h}) = \sum_i \sum_j w_{ij}v_ih_j + \sum_i b_iv_i   + \sum_j c_jh_j" class="math" /></p>
</div>
<p class="calibre10">在上面的公式中， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000152.jpg" alt="\mathbf{b}" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000582.jpg" alt="\mathbf{c}" /> 分别是可见层和隐藏层的偏置向量。 模型的联合概率是根据能量来定义的:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000262.jpg" alt="P(\mathbf{v}, \mathbf{h}) = \frac{e^{-E(\mathbf{v}, \mathbf{h})}}{Z}" class="math" /></p>
</div>
<p class="calibre10">“限制”是指模型的二分图结构，它禁止隐藏单元之间或可见单元之间的直接交互。 这代表以下条件独立性成立:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000786.jpg" alt="h_i \bot h_j | \mathbf{v} \\ v_i \bot v_j | \mathbf{h}" class="math" /></p>
</div>
<p class="calibre10">二分图结构允许使用高效的块吉比斯采样(block Gibbs sampling)进行推断。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-692">
<h3 class="sigil_not_in_toc1">2.13.1.2. 伯努利限制玻尔兹曼机</h3>
<p class="calibre2">在 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code class="docutils"><span class="calibre4">BernoulliRBM</span></code></a> 中，所有单位都是二进制随机单元。 这意味着输入数据应该是二进制的，或者在 0 和 1 之间的实数值表示可见单元活跃或不活跃的概率。 这是一个很好的字符识别模型，其中的关注点是哪些像素是活跃的，哪些不是。 对于自然场景的图像，它不再适合，因为背景，深度和相邻像素的趋势取相同的值。</p>
<p class="calibre10">每个单位的条件概率分布由其接收的输入的sigmoid函数给出:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000276.jpg" alt="P(v_i=1|\mathbf{h}) = \sigma(\sum_j w_{ij}h_j + b_i) \\ P(h_i=1|\mathbf{v}) = \sigma(\sum_i w_{ij}v_i + c_j)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000612.jpg" alt="\sigma" /> 是Sigmoid函数:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000525.jpg" alt="\sigma(x) = \frac{1}{1 + e^{-x}}" class="math" /></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-54">
<span id="calibre_link-693" class="calibre4"></span><h3 class="sigil_not_in_toc1">2.13.1.3. 随机最大似然学习</h3>
<p class="calibre2">在 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code class="docutils"><span class="calibre4">BernoulliRBM</span></code></a> 函数中实现的学习算法被称为随机最大似然（Stochastic Maximum Likelihood (SML)）或持续对比发散（Persistent Contrastive Divergence (PCD)）。由于数据的似然函数的形式，直接优化最大似然是不可行的:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000137.jpg" alt="\log P(v) = \log \sum_h e^{-E(v, h)} - \log \sum_{x, y} e^{-E(x, y)}" class="math" /></p>
</div>
<p class="calibre10">为了简单起见，上面的等式是针对单个训练样本所写的。相对于权重的梯度由对应于上述的两个项构成。根据它们的符号，它们通常被称为正梯度和负梯度。在这种实现中，按照小批量梯度（mini-batches of samples ）对梯度进行计算。</p>
<p class="calibre10">在 maximizing the log-likelihood （最大化对数似然度）的情况下，正梯度使模型更倾向于与观察到的训练数据兼容的隐藏状态。由于 RBM 的二分体结构，可以有效地计算。然而，负梯度是棘手的。其目标是降低模型偏好的联合状态的能量，从而使数据保持真实。可以通过马尔可夫链蒙特卡罗近似，使用块 Gibbs 采样，通过迭代地对每个给定另一个的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000873.jpg" alt="v" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 进行采样，直到链混合。以这种方式产生的样品有时被称为幻想粒子。这是无效的，很难确定马可夫链是否混合。</p>
<p class="calibre10">对比发散方法建议在经过少量迭代后停止链，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 通常为 1.该方法快速且方差小，但样本远离模型分布。</p>
<p class="calibre10">持续的对比分歧解决这个问题。而不是每次需要梯度启动一个新的链，并且只执行一个 Gibbs 采样步骤，在 PCD 中，我们保留了在每个权重更新之后更新的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> Gibbs 步长的多个链（幻想粒子）。这使得颗粒更彻底地探索空间.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">“A fast learning algorithm for deep belief nets”</a>
G. Hinton, S. Osindero, Y.-W. Teh, 2006</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf">“Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient”</a>
T. Tieleman, 2008</li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-65">
<span id="calibre_link-694" class="calibre4"></span><h1 class="calibre5">3. 模型选择和评估</h1>
<div class="toctree-wrapper">
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html">3.1. 交叉验证：评估估算器的表现</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id2">3.1.1. 计算交叉验证的指标</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#cross-validate">3.1.1.1. cross_validate 函数和多度量评估</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id3">3.1.1.2. 通过交叉验证获取预测</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id4">3.1.2. 交叉验证迭代器</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#iid-cv">3.1.3. 交叉验证迭代器&ndash;循环遍历数据</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#k">3.1.3.1. K 折</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id6">3.1.3.2. 重复 K-折交叉验证</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#loo">3.1.3.3. 留一交叉验证 (LOO)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#p-lpo">3.1.3.4. 留 P 交叉验证 (LPO)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#a-k-a-shuffle-split">3.1.3.5. 随机排列交叉验证 a.k.a. Shuffle &amp; Split</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id7">3.1.4. 基于类标签、具有分层的交叉验证迭代器</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id8">3.1.4.1. 分层 k 折</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#split">3.1.4.2. 分层随机 Split</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#group-cv">3.1.5. 用于分组数据的交叉验证迭代器</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#k-fold">3.1.5.1. 组 k-fold</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id10">3.1.5.2. 留一组交叉验证</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#p">3.1.5.3. 留 P 组交叉验证</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#group-shuffle-split">3.1.5.4. Group Shuffle Split</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id11">3.1.6. 预定义的折叠 / 验证集</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#timeseries-cv">3.1.7. 交叉验证在时间序列数据中应用</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id13">3.1.7.1. 时间序列分割</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#a-note-on-shuffling">3.1.8. A note on shuffling</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/cross_validation.html#id14">3.1.9. 交叉验证和模型选择</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html">3.2. 调整估计器的超参数</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id3">3.2.1. 网格追踪法&ndash;穷尽的网格搜索</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#randomized-parameter-search">3.2.2. 随机参数优化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#grid-search-tips">3.2.3. 参数搜索技巧</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#gridsearch-scoring">3.2.3.1. 指定目标度量</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#multimetric-grid-search">3.2.3.2. 为评估指定多个指标</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id16">3.2.3.3. 复合估计和参数空间</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id18">3.2.3.4. 模型选择：开发和评估</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id19">3.2.3.5. 并行机制</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id20">3.2.3.6. 对故障的鲁棒性</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#alternative-cv">3.2.4. 暴力参数搜索的替代方案</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id22">3.2.4.1. 模型特定交叉验证</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.ElasticNetCV.html">3.2.4.1.1. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.ElasticNetCV</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LarsCV.html">3.2.4.1.2. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.LarsCV</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoCV.html">3.2.4.1.3. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.LassoCV</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoCV.html#examples-using-sklearn-linear-model-lassocv">3.2.4.1.3.1. Examples using <code class="docutils"><span class="calibre4">sklearn.linear_model.LassoCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoLarsCV.html">3.2.4.1.4. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.LassoLarsCV</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoLarsCV.html#examples-using-sklearn-linear-model-lassolarscv">3.2.4.1.4.1. Examples using <code class="docutils"><span class="calibre4">sklearn.linear_model.LassoLarsCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LogisticRegressionCV.html">3.2.4.1.5. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.LogisticRegressionCV</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html">3.2.4.1.6. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.MultiTaskElasticNetCV</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.MultiTaskLassoCV.html">3.2.4.1.7. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.MultiTaskLassoCV</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.OrthogonalMatchingPursuitCV.html">3.2.4.1.8. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.OrthogonalMatchingPursuitCV</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.OrthogonalMatchingPursuitCV.html#examples-using-sklearn-linear-model-orthogonalmatchingpursuitcv">3.2.4.1.8.1. Examples using <code class="docutils"><span class="calibre4">sklearn.linear_model.OrthogonalMatchingPursuitCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.RidgeCV.html">3.2.4.1.9. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.RidgeCV</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.RidgeCV.html#examples-using-sklearn-linear-model-ridgecv">3.2.4.1.9.1. Examples using <code class="docutils"><span class="calibre4">sklearn.linear_model.RidgeCV</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.RidgeClassifierCV.html">3.2.4.1.10. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.RidgeClassifierCV</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#id23">3.2.4.2. 信息标准</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoLarsIC.html">3.2.4.2.1. <code class="docutils"><span class="calibre4">sklearn.linear_model</span></code>.LassoLarsIC</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.linear_model.LassoLarsIC.html#examples-using-sklearn-linear-model-lassolarsic">3.2.4.2.1.1. Examples using <code class="docutils"><span class="calibre4">sklearn.linear_model.LassoLarsIC</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/grid_search.html#out-of-bag">3.2.4.3. 出袋估计</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.RandomForestClassifier.html">3.2.4.3.1. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.RandomForestClassifier</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.RandomForestClassifier.html#examples-using-sklearn-ensemble-randomforestclassifier">3.2.4.3.1.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestClassifier</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.RandomForestRegressor.html">3.2.4.3.2. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.RandomForestRegressor</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.RandomForestRegressor.html#examples-using-sklearn-ensemble-randomforestregressor">3.2.4.3.2.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.RandomForestRegressor</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">3.2.4.3.3. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.ExtraTreesClassifier</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#examples-using-sklearn-ensemble-extratreesclassifier">3.2.4.3.3.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.ExtraTreesClassifier</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.ExtraTreesRegressor.html">3.2.4.3.4. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.ExtraTreesRegressor</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#examples-using-sklearn-ensemble-extratreesregressor">3.2.4.3.4.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.ExtraTreesRegressor</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">3.2.4.3.5. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.GradientBoostingClassifier</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#examples-using-sklearn-ensemble-gradientboostingclassifier">3.2.4.3.5.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.GradientBoostingClassifier</span></code></a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.GradientBoostingRegressor.html">3.2.4.3.6. <code class="docutils"><span class="calibre4">sklearn.ensemble</span></code>.GradientBoostingRegressor</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#examples-using-sklearn-ensemble-gradientboostingregressor">3.2.4.3.6.1. Examples using <code class="docutils"><span class="calibre4">sklearn.ensemble.GradientBoostingRegressor</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html">3.3. 模型评估: 量化预测的质量</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#scoring">3.3.1. <code class="docutils"><span class="calibre4">scoring</span></code> 参数: 定义模型评估规则</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#id2">3.3.1.1. 常见场景: 预定义值</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#metric">3.3.1.2. 根据 metric 函数定义您的评分策略</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#diy-scoring">3.3.1.3. 实现自己的记分对象</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#multimetric-scoring">3.3.1.4. 使用多个指数评估</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#classification-metrics">3.3.2. 分类指标</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#multilabel">3.3.2.1. 从二分到多分类和 multilabel</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#accuracy-score">3.3.2.2. 精确度得分</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#cohen-s-kappa">3.3.2.3. Cohen’s kappa</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#confusion-matrix">3.3.2.4. 混淆矩阵</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#classification-report">3.3.2.5. 分类报告</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#hamming-loss">3.3.2.6. 汉明损失</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#jaccard-score">3.3.2.7. Jaccard 相似系数 score</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#f-measures">3.3.2.8. 精准，召回和 F-measures</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#id14">3.3.2.8.1. 二分类</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#id15">3.3.2.8.2. 多类和多标签分类</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#hinge-loss">3.3.2.9. Hinge loss</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#log">3.3.2.10. Log 损失</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#matthews-corrcoef">3.3.2.11. 马修斯相关系数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#receiver-operating-characteristic-roc">3.3.2.12. Receiver operating characteristic (ROC)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#zero-one-loss">3.3.2.13. 零一损失</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#brier">3.3.2.14. Brier 分数损失</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#multilabel-ranking-metrics">3.3.3. 多标签排名指标</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#coverage-error">3.3.3.1. 覆盖误差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#label-ranking-average-precision">3.3.3.2. 标签排名平均精度</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#label-ranking-loss">3.3.3.3. 排序损失</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#regression-metrics">3.3.4. 回归指标</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#explained-variance-score">3.3.4.1. 解释方差得分</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#mean-absolute-error">3.3.4.2. 平均绝对误差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#mean-squared-error">3.3.4.3. 均方误差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#mean-squared-log-error">3.3.4.4. 均方误差对数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#median-absolute-error">3.3.4.5. 中位绝对误差</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#r2-score">3.3.4.6. R² score, 可决系数</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#clustering-metrics">3.3.5. 聚类指标</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_evaluation.html#dummy-estimators">3.3.6. 虚拟估计</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_persistence.html">3.4. 模型持久化</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_persistence.html#id2">3.4.1. 持久化示例</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/model_persistence.html#persistence-limitations">3.4.2. 安全性和可维护性的局限性</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/learning_curve.html">3.5. 验证曲线: 绘制分数以评估模型</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/learning_curve.html#validation-curve">3.5.1. 验证曲线</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/learning_curve.html#learning-curve">3.5.2. 学习曲线</a></li>
</ul>
</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-55">
<span id="calibre_link-695" class="calibre4"></span><h1 class="calibre5">3.1. 交叉验证：评估估算器的表现</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@想和太阳肩并肩</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@樊雯</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@\S^R^Y/</a><br class="calibre9" />
    </div>
<p class="calibre10">学习预测函数的参数，并在相同数据集上进行测试是一种错误的做法:  一个仅给出测试用例标签的模型将会获得极高的分数，但对于尚未出现过的数据它则无法预测出任何有用的信息。
这种情况称为 <strong class="calibre14">overfitting（过拟合）</strong>.
为了避免这种情况，在进行（监督）机器学习实验时，通常取出部分可利用数据作为 <strong class="calibre14">test set（测试数据集）</strong> <code class="docutils"><span class="calibre4">X_test,</span> <span class="calibre4">y_test</span></code>。</p>
<p class="calibre10">需要强调的是这里说的 “experiment(实验)” 并不仅限于学术（academic），因为即使是在商业场景下机器学习也往往是从实验开始的。</p>
<p class="calibre10">利用 scikit-learn 包中的 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="sklearn.model_selection.train_test_split"><code class="docutils"><span class="calibre4">train_test_split</span></code></a> 辅助函数可以很快地将实验数据集划分为任何训练集（training sets）和测试集（test sets）。
下面让我们载入 iris 数据集，并在此数据集上训练出线性支持向量机:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">train_test_split</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">((150, 4), (150,))</span>
</pre>
</div>
</div>
<p class="calibre10">我们能快速采样到原数据集的 40% 作为测试集，从而测试（评估）我们的分类器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">train_test_split</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.4</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">((90, 4), (90,))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">((60, 4), (60,))</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span>                           
<span class="calibre4">0.96...</span>
</pre>
</div>
</div>
<p class="calibre10">当评价估计器的不同设置（”hyperparameters(超参数)”）时，例如手动为 SVM 设置的 <code class="docutils"><span class="calibre4">C</span></code> 参数，
由于在训练集上，通过调整参数设置使估计器的性能达到了最佳状态；但 <em class="calibre13">在测试集上</em> 可能会出现过拟合的情况。
此时，测试集上的信息反馈足以颠覆训练好的模型，评估的指标不再有效反映出模型的泛化性能。
为了解决此类问题，还应该准备另一部分被称为 “validation set(验证集)” 的数据集，模型训练完成以后在验证集上对模型进行评估。
当验证集上的评估实验比较成功时，在测试集上进行最后的评估。</p>
<p class="calibre10">然而，通过将原始数据分为3个数据集合，我们就大大减少了可用于模型学习的样本数量，
并且得到的结果依赖于集合对（训练，验证）的随机选择。</p>
<p class="calibre10">这个问题可以通过
<a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">交叉验证（CV 缩写）</a>
来解决。
交叉验证仍需要测试集做最后的模型评估，但不再需要验证集。</p>
<p class="calibre10">最基本的方法被称之为，<em class="calibre13">k-折交叉验证</em> 。
k-折交叉验证将训练集划分为 k 个较小的集合（其他方法会在下面描述，主要原则基本相同）。
每一个 <em class="calibre13">k</em> 折都会遵循下面的过程：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">将 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000468.jpg" alt="k-1" /> 份训练集子集作为 training data （训练集）训练模型，</li>
<li class="toctree-l">将剩余的 1 份训练集子集作为验证集用于模型验证（也就是利用该数据集计算模型的性能指标，例如准确率）。</li>
</ul>
</div>
</blockquote>
<p class="calibre10"><em class="calibre13">k</em>-折交叉验证得出的性能指标是循环计算中每个值的平均值。
该方法虽然计算代价很高，但是它不会浪费太多的数据（如固定任意测试集的情况一样），
在处理样本数据集较少的问题（例如，逆向推理）时比较有优势。</p>
<div class="toctree-wrapper" id="calibre_link-696">
<h2 class="sigil_not_in_toc">3.1.1. 计算交叉验证的指标</h2>
<p class="calibre2">使用交叉验证最简单的方法是在估计器和数据集上调用 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 辅助函数。</p>
<p class="calibre10">下面的例子展示了如何通过分割数据，拟合模型和计算连续 5 次的分数（每次不同分割）来估计 linear kernel 支持向量机在 iris 数据集上的精度:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span>                                              
<span class="calibre4">array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])</span>
</pre>
</div>
</div>
<p class="calibre10">评分估计的平均得分和 95% 置信区间由此给出:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Accuracy: </span><span class="calibre4">%0.2f</span><span class="calibre4"> (+/- </span><span class="calibre4">%0.2f</span><span class="calibre4">)"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">(),</span> <span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">std</span><span class="calibre4">()</span> <span class="calibre4">*</span> <span class="calibre4">2</span><span class="calibre4">))</span>
<span class="calibre4">Accuracy: 0.98 (+/- 0.03)</span>
</pre>
</div>
</div>
<p class="calibre10">默认情况下，每个 CV 迭代计算的分数是估计器的 <code class="docutils"><span class="calibre4">score</span></code> 方法。可以通过使用 scoring 参数来改变计算方式如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_score</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'f1_macro'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span>                                              
<span class="calibre4">array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])</span>
</pre>
</div>
</div>
<p class="calibre10">详情请参阅 <a class="calibre3 pcalibre" href="model_evaluation.html#scoring-parameter"><span class="calibre4">scoring 参数: 定义模型评估规则</span></a> 。
在 Iris 数据集的情形下，样本在各个目标类别之间是平衡的，因此准确度和 F1-score 几乎相等。</p>
<p class="calibre10">当 <code class="docutils"><span class="calibre4">cv</span></code> 参数是一个整数时， <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 默认使用 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 或 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="docutils"><span class="calibre4">StratifiedKFold</span></code></a> 策略，后者会在估计器派生自 <a class="calibre3 pcalibre" href="generated/sklearn.base.ClassifierMixin.html#sklearn.base.ClassifierMixin" title="sklearn.base.ClassifierMixin"><code class="docutils"><span class="calibre4">ClassifierMixin</span></code></a> 时使用。</p>
<p class="calibre10">也可以通过传入一个交叉验证迭代器来使用其他交叉验证策略，比如:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">ShuffleSplit</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">n_samples</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cv</span> <span class="calibre4">=</span> <span class="calibre4">ShuffleSplit</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">cv</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                    
<span class="calibre4">array([ 0.97...,  0.97...,  1.        ])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">保留数据的数据转换</p>
<p class="calibre10">正如在训练集中保留的数据上测试一个 predictor （预测器）是很重要的一样，预处理（如标准化，特征选择等）和类似的 <a class="calibre3 pcalibre" href="../data_transforms.html#data-transforms"><span class="calibre4">data transformations</span></a> 也应该从训练集中学习，并应用于预测数据以进行预测:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">preprocessing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">train_test_split</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.4</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">StandardScaler</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_transformed</span> <span class="calibre4">=</span> <span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train_transformed</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_transformed</span> <span class="calibre4">=</span> <span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test_transformed</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span>  
<span class="calibre4">0.9333...</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 可以更容易地组合估计器，在交叉验证下使用如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">make_pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">make_pipeline</span><span class="calibre4">(</span><span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">StandardScaler</span><span class="calibre4">(),</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">cv</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                
<span class="calibre4">array([ 0.97...,  0.93...,  0.95...])</span>
</pre>
</div>
</div>
<p class="calibre10">可以参阅 <a class="calibre3 pcalibre" href="pipeline.html#combining-estimators"><span class="calibre4">Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</span></a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-697">
<span id="calibre_link-698" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.1.1.1. cross_validate 函数和多度量评估</h3>
<p class="calibre2"><code class="docutils"><span class="calibre4">cross_validate</span></code> 函数与 <code class="docutils"><span class="calibre4">cross_val_score</span></code> 在下面的两个方面有些不同 -</p>
<ul class="calibre6">
<li class="toctree-l">它允许指定多个指标进行评估.</li>
<li class="toctree-l">除了测试得分之外，它还会返回一个包含训练得分，拟合次数， score-times （得分次数）的一个字典。 It returns a dict containing training scores, fit-times and score-times in
addition to the test score.</li>
</ul>
<p class="calibre10">对于单个度量评估，其中 scoring 参数是一个字符串，可以调用或 None ， keys 将是 - <code class="docutils"><span class="calibre4">['test_score',</span> <span class="calibre4">'fit_time',</span> <span class="calibre4">'score_time']</span></code></p>
<p class="calibre10">而对于多度量评估，返回值是一个带有以下的 keys 的字典 -
<code class="docutils"><span class="calibre4">['test_&lt;scorer1_name&gt;',</span> <span class="calibre4">'test_&lt;scorer2_name&gt;',</span> <span class="calibre4">'test_&lt;scorer...&gt;',</span> <span class="calibre4">'fit_time',</span> <span class="calibre4">'score_time']</span></code></p>
<p class="calibre10"><code class="docutils"><span class="calibre4">return_train_score</span></code> 默认设置为 <code class="docutils"><span class="calibre4">True</span></code> 。 它增加了所有 scorers(得分器) 的训练得分 keys 。如果不需要训练 scores ，则应将其明确设置为 <code class="docutils"><span class="calibre4">False</span></code> 。</p>
<p class="calibre10">可以将多个指标指定为 predefined scorer names（预定义的得分器的名称） list ，tuple 或者 set</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_validate</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">recall_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scoring</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'precision_macro'</span><span class="calibre4">,</span> <span class="calibre4">'recall_macro'</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_validate</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">scoring</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                        <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">return_train_score</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">sorted</span><span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">keys</span><span class="calibre4">())</span>
<span class="calibre4">['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">[</span><span class="calibre4">'test_recall_macro'</span><span class="calibre4">]</span>                       
<span class="calibre4">array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])</span>
</pre>
</div>
</div>
<p class="calibre10">或作为一个字典 mapping 得分器名称预定义或自定义的得分函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics.scorer</span> <span class="calibre4">import</span> <span class="calibre4">make_scorer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scoring</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'prec_macro'</span><span class="calibre4">:</span> <span class="calibre4">'precision_macro'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>           <span class="calibre4">'rec_micro'</span><span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">recall_score</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)}</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_validate</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">scoring</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                        <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">return_train_score</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">sorted</span><span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">keys</span><span class="calibre4">())</span>                 
<span class="calibre4">['fit_time', 'score_time', 'test_prec_macro', 'test_rec_micro',</span>
<span class="calibre4"> 'train_prec_macro', 'train_rec_micro']</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span><span class="calibre4">[</span><span class="calibre4">'train_rec_micro'</span><span class="calibre4">]</span>                         
<span class="calibre4">array([ 0.97...,  0.97...,  0.99...,  0.98...,  0.98...])</span>
</pre>
</div>
</div>
<p class="calibre10">这里是一个使用单一指标的 <code class="docutils"><span class="calibre4">cross_validate</span></code> 的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">cross_validate</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                        <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'precision_macro'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">sorted</span><span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">keys</span><span class="calibre4">())</span>
<span class="calibre4">['fit_time', 'score_time', 'test_score', 'train_score']</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-699">
<h3 class="sigil_not_in_toc1">3.1.1.2. 通过交叉验证获取预测</h3>
<p class="calibre2">除了返回结果不同，函数 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict" title="sklearn.model_selection.cross_val_predict"><code class="docutils"><span class="calibre4">cross_val_predict</span></code></a> 具有和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 相同的接口， 对于每一个输入的元素，如果其在测试集合中，将会得到预测结果。交叉验证策略会将可用的元素提交到测试集合有且仅有一次（否则会抛出一个异常）。</p>
<p class="calibre10">这些预测可以用于评价分类器的效果:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_predict</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">predicted</span> <span class="calibre4">=</span> <span class="calibre4">cross_val_predict</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">accuracy_score</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">predicted</span><span class="calibre4">)</span> 
<span class="calibre4">0.973...</span>
</pre>
</div>
</div>
<p class="calibre10">注意，这个计算的结果和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 有轻微的差别，因为后者用另一种方式组织元素。</p>
<p class="calibre10">可用的交叉验证迭代器在下面的部分中。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py"><span class="calibre4">Receiver Operating Characteristic (ROC) with cross validation</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"><span class="calibre4">Recursive feature elimination with cross-validation</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="calibre4">Parameter estimation using grid search with cross-validation</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="calibre4">Sample pipeline for text feature extraction and evaluation</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_cv_predict.html#sphx-glr-auto-examples-plot-cv-predict-py"><span class="calibre4">绘制交叉验证预测图</span></a>,</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py"><span class="calibre4">Nested versus non-nested cross-validation</span></a>.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-700">
<h2 class="sigil_not_in_toc">3.1.2. 交叉验证迭代器</h2>
<p class="calibre2">接下来的部分列出了一些用于生成索引标号，用于在不同的交叉验证策略中生成数据划分的工具。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-701">
<span id="calibre_link-702" class="calibre4"></span><h2 class="sigil_not_in_toc">3.1.3. 交叉验证迭代器&ndash;循环遍历数据</h2>
<p class="calibre2">假设一些数据是独立的和相同分布的 (i.i.d) 假定所有的样本来源于相同的生成过程，并假设生成过程没有记忆过去生成的样本。</p>
<p class="calibre10">在这种情况下可以使用下面的交叉验证器。</p>
<p class="calibre10"><strong class="calibre14">注意</strong></p>
<p class="calibre10">而 i.i.d 数据是机器学习理论中的一个常见假设，在实践中很少成立。如果知道样本是使用时间相关的过程生成的，则使用 <a class="calibre3 pcalibre" href="#calibre_link-56"><span class="calibre4">time-series aware cross-validation scheme</span></a> 更安全。
同样，如果我们知道生成过程具有 group structure （群体结构）（从不同 subjects（主体） ， experiments（实验）， measurement devices （测量设备）收集的样本），则使用 <a class="calibre3 pcalibre" href="#calibre_link-57"><span class="calibre4">group-wise cross-validation</span></a> 更安全。</p>
<div class="toctree-wrapper" id="calibre_link-703">
<h3 class="sigil_not_in_toc1">3.1.3.1. K 折</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 将所有的样例划分为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 个组，称为折叠 (fold) （如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000646.jpg" alt="k = n" />， 这等价于 <em class="calibre13">Leave One Out（留一）</em> 策略），都具有相同的大小（如果可能）。预测函数学习时使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000110.jpg" alt="k - 1" /> 个折叠中的数据，最后一个剩下的折叠会用于测试。</p>
<p class="calibre10">在 4 个样例的数据集上使用 2-fold 交叉验证的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">KFold</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">"a"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"d"</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">kf</span> <span class="calibre4">=</span> <span class="calibre4">KFold</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">kf</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[2 3] [0 1]</span>
<span class="calibre4">[0 1] [2 3]</span>
</pre>
</div>
</div>
<p class="calibre10">每个折叠由两个 arrays 组成，第一个作为 <em class="calibre13">training set</em> ，另一个作为 <em class="calibre13">test set</em> 。
由此，可以通过使用 numpy 的索引创建训练/测试集合:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2.</span><span class="calibre4">,</span> <span class="calibre4">2.</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">train</span><span class="calibre4">],</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">test</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">train</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">test</span><span class="calibre4">]</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-704">
<h3 class="sigil_not_in_toc1">3.1.3.2. 重复 K-折交叉验证</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold" title="sklearn.model_selection.RepeatedKFold"><code class="docutils"><span class="calibre4">RepeatedKFold</span></code></a> 重复 K-Fold n 次。当需要运行时可以使用它 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> n 次，在每次重复中产生不同的分割。</p>
<p class="calibre10">2折 K-Fold 重复 2 次的示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">RepeatedKFold</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">random_state</span> <span class="calibre4">=</span> <span class="calibre4">12883823</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rkf</span> <span class="calibre4">=</span> <span class="calibre4">RepeatedKFold</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">n_repeats</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">random_state</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">rkf</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">...</span>
<span class="calibre4">[2 3] [0 1]</span>
<span class="calibre4">[0 1] [2 3]</span>
<span class="calibre4">[0 2] [1 3]</span>
<span class="calibre4">[1 3] [0 2]</span>
</pre>
</div>
</div>
<p class="calibre10">类似地， <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold" title="sklearn.model_selection.RepeatedStratifiedKFold"><code class="docutils"><span class="calibre4">RepeatedStratifiedKFold</span></code></a> 在每个重复中以不同的随机化重复 n 次分层的 K-Fold 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-705">
<h3 class="sigil_not_in_toc1">3.1.3.3. 留一交叉验证 (LOO)</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code class="docutils"><span class="calibre4">LeaveOneOut</span></code></a> (或 LOO) 是一个简单的交叉验证。每个学习集都是通过除了一个样本以外的所有样本创建的，测试集是被留下的样本。
因此，对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个样本，我们有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个不同的训练集和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个不同的测试集。这种交叉验证程序不会浪费太多数据，因为只有一个样本是从训练集中删除掉的:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">LeaveOneOut</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">loo</span> <span class="calibre4">=</span> <span class="calibre4">LeaveOneOut</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">loo</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[1 2 3] [0]</span>
<span class="calibre4">[0 2 3] [1]</span>
<span class="calibre4">[0 1 3] [2]</span>
<span class="calibre4">[0 1 2] [3]</span>
</pre>
</div>
</div>
<p class="calibre10">LOO 潜在的用户选择模型应该权衡一些已知的警告。
当与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 折交叉验证进行比较时，可以从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 样本中构建 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 模型，而不是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 模型，其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000203.jpg" alt="n &gt; k" /> 。
此外，每个在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000504.jpg" alt="n - 1" /> 个样本而不是在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000185.jpg" alt="(k-1) n / k" /> 上进行训练。在两种方式中，假设 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 不是太大，并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000084.jpg" alt="k &lt; n" /> ， LOO 比 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 折交叉验证计算开销更加昂贵。</p>
<p class="calibre10">就精度而言， LOO 经常导致较高的方差作为测试误差的估计器。直观地说，因为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个样本中的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000504.jpg" alt="n - 1" /> 被用来构建每个模型，折叠构建的模型实际上是相同的，并且是从整个训练集建立的模型。</p>
<p class="calibre10">但是，如果学习曲线对于所讨论的训练大小是陡峭的，那么 5- 或 10- 折交叉验证可以泛化误差增高。</p>
<p class="calibre10">作为一般规则，大多数作者和经验证据表明， 5- 或者 10- 交叉验证应该优于 LOO 。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html">http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html</a>;</li>
<li class="toctree-l">T. Hastie, R. Tibshirani, J. Friedman,  <a class="calibre3 pcalibre" href="http://statweb.stanford.edu/~tibs/ElemStatLearn">The Elements of Statistical Learning</a>, Springer 2009</li>
<li class="toctree-l">L. Breiman, P. Spector <a class="calibre3 pcalibre" href="http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf">Submodel selection and evaluation in regression: The X-random case</a>, International Statistical Review 1992;</li>
<li class="toctree-l">R. Kohavi, <a class="calibre3 pcalibre" href="http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf">A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection</a>, Intl. Jnt. Conf. AI</li>
<li class="toctree-l">R. Bharat Rao, G. Fung, R. Rosales, <a class="calibre3 pcalibre" href="http://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf">On the Dangers of Cross-Validation. An Experimental Evaluation</a>, SIAM 2008;</li>
<li class="toctree-l">G. James, D. Witten, T. Hastie, R Tibshirani, <a class="calibre3 pcalibre" href="http://www-bcf.usc.edu/~gareth/ISL">An Introduction to
Statistical Learning</a>, Springer 2013.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-706">
<h3 class="sigil_not_in_toc1">3.1.3.4. 留 P 交叉验证 (LPO)</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut" title="sklearn.model_selection.LeavePOut"><code class="docutils"><span class="calibre4">LeavePOut</span></code></a> 与 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code class="docutils"><span class="calibre4">LeaveOneOut</span></code></a> 非常相似，因为它通过从整个集合中删除 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" /> 个样本来创建所有可能的 训练/测试集。对于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 个样本，这产生了 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000889.jpg" alt="{n \choose p}" /> 个 训练-测试 对。与 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code class="docutils"><span class="calibre4">LeaveOneOut</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 不同，当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000500.jpg" alt="p &gt; 1" /> 时，测试集会重叠。</p>
<p class="calibre10">在有 4 个样例的数据集上使用 Leave-2-Out 的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">LeavePOut</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">(</span><span class="calibre4">4</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lpo</span> <span class="calibre4">=</span> <span class="calibre4">LeavePOut</span><span class="calibre4">(</span><span class="calibre4">p</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">lpo</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[2 3] [0 1]</span>
<span class="calibre4">[1 3] [0 2]</span>
<span class="calibre4">[1 2] [0 3]</span>
<span class="calibre4">[0 3] [1 2]</span>
<span class="calibre4">[0 2] [1 3]</span>
<span class="calibre4">[0 1] [2 3]</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-707">
<span id="calibre_link-708" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.1.3.5. 随机排列交叉验证 a.k.a. Shuffle &amp; Split</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a></p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a> 迭代器 将会生成一个用户给定数量的独立的训练/测试数据划分。样例首先被打散然后划分为一对训练测试集合。</p>
<p class="calibre10">可以通过设定明确的 <code class="docutils"><span class="calibre4">random_state</span></code> ，使得伪随机生成器的结果可以重复。</p>
<p class="calibre10">这是一个使用的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">ShuffleSplit</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ss</span> <span class="calibre4">=</span> <span class="calibre4">ShuffleSplit</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.25</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train_index</span><span class="calibre4">,</span> <span class="calibre4">test_index</span> <span class="calibre4">in</span> <span class="calibre4">ss</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train_index</span><span class="calibre4">,</span> <span class="calibre4">test_index</span><span class="calibre4">))</span>
<span class="calibre4">...</span>
<span class="calibre4">[1 3 4] [2 0]</span>
<span class="calibre4">[1 4 3] [0 2]</span>
<span class="calibre4">[4 0 2] [1 3]</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a> 可以替代 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 交叉验证，因为其提供了细致的训练 / 测试划分的 数量和样例所占的比例等的控制。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-709">
<h2 class="sigil_not_in_toc">3.1.4. 基于类标签、具有分层的交叉验证迭代器</h2>
<p class="calibre2">一些分类问题在目标类别的分布上可能表现出很大的不平衡性：例如，可能会出现比正样本多数倍的负样本。在这种情况下，建议采用如 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="docutils"><span class="calibre4">StratifiedKFold</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit" title="sklearn.model_selection.StratifiedShuffleSplit"><code class="docutils"><span class="calibre4">StratifiedShuffleSplit</span></code></a> 中实现的分层抽样方法，确保相对的类别频率在每个训练和验证 折叠 中大致保留。</p>
<div class="toctree-wrapper" id="calibre_link-710">
<h3 class="sigil_not_in_toc1">3.1.4.1. 分层 k 折</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="docutils"><span class="calibre4">StratifiedKFold</span></code></a> 是 <em class="calibre13">k-fold</em> 的变种，会返回 <em class="calibre13">stratified（分层）</em> 的折叠：每个小集合中， 各个类别的样例比例大致和完整数据集中相同。</p>
<p class="calibre10">在有 10 个样例的，有两个略不均衡类别的数据集上进行分层 3-fold 交叉验证的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">StratifiedKFold</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">(</span><span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">skf</span> <span class="calibre4">=</span> <span class="calibre4">StratifiedKFold</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">skf</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[2 3 6 7 8 9] [0 1 4 5]</span>
<span class="calibre4">[0 1 3 4 5 8 9] [2 6 7]</span>
<span class="calibre4">[0 1 2 4 5 6 7] [3 8 9]</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold" title="sklearn.model_selection.RepeatedStratifiedKFold"><code class="docutils"><span class="calibre4">RepeatedStratifiedKFold</span></code></a> 可用于在每次重复中用不同的随机化重复分层 K-Fold n 次。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-711">
<h3 class="sigil_not_in_toc1">3.1.4.2. 分层随机 Split</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit" title="sklearn.model_selection.StratifiedShuffleSplit"><code class="docutils"><span class="calibre4">StratifiedShuffleSplit</span></code></a> 是 <em class="calibre13">ShuffleSplit</em> 的一个变种，会返回直接的划分，比如： 创建一个划分，但是划分中每个类的比例和完整数据集中的相同。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-57">
<span id="calibre_link-712" class="calibre4"></span><h2 class="sigil_not_in_toc">3.1.5. 用于分组数据的交叉验证迭代器</h2>
<p class="calibre2">如果潜在的生成过程产生依赖样本的 groups ，那么 i.i.d. 假设将会被打破。</p>
<p class="calibre10">这样的数据分组是特定于域的。一个例子是从多个患者收集医学数据，从每个患者身上采集多个样本。而这样的数据很可能取决于个人群体。
在我们的例子中，每个样本的患者 ID 将是其 group identifier （组标识符）。</p>
<p class="calibre10">在这种情况下，我们想知道在一组特定的 groups 上训练的模型是否能很好地适用于看不见的 group 。为了衡量这一点，我们需要确保验证对象中的所有样本来自配对训练折叠中完全没有表示的组。</p>
<p class="calibre10">下面的交叉验证分离器可以用来做到这一点。
样本的 grouping identifier （分组标识符） 通过 <code class="docutils"><span class="calibre4">groups</span></code> 参数指定。</p>
<div class="toctree-wrapper" id="calibre_link-713">
<h3 class="sigil_not_in_toc1">3.1.5.1. 组 k-fold</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold" title="sklearn.model_selection.GroupKFold"><code class="docutils"><span class="calibre4">GroupKFold</span></code></a> 是 k-fold 的变体，它确保同一个 group 在测试和训练集中都不被表示。
例如，如果数据是从不同的 subjects 获得的，每个 subject 有多个样本，并且如果模型足够灵活以高度人物指定的特征中学习，则可能无法推广到新的 subject 。 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold" title="sklearn.model_selection.GroupKFold"><code class="docutils"><span class="calibre4">GroupKFold</span></code></a> 可以检测到这种过拟合的情况。
Imagine you have three subjects, each with an associated number from 1 to 3:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GroupKFold</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">2.2</span><span class="calibre4">,</span> <span class="calibre4">2.4</span><span class="calibre4">,</span> <span class="calibre4">2.3</span><span class="calibre4">,</span> <span class="calibre4">4.55</span><span class="calibre4">,</span> <span class="calibre4">5.8</span><span class="calibre4">,</span> <span class="calibre4">8.8</span><span class="calibre4">,</span> <span class="calibre4">9</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">"a"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"d"</span><span class="calibre4">,</span> <span class="calibre4">"d"</span><span class="calibre4">,</span> <span class="calibre4">"d"</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">groups</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gkf</span> <span class="calibre4">=</span> <span class="calibre4">GroupKFold</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">gkf</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">groups</span><span class="calibre4">=</span><span class="calibre4">groups</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[0 1 2 3 4 5] [6 7 8 9]</span>
<span class="calibre4">[0 1 2 6 7 8 9] [3 4 5]</span>
<span class="calibre4">[3 4 5 6 7 8 9] [0 1 2]</span>
</pre>
</div>
</div>
<p class="calibre10">每个 subject 都处于不同的测试阶段，同一个科目从来没有在测试和训练过程中。请注意，由于数据不平衡，折叠的大小并不完全相同。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-714">
<h3 class="sigil_not_in_toc1">3.1.5.2. 留一组交叉验证</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code class="docutils"><span class="calibre4">LeaveOneGroupOut</span></code></a> 是一个交叉验证方案，它根据第三方提供的 array of integer groups （整数组的数组）来提供样本。这个组信息可以用来编码任意域特定的预定义交叉验证折叠。</p>
<p class="calibre10">每个训练集都是由除特定组别以外的所有样本构成的。</p>
<p class="calibre10">例如，在多个实验的情况下， <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code class="docutils"><span class="calibre4">LeaveOneGroupOut</span></code></a> 可以用来根据不同的实验创建一个交叉验证：我们使用除去一个实验的所有实验的样本创建一个训练集:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">LeaveOneGroupOut</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">60</span><span class="calibre4">,</span> <span class="calibre4">70</span><span class="calibre4">,</span> <span class="calibre4">80</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">groups</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">logo</span> <span class="calibre4">=</span> <span class="calibre4">LeaveOneGroupOut</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">logo</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">groups</span><span class="calibre4">=</span><span class="calibre4">groups</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[2 3 4 5 6] [0 1]</span>
<span class="calibre4">[0 1 4 5 6] [2 3]</span>
<span class="calibre4">[0 1 2 3] [4 5 6]</span>
</pre>
</div>
</div>
<p class="calibre10">另一个常见的应用是使用时间信息：例如，组可以是收集样本的年份，从而允许与基于时间的分割进行交叉验证。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-715">
<h3 class="sigil_not_in_toc1">3.1.5.3. 留 P 组交叉验证</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code class="docutils"><span class="calibre4">LeavePGroupsOut</span></code></a> 类似于 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code class="docutils"><span class="calibre4">LeaveOneGroupOut</span></code></a> ，但为每个训练/测试集删除与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000138.jpg" alt="P" /> 组有关的样本。</p>
<p class="calibre10">Leave-2-Group Out 的示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">LeavePGroupsOut</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">6</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">groups</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lpgo</span> <span class="calibre4">=</span> <span class="calibre4">LeavePGroupsOut</span><span class="calibre4">(</span><span class="calibre4">n_groups</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">lpgo</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">groups</span><span class="calibre4">=</span><span class="calibre4">groups</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[4 5] [0 1 2 3]</span>
<span class="calibre4">[2 3] [0 1 4 5]</span>
<span class="calibre4">[0 1] [2 3 4 5]</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-716">
<h3 class="sigil_not_in_toc1">3.1.5.4. Group Shuffle Split</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit" title="sklearn.model_selection.GroupShuffleSplit"><code class="docutils"><span class="calibre4">GroupShuffleSplit</span></code></a> 迭代器是 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code class="docutils"><span class="calibre4">LeavePGroupsOut</span></code></a> 的组合，它生成一个随机划分分区的序列，其中为每个分组提供了一个组子集。</p>
<p class="calibre10">这是使用的示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GroupShuffleSplit</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">2.2</span><span class="calibre4">,</span> <span class="calibre4">2.4</span><span class="calibre4">,</span> <span class="calibre4">2.3</span><span class="calibre4">,</span> <span class="calibre4">4.55</span><span class="calibre4">,</span> <span class="calibre4">5.8</span><span class="calibre4">,</span> <span class="calibre4">0.001</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">"a"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"a"</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">groups</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gss</span> <span class="calibre4">=</span> <span class="calibre4">GroupShuffleSplit</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">gss</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">groups</span><span class="calibre4">=</span><span class="calibre4">groups</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">...</span>
<span class="calibre4">[0 1 2 3] [4 5 6 7]</span>
<span class="calibre4">[2 3 6 7] [0 1 4 5]</span>
<span class="calibre4">[2 3 4 5] [0 1 6 7]</span>
<span class="calibre4">[4 5 6 7] [0 1 2 3]</span>
</pre>
</div>
</div>
<p class="calibre10">当需要 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code class="docutils"><span class="calibre4">LeavePGroupsOut</span></code></a> 的操作时，这个类的信息是很有必要的，但是 组 的数目足够大，以至于用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000138.jpg" alt="P" /> 组生成所有可能的分区将会花费很大的代价。在这种情况下， <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit" title="sklearn.model_selection.GroupShuffleSplit"><code class="docutils"><span class="calibre4">GroupShuffleSplit</span></code></a> 通过 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code class="docutils"><span class="calibre4">LeavePGroupsOut</span></code></a> 提供了一个随机（可重复）的训练 / 测试划分采样。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-717">
<h2 class="sigil_not_in_toc">3.1.6. 预定义的折叠 / 验证集</h2>
<p class="calibre2">对一些数据集，一个预定义的，将数据划分为训练和验证集合或者划分为几个交叉验证集合的划分已经存在。 可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit" title="sklearn.model_selection.PredefinedSplit"><code class="docutils"><span class="calibre4">PredefinedSplit</span></code></a> 来使用这些集合来搜索超参数。</p>
<p class="calibre10">比如，当使用验证集合时，设置所有验证集合中的样例的 <code class="docutils"><span class="calibre4">test_fold</span></code> 为 0，而将其他样例设置为 -1 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-56">
<span id="calibre_link-718" class="calibre4"></span><h2 class="sigil_not_in_toc">3.1.7. 交叉验证在时间序列数据中应用</h2>
<p class="calibre2">时间序列数据的特点是时间 (<em class="calibre13">autocorrelation(自相关性)</em>) 附近的观测之间的相关性。
然而，传统的交叉验证技术，例如 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a> 假设样本是独立的且分布相同的，并且在时间序列数据上会导致训练和测试实例之间不合理的相关性（产生广义误差的估计较差）。
因此，对 “future(未来)” 观测的时间序列数据模型的评估至少与用于训练模型的观测模型非常重要。为了达到这个目的，一个解决方案是由 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit" title="sklearn.model_selection.TimeSeriesSplit"><code class="docutils"><span class="calibre4">TimeSeriesSplit</span></code></a> 提供的。</p>
<div class="toctree-wrapper" id="calibre_link-719">
<h3 class="sigil_not_in_toc1">3.1.7.1. 时间序列分割</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit" title="sklearn.model_selection.TimeSeriesSplit"><code class="docutils"><span class="calibre4">TimeSeriesSplit</span></code></a> 是 <em class="calibre13">k-fold</em> 的一个变体，它首先返回 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 折作为训练数据集，并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000343.jpg" alt="(k+1)" /> 折作为测试数据集。
请注意，与标准的交叉验证方法不同，连续的训练集是超越前者的超集。
另外，它将所有的剩余数据添加到第一个训练分区，它总是用来训练模型。</p>
<p class="calibre10">这个类可以用来交叉验证以固定时间间隔观察到的时间序列数据样本。</p>
<p class="calibre10">对具有 6 个样本的数据集进行 3-split 时间序列交叉验证的示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">TimeSeriesSplit</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tscv</span> <span class="calibre4">=</span> <span class="calibre4">TimeSeriesSplit</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">tscv</span><span class="calibre4">)</span>  
<span class="calibre4">TimeSeriesSplit(max_train_size=None, n_splits=3)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">tscv</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4"> </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span><span class="calibre4">))</span>
<span class="calibre4">[0 1 2] [3]</span>
<span class="calibre4">[0 1 2 3] [4]</span>
<span class="calibre4">[0 1 2 3 4] [5]</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-720">
<h2 class="sigil_not_in_toc">3.1.8. A note on shuffling</h2>
<p class="calibre2">(如果数据的顺序不是任意的（比如说，相同标签的样例连续出现），为了获得有意义的交叉验证结果，首先对其进行 打散是很有必要的。然而，当样例不是独立同分布时打散则是不可行的。例如：样例是相关的文章，以他们发表的时间 进行排序，这时候如果对数据进行打散，将会导致模型过拟合，得到一个过高的验证分数：因为验证样例更加相似（在时间上更接近） 于训练数据。</p>
<p class="calibre10">一些交叉验证迭代器， 比如 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> ，有一个内建的在划分数据前进行数据索引打散的选项。注意:</p>
<ul class="calibre6">
<li class="toctree-l">这种方式仅需要很少的内存就可以打散数据。</li>
<li class="toctree-l">默认不会进行打散，包括设置 <code class="docutils"><span class="calibre4">cv=some_integer</span></code> （直接）k 折叠交叉验证的 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> ， 表格搜索等。注意 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" title="sklearn.model_selection.train_test_split"><code class="docutils"><span class="calibre4">train_test_split</span></code></a> 会返回一个随机的划分。</li>
<li class="toctree-l">参数 <code class="docutils"><span class="calibre4">random_state</span></code> 默认设置为 <code class="docutils"><span class="calibre4">None</span></code> ，这意为着每次进行 <code class="docutils"><span class="calibre4">KFold(...,</span> <span class="calibre4">shuffle=True)</span></code>  时，打散都是不同的。 然而， <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 通过调用 <code class="docutils"><span class="calibre4">fit</span></code> 方法验证时，将会使用相同的打散来训练每一组参数。</li>
<li class="toctree-l">为了保证结果的可重复性（在相同的平台上），应该给 <code class="docutils"><span class="calibre4">random_state</span></code> 设定一个固定的值。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-721">
<h2 class="sigil_not_in_toc">3.1.9. 交叉验证和模型选择</h2>
<p class="calibre2">交叉验证迭代器可以通过网格搜索得到最优的模型超参数，从而直接用于模型的选择。 这是另一部分 <a class="calibre3 pcalibre" href="grid_search.html#grid-search"><span class="calibre4">调整估计器的超参数</span></a> 的主要内容。</p>
</div>
</div>


<div class="calibre" id="calibre_link-58">
<span id="calibre_link-722" class="calibre4"></span><h1 class="calibre5">3.2. 调整估计器的超参数</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@想和太阳肩并肩</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@\S^R^Y/</a><br class="calibre9" />
    </div>
<p class="calibre10">超参数，即不直接在估计器内学习的参数。在 scikit-learn 包中，它们作为估计器类中构造函数的参数进行传递。典型的例子有：用于支持向量分类器的 <code class="docutils"><span class="calibre4">C</span></code> 、<code class="docutils"><span class="calibre4">kernel</span></code> 和 <code class="docutils"><span class="calibre4">gamma</span></code> ，用于Lasso的 <code class="docutils"><span class="calibre4">alpha</span></code> 等。</p>
<p class="calibre10">搜索超参数空间以便获得最好 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/cross_validation.html#cross-validation">交叉验证</a> 分数的方法是可能的而且是值得提倡的。</p>
<p class="calibre10">通过这种方式，构造估计器时被提供的任何参数或许都能被优化。具体来说，要获取到给定估计器的所有参数的名称和当前值，使用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">get_params</span><span class="calibre4">()</span>
</pre>
</div>
</div>
<p class="calibre10">搜索包括:</p>
<ul class="calibre6">
<li class="toctree-l">估计器(回归器或分类器，例如 <code class="docutils"><span class="calibre4">sklearn.svm.SVC()</span></code>)</li>
<li class="toctree-l">参数空间</li>
<li class="toctree-l">搜寻或采样候选的方法</li>
<li class="toctree-l">交叉验证方案</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="#calibre_link-59"><span class="calibre4">计分函数</span></a></li>
</ul>
<p class="calibre10">有些模型支持专业化的、高效的参数搜索策略, <a class="calibre3 pcalibre" href="#calibre_link-60"><span class="calibre4">描述如下</span></a> 。在 scikit-learn 包中提供了两种采样搜索候选的通用方法:对于给定的值, <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV">GridSearchCV</a> 考虑了所有参数组合；而 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a> 可以从具有指定分布的参数空间中抽取给定数量的候选。介绍完这些工具后，我们将详细介绍适用于这两种方法的 <a class="calibre3 pcalibre" href="#calibre_link-61"><span class="calibre4">最佳实践</span></a> 。</p>
<p class="calibre10"><strong class="calibre14">注意</strong>，通常这些参数的一小部分会对模型的预测或计算性能有很大的影响，而其他参数可以保留为其默认值。
建议阅读估计器类的相关文档，以更好地了解其预期行为，可能的话还可以阅读下引用的文献。</p>
<div class="toctree-wrapper" id="calibre_link-723">
<h2 class="sigil_not_in_toc">3.2.1. 网格追踪法&ndash;穷尽的网格搜索</h2>
<p class="calibre2"><code class="docutils"><span class="calibre4">GridSearchCV</span></code> 提供的网格搜索从通过 <code class="docutils"><span class="calibre4">param_grid</span></code> 参数确定的网格参数值中全面生成候选。例如，下面的 <code class="docutils"><span class="calibre4">param_grid</span></code>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">param_grid</span> <span class="calibre4">=</span> <span class="calibre4">[</span>
  <span class="calibre4">{</span><span class="calibre4">'C'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">1000</span><span class="calibre4">],</span> <span class="calibre4">'kernel'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">'linear'</span><span class="calibre4">]},</span>
  <span class="calibre4">{</span><span class="calibre4">'C'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">1000</span><span class="calibre4">],</span> <span class="calibre4">'gamma'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">0.001</span><span class="calibre4">,</span> <span class="calibre4">0.0001</span><span class="calibre4">],</span> <span class="calibre4">'kernel'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">'rbf'</span><span class="calibre4">]},</span>
 <span class="calibre4">]</span>
</pre>
</div>
</div>
<p class="calibre10">探索两个网格的详细解释：
一个具有线性内核并且C在[1,10,100,1000]中取值；
另一个具有RBF内核，C值的交叉乘积范围在[1,10，100,1000]，gamma在[0.001，0.0001]中取值。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">GridSearchCV</span></code> 实例实现了常用估计器 API：当在数据集上“拟合”时，参数值的所有可能的组合都会被评估，从而计算出最佳的组合。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">有关在数字数据集上的网格搜索计算示例，请参阅  <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py">基于交叉验证的网格搜索参数估计</a>。</li>
<li class="toctree-l">有关来自文本文档特征提取器（n-gram计数向量化器和TF-IDF变换器）的网格搜索耦合参数与分类器（这里是使用具有弹性网格的SGD训练的线性SVM 或L2惩罚）使用 <cite class="calibre13">pipeline.Pipeline</cite> 示例,请参阅  <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py">用于文本特征提取和评估的示例管道</a>。</li>
<li class="toctree-l">有关iris数据集的交叉验证循环中的网格搜索示例, 请参阅  <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py">嵌套与非嵌套交叉验证</a>。</li>
<li class="toctree-l">有关用于同时评估多个指标的GridSearchCV示例，请参阅  <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py">cross_val_score 与 GridSearchCV 多指标评价的实证研究</a>。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-724">
<span id="calibre_link-725" class="calibre4"></span><h2 class="sigil_not_in_toc">3.2.2. 随机参数优化</h2>
<p class="calibre2">尽管使用参数设置的网格法是目前最广泛使用的参数优化方法, 其他搜索方法也具有更有利的性能。 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">RandomizedSearchCV</a> 实现了对参数的随机搜索, 其中每个设置都是从可能的参数值的分布中进行取样。
这对于穷举搜索有两个主要优势:</p>
<ul class="calibre6">
<li class="toctree-l">可以选择独立于参数个数和可能值的预算</li>
<li class="toctree-l">添加不影响性能的参数不会降低效率</li>
</ul>
<p class="calibre10">指定如何取样的参数是使用字典完成的, 非常类似于为 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 指定参数。
此外, 通过 <code class="docutils"><span class="calibre4">n_iter</span></code> 参数指定计算预算, 即取样候选项数或取样迭代次数。
对于每个参数, 可以指定在可能值上的分布或离散选择的列表 (均匀取样):</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">{</span><span class="calibre4">'C'</span><span class="calibre4">:</span> <span class="calibre4">scipy</span><span class="calibre4">.</span><span class="calibre4">stats</span><span class="calibre4">.</span><span class="calibre4">expon</span><span class="calibre4">(</span><span class="calibre4">scale</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">),</span> <span class="calibre4">'gamma'</span><span class="calibre4">:</span> <span class="calibre4">scipy</span><span class="calibre4">.</span><span class="calibre4">stats</span><span class="calibre4">.</span><span class="calibre4">expon</span><span class="calibre4">(</span><span class="calibre4">scale</span><span class="calibre4">=.</span><span class="calibre4">1</span><span class="calibre4">),</span>
  <span class="calibre4">'kernel'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">'rbf'</span><span class="calibre4">],</span> <span class="calibre4">'class_weight'</span><span class="calibre4">:[</span><span class="calibre4">'balanced'</span><span class="calibre4">,</span> <span class="calibre4">None</span><span class="calibre4">]}</span>
</pre>
</div>
</div>
<p class="calibre10">本示例使用 <code class="docutils"><span class="calibre4">scipy.stats</span></code> 模块, 它包含许多用于采样参数的有用分布, 如 <code class="docutils"><span class="calibre4">expon</span></code>，<code class="docutils"><span class="calibre4">gamma</span></code>，<code class="docutils"><span class="calibre4">uniform</span></code> 或者 <code class="docutils"><span class="calibre4">randint</span></code>。
原则上, 任何函数都可以通过提供一个 <code class="docutils"><span class="calibre4">rvs</span></code> （随机变量样本）方法来采样一个值。
对 <code class="docutils"><span class="calibre4">rvs</span></code> 函数的调用应在连续调用中提供来自可能参数值的独立随机样本。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">The distributions in <code class="docutils"><span class="calibre4">scipy.stats</span></code> prior to version scipy 0.16
do not allow specifying a random state. Instead, they use the global
numpy random state, that can be seeded via <code class="docutils"><span class="calibre4">np.random.seed</span></code> or set
using <code class="docutils"><span class="calibre4">np.random.set_state</span></code>. However, beginning scikit-learn 0.18,
the <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/classes.html#module-sklearn.model_selection">sklearn.model_selection</a> module sets the random state provided
by the user if scipy &gt;= 0.16 is also available.</p>
</div>
</div>
</blockquote>
<p class="calibre10">对于连续参数 (如上面提到的 <code class="docutils"><span class="calibre4">C</span></code> )，指定连续分布以充分利用随机化是很重要的。这样，有助于 <code class="docutils"><span class="calibre4">n_iter</span></code> 总是趋向于更精细的搜索。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">随机搜索和网格搜索的使用和效率的比较： <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py">有关随机搜索和网格搜索超参数估计的对比</a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">引用:</p>
<ul class="calibre6">
<li class="toctree-l">Bergstra, J. and Bengio, Y.,
Random search for hyper-parameter optimization,
The Journal of Machine Learning Research (2012)</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-61">
<span id="calibre_link-726" class="calibre4"></span><h2 class="sigil_not_in_toc">3.2.3. 参数搜索技巧</h2>
<div class="toctree-wrapper" id="calibre_link-59">
<span id="calibre_link-727" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.2.3.1. 指定目标度量</h3>
<p class="calibre2">默认情况下, 参数搜索使用估计器的评分函数来评估（衡量）参数设置。
比如 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score">sklearn.metrics.accuracy_score</a> 用于分类和 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score">sklearn.metrics.r2_score</a> 用于回归。
对于一些应用, 其他评分函数将会更加适合 (例如在不平衡的分类, 精度评分往往是信息不足的)。
一个可选的评分功能可以通过评分参数指定给 <code class="docutils"><span class="calibre4">GridSearchCV</span></code>， <code class="docutils"><span class="calibre4">RandomizedSearchCV</span></code> 和许多下文将要描述的、专业化的交叉验证工具。
有关详细信息, 请参阅 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/model_evaluation.html#scoring-parameter">评分参数:定义模型评估规则</a>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-728">
<span id="calibre_link-729" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.2.3.2. 为评估指定多个指标</h3>
<p class="calibre2"><code class="docutils"><span class="calibre4">GridSearchCV</span></code> 和 <code class="docutils"><span class="calibre4">RandomizedSearchCV</span></code> 允许为评分参数指定多个指标。</p>
<p class="calibre10">多指标评分可以被指定为一个预先定义分数名称字符串列表或者是一个得分手名字到得分手的函数或预先定义的记分员名字的映射字典。
有关详细信息, 请参阅 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/model_evaluation.html#multimetric-scoring">多指标评估</a>。</p>
<p class="calibre10">在指定多个指标时,必须将 <code class="docutils"><span class="calibre4">refit</span></code> 参数设置为要在其中找到 <code class="docutils"><span class="calibre4">best_params_</span></code>,并用于在整个数据集上构建 <code class="docutils"><span class="calibre4">best_estimator_</span></code> 的度量标准（字符串）。
如果搜索不应该 refit, 则设置 <code class="docutils"><span class="calibre4">refit=False</span></code>。在使用多个度量值时,如果将 refit 保留为默认值,不会导致结果错误。</p>
<p class="calibre10">有关示例用法, 请参见 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py">cross_val_score 与 GridSearchCV 多指标评价的实证研究</a>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-730">
<h3 class="sigil_not_in_toc1">3.2.3.3. 复合估计和参数空间</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/pipeline.html#pipeline">管道：链式评估器</a> 描述了如何使用这些工具搜索参数空间构建链式评估器。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-731">
<h3 class="sigil_not_in_toc1">3.2.3.4. 模型选择：开发和评估</h3>
<p class="calibre2">通过评估各种参数设置，可以将模型选择视为使用标记数据训练网格参数的一种方法。</p>
<p class="calibre10">在评估结果模型时, 重要的是在网格搜索过程中未看到的 held-out 样本数据上执行以下操作:
建议将数据拆分为开发集 (<strong class="calibre14">development set</strong>,供 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 实例使用)和评估集(<strong class="calibre14">evaluation set</strong>)来计算性能指标。</p>
<p class="calibre10">这可以通过使用效用函数 <a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/doc/cn/0.19.0/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split">train_test_split</a> 来完成。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-732">
<h3 class="sigil_not_in_toc1">3.2.3.5. 并行机制</h3>
<p class="calibre2"><code class="docutils"><span class="calibre4">GridSearchCV</span></code> 和 <code class="docutils"><span class="calibre4">RandomizedSearchCV</span></code> 可以独立地评估每个参数设置。如果您的OS支持,通过使用关键字 <code class="docutils"><span class="calibre4">n_jobs=-1</span></code> 可以使计算并行运行。
有关详细信息, 请参见函数签名。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-733">
<h3 class="sigil_not_in_toc1">3.2.3.6. 对故障的鲁棒性</h3>
<p class="calibre2">某些参数设置可能导致无法 <code class="docutils"><span class="calibre4">fit</span></code> 数据的一个或多个折叠。
默认情况下, 这将导致整个搜索失败, 即使某些参数设置可以完全计算。
设置 <code class="docutils"><span class="calibre4">error_score=0</span></code> (或`=np.NaN`) 将使程序对此类故障具有鲁棒性,发出警告并将该折叠的分数设置为0(或`NaN`), 但可以完成搜索。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-60">
<span id="calibre_link-734" class="calibre4"></span><h2 class="sigil_not_in_toc">3.2.4. 暴力参数搜索的替代方案</h2>
<div class="toctree-wrapper" id="calibre_link-735">
<h3 class="sigil_not_in_toc1">3.2.4.1. 模型特定交叉验证</h3>
<p class="calibre2">某些模型可以与参数的单个值的估计值一样有效地适应某一参数范围内的数据。
此功能可用于执行更有效的交叉验证, 用于此参数的模型选择。</p>
<p class="calibre10">该策略最常用的参数是编码正则化矩阵强度的参数。在这种情况下, 我们称之为, 计算估计器的正则化路径(<strong class="calibre14">regularization path</strong>)。</p>
<p class="calibre10">以下是这些模型的列表:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="docutils"><span class="calibre4">linear_model.ElasticNetCV</span></code></a>([l1_ratio,&nbsp;eps,&nbsp;…])</td>
<td class="label1">Elastic Net model with iterative fitting along a regularization path</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LarsCV.html#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="docutils"><span class="calibre4">linear_model.LarsCV</span></code></a>([fit_intercept,&nbsp;…])</td>
<td class="label1">Cross-validated Least Angle Regression model</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="docutils"><span class="calibre4">linear_model.LassoCV</span></code></a>([eps,&nbsp;n_alphas,&nbsp;…])</td>
<td class="label1">Lasso linear model with iterative fitting along a regularization path</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="docutils"><span class="calibre4">linear_model.LassoLarsCV</span></code></a>([fit_intercept,&nbsp;…])</td>
<td class="label1">Cross-validated Lasso, using the LARS algorithm</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="docutils"><span class="calibre4">linear_model.LogisticRegressionCV</span></code></a>([Cs,&nbsp;…])</td>
<td class="label1">Logistic Regression CV (aka logit, MaxEnt) classifier.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="docutils"><span class="calibre4">linear_model.MultiTaskElasticNetCV</span></code></a>([…])</td>
<td class="label1">Multi-task L1/L2 ElasticNet with built-in cross-validation.</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.MultiTaskLassoCV.html#sklearn.linear_model.MultiTaskLassoCV" title="sklearn.linear_model.MultiTaskLassoCV"><code class="docutils"><span class="calibre4">linear_model.MultiTaskLassoCV</span></code></a>([eps,&nbsp;…])</td>
<td class="label1">Multi-task L1/L2 Lasso with built-in cross-validation.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.OrthogonalMatchingPursuitCV.html#sklearn.linear_model.OrthogonalMatchingPursuitCV" title="sklearn.linear_model.OrthogonalMatchingPursuitCV"><code class="docutils"><span class="calibre4">linear_model.OrthogonalMatchingPursuitCV</span></code></a>([…])</td>
<td class="label1">Cross-validated Orthogonal Matching Pursuit model (OMP)</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="docutils"><span class="calibre4">linear_model.RidgeCV</span></code></a>([alphas,&nbsp;…])</td>
<td class="label1">Ridge regression with built-in cross-validation.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV" title="sklearn.linear_model.RidgeClassifierCV"><code class="docutils"><span class="calibre4">linear_model.RidgeClassifierCV</span></code></a>([alphas,&nbsp;…])</td>
<td class="label1">Ridge classifier with built-in cross-validation.</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-736">
<h3 class="sigil_not_in_toc1">3.2.4.2. 信息标准</h3>
<p class="calibre2">一些模型通过计算一个正则化路径 (代替使用交叉验证得出数个参数), 可以给出正则化参数最优估计的信息理论闭包公式。</p>
<p class="calibre10">以下是从 Akaike 信息标准 (AIC) 或贝叶斯信息标准 (可用于自动选择模型) 中受益的模型列表:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code class="docutils"><span class="calibre4">linear_model.LassoLarsIC</span></code></a>([criterion,&nbsp;…])</td>
<td class="label1">Lasso model fit with Lars using BIC or AIC for model selection</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-737">
<span id="calibre_link-738" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.2.4.3. 出袋估计</h3>
<p class="calibre2">当使用基于装袋的集合方法时，即使用具有替换的采样产生新的训练集，部分训练集保持不用。 对于集合中的每个分类器，训练集的不同部分被忽略。</p>
<p class="calibre10">这个省略的部分可以用来估计泛化误差，而不必依靠单独的验证集。 此估计是”免费的”，因为不需要额外的数据，可以用于模型选择。</p>
<p class="calibre10">目前该方法已经实现的类以下几个:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="docutils"><span class="calibre4">ensemble.RandomForestClassifier</span></code></a>([…])</td>
<td class="label1">A random forest classifier.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" title="sklearn.ensemble.RandomForestRegressor"><code class="docutils"><span class="calibre4">ensemble.RandomForestRegressor</span></code></a>([…])</td>
<td class="label1">A random forest regressor.</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="docutils"><span class="calibre4">ensemble.ExtraTreesClassifier</span></code></a>([…])</td>
<td class="label1">An extra-trees classifier.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="docutils"><span class="calibre4">ensemble.ExtraTreesRegressor</span></code></a>([n_estimators,&nbsp;…])</td>
<td class="label1">An extra-trees regressor.</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="docutils"><span class="calibre4">ensemble.GradientBoostingClassifier</span></code></a>([loss,&nbsp;…])</td>
<td class="label1">Gradient Boosting for classification.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="docutils"><span class="calibre4">ensemble.GradientBoostingRegressor</span></code></a>([loss,&nbsp;…])</td>
<td class="label1">Gradient Boosting for regression.</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-167">
<span id="calibre_link-739" class="calibre4"></span><h1 class="calibre5">3.3. 模型评估: 量化预测的质量</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@飓风</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@FAME</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@v</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" />     
    </div>
<p class="calibre10">有 3 种不同的 API 用于评估模型预测的质量:</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">Estimator score method（估计器得分的方法）</strong>: Estimators（估计器）有一个 <code class="docutils"><span class="calibre4">score（得分）</span></code> 方法，为其解决的问题提供了默认的 evaluation criterion （评估标准）。
在这个页面上没有相关讨论，但是在每个 estimator （估计器）的文档中会有相关的讨论。</li>
<li class="toctree-l"><strong class="calibre14">Scoring parameter（评分参数）</strong>: Model-evaluation tools （模型评估工具）使用 <a class="calibre3 pcalibre" href="cross_validation.html#cross-validation"><span class="calibre4">cross-validation</span></a> (如 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">model_selection.cross_val_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="docutils"><span class="calibre4">model_selection.GridSearchCV</span></code></a>) 依靠 internal <em class="calibre13">scoring</em> strategy （内部 <em class="calibre13">scoring（得分）</em> 策略）。这在 <a class="calibre3 pcalibre" href="#calibre_link-168"><span class="calibre4">scoring 参数: 定义模型评估规则</span></a> 部分讨论。</li>
<li class="toctree-l"><strong class="calibre14">Metric functions（指标函数）</strong>: <code class="docutils"><span class="calibre4">metrics</span></code> 模块实现了针对特定目的评估预测误差的函数。这些指标在以下部分部分详细介绍 <a class="calibre3 pcalibre" href="#calibre_link-169"><span class="calibre4">分类指标</span></a>, <a class="calibre3 pcalibre" href="#calibre_link-170"><span class="calibre4">多标签排名指标</span></a>, <a class="calibre3 pcalibre" href="#calibre_link-171"><span class="calibre4">回归指标</span></a> 和 <a class="calibre3 pcalibre" href="#calibre_link-172"><span class="calibre4">聚类指标</span></a> 。</li>
</ul>
<p class="calibre10">最后， <a class="calibre3 pcalibre" href="#calibre_link-173"><span class="calibre4">虚拟估计</span></a> 用于获取随机预测的这些指标的基准值。</p>
<div class="toctree-wrapper">
<p class="calibre10">See also</p>
<p class="calibre10">对于 “pairwise（成对）” metrics（指标），<em class="calibre13">samples（样本）</em> 之间而不是 estimators （估计量）或者 predictions（预测值），请参阅 <a class="calibre3 pcalibre" href="metrics.html#metrics"><span class="calibre4">成对的矩阵, 类别和核函数</span></a> 部分。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-740">
<span id="calibre_link-168" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.1. <code class="docutils2"><span class="calibre4">scoring</span></code> 参数: 定义模型评估规则</h2>
<p class="calibre2">Model selection （模型选择）和 evaluation （评估）使用工具，例如 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="docutils"><span class="calibre4">model_selection.GridSearchCV</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">model_selection.cross_val_score</span></code></a> ，采用 <code class="docutils"><span class="calibre4">scoring</span></code> 参数来控制它们对 estimators evaluated （评估的估计量）应用的指标。</p>
<div class="toctree-wrapper" id="calibre_link-741">
<h3 class="sigil_not_in_toc1">3.3.1.1. 常见场景: 预定义值</h3>
<p class="calibre2">对于最常见的用例, 您可以使用 <code class="docutils"><span class="calibre4">scoring</span></code> 参数指定一个 scorer object （记分对象）; 下表显示了所有可能的值。
所有 scorer objects （记分对象）遵循惯例  <strong class="calibre14">higher return values are better than lower return values（较高的返回值优于较低的返回值）</strong> 。因此，测量模型和数据之间距离的 metrics （度量），如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="docutils"><span class="calibre4">metrics.mean_squared_error</span></code></a> 可用作返回 metric （指数）的 negated value （否定值）的 neg_mean_squared_error 。</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="21%" class="label"></col>
<col width="32%" class="label"></col>
<col width="46%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">Scoring（得分）</th>
<th class="head">Function（函数）</th>
<th class="head">Comment（注解）</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><strong class="calibre14">Classification（分类）</strong></td>
<td class="label1">&nbsp;</td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘accuracy’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="docutils"><span class="calibre4">metrics.accuracy_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘average_precision’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">metrics.average_precision_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘f1’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">metrics.f1_score</span></code></a></td>
<td class="label1">for binary targets（用于二进制目标）</td>
</tr>
<tr class="calibre23"><td class="label1">‘f1_micro’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">metrics.f1_score</span></code></a></td>
<td class="label1">micro-averaged（微平均）</td>
</tr>
<tr class="row-odd"><td class="label1">‘f1_macro’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">metrics.f1_score</span></code></a></td>
<td class="label1">macro-averaged（微平均）</td>
</tr>
<tr class="calibre23"><td class="label1">‘f1_weighted’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">metrics.f1_score</span></code></a></td>
<td class="label1">weighted average（加权平均）</td>
</tr>
<tr class="row-odd"><td class="label1">‘f1_samples’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">metrics.f1_score</span></code></a></td>
<td class="label1">by multilabel sample（通过 multilabel 样本）</td>
</tr>
<tr class="calibre23"><td class="label1">‘neg_log_loss’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="docutils"><span class="calibre4">metrics.log_loss</span></code></a></td>
<td class="label1">requires <code class="docutils"><span class="calibre4">predict_proba</span></code> support（需要 <code class="docutils"><span class="calibre4">predict_proba</span></code> 支持）</td>
</tr>
<tr class="row-odd"><td class="label1">‘precision’ etc.</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="docutils"><span class="calibre4">metrics.precision_score</span></code></a></td>
<td class="label1">suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr class="calibre23"><td class="label1">‘recall’ etc.</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="docutils"><span class="calibre4">metrics.recall_score</span></code></a></td>
<td class="label1">suffixes apply as with ‘f1’（后缀适用于 ‘f1’）</td>
</tr>
<tr class="row-odd"><td class="label1">‘roc_auc’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">metrics.roc_auc_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1"><strong class="calibre14">Clustering（聚类）</strong></td>
<td class="label1">&nbsp;</td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘adjusted_mutual_info_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="docutils"><span class="calibre4">metrics.adjusted_mutual_info_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘adjusted_rand_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="docutils"><span class="calibre4">metrics.adjusted_rand_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘completeness_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="docutils"><span class="calibre4">metrics.completeness_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘fowlkes_mallows_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code class="docutils"><span class="calibre4">metrics.fowlkes_mallows_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘homogeneity_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="docutils"><span class="calibre4">metrics.homogeneity_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘mutual_info_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code class="docutils"><span class="calibre4">metrics.mutual_info_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘normalized_mutual_info_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="docutils"><span class="calibre4">metrics.normalized_mutual_info_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘v_measure_score’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="docutils"><span class="calibre4">metrics.v_measure_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1"><strong class="calibre14">Regression（回归）</strong></td>
<td class="label1">&nbsp;</td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘explained_variance’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="docutils"><span class="calibre4">metrics.explained_variance_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘neg_mean_absolute_error’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="docutils"><span class="calibre4">metrics.mean_absolute_error</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘neg_mean_squared_error’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="docutils"><span class="calibre4">metrics.mean_squared_error</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘neg_mean_squared_log_error’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="docutils"><span class="calibre4">metrics.mean_squared_log_error</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="calibre23"><td class="label1">‘neg_median_absolute_error’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="docutils"><span class="calibre4">metrics.median_absolute_error</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
<tr class="row-odd"><td class="label1">‘r2’</td>
<td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">metrics.r2_score</span></code></a></td>
<td class="label1">&nbsp;</td>
</tr>
</tbody>
</table>
<p class="calibre10">使用案例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span><span class="calibre4">,</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">probability</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'neg_log_loss'</span><span class="calibre4">)</span> 
<span class="calibre4">array([-0.07..., -0.16..., -0.06...])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">model</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">model</span><span class="calibre4">,</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'wrong_choice'</span><span class="calibre4">)</span>
<span class="calibre4">Traceback (most recent call last):</span>
<span class="calibre4">ValueError</span>: <span class="calibre4">'wrong_choice' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">ValueError exception 列出的值对应于以下部分描述的 functions measuring prediction accuracy （测量预测精度的函数）。
这些函数的 scorer objects （记分对象）存储在 dictionary <code class="docutils"><span class="calibre4">sklearn.metrics.SCORERS</span></code> 中。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-742">
<span id="calibre_link-743" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.1.2. 根据 metric 函数定义您的评分策略</h3>
<p class="calibre2">模块 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="docutils"><span class="calibre4">sklearn.metrics</span></code></a> 还公开了一组 measuring a prediction error （测量预测误差）的简单函数，给出了基础真实的数据和预测:</p>
<ul class="calibre6">
<li class="toctree-l">函数以 <code class="docutils"><span class="calibre4">_score</span></code> 结尾返回一个值来最大化，越高越好。</li>
<li class="toctree-l">函数 <code class="docutils"><span class="calibre4">_error</span></code> 或 <code class="docutils"><span class="calibre4">_loss</span></code> 结尾返回一个值来 minimize （最小化），越低越好。当使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="docutils"><span class="calibre4">make_scorer</span></code></a> 转换成 scorer object （记分对象）时，将 <code class="docutils"><span class="calibre4">greater_is_better</span></code> 参数设置为 False（默认为 True; 请参阅下面的参数说明）。</li>
</ul>
<p class="calibre10">可用于各种机器学习任务的 Metrics （指标）在下面详细介绍。</p>
<p class="calibre10">许多 metrics （指标）没有被用作 <code class="docutils"><span class="calibre4">scoring（得分）</span></code> 值的名称，有时是因为它们需要额外的参数，例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="docutils"><span class="calibre4">fbeta_score</span></code></a> 。在这种情况下，您需要生成一个适当的 scoring object （评分对象）。生成 callable object for scoring （可评估对象进行评分）的最简单方法是使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="docutils"><span class="calibre4">make_scorer</span></code></a> 。该函数将 metrics （指数）转换为可用于可调用的 model evaluation （模型评估）。</p>
<p class="calibre10">一个典型的用例是从库中包含一个非默认值参数的 existing metric function （现有指数函数），例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="docutils"><span class="calibre4">fbeta_score</span></code></a> 函数的 <code class="docutils"><span class="calibre4">beta</span></code> 参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">fbeta_score</span><span class="calibre4">,</span> <span class="calibre4">make_scorer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ftwo_scorer</span> <span class="calibre4">=</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">fbeta_score</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">LinearSVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">grid</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">LinearSVC</span><span class="calibre4">(),</span> <span class="calibre4">param_grid</span><span class="calibre4">=</span><span class="calibre4">{</span><span class="calibre4">'C'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">]},</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">ftwo_scorer</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">第二个用例是使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="docutils"><span class="calibre4">make_scorer</span></code></a> 从简单的 python 函数构建一个完全 custom scorer object （自定义的记分对象），可以使用几个参数 :</p>
<ul class="calibre6">
<li class="toctree-l">你要使用的 python 函数（在下面的例子中是 <code class="docutils"><span class="calibre4">my_custom_loss_func</span></code>）</li>
<li class="toctree-l">python 函数是否返回一个分数 (<code class="docutils"><span class="calibre4">greater_is_better=True</span></code>, 默认值) 或者一个 loss （损失） (<code class="docutils"><span class="calibre4">greater_is_better=False</span></code>)。 如果是一个 loss （损失），scorer object （记分对象）的 python 函数的输出被 negated （否定），符合 cross validation convention （交叉验证约定），scorers 为更好的模型返回更高的值。</li>
<li class="toctree-l">仅用于 classification metrics （分类指数）: 您提供的 python 函数是否需要连续的 continuous decision certainties （判断确定性）（<code class="docutils"><span class="calibre4">needs_threshold=True</span></code>）。默认值为 False 。</li>
<li class="toctree-l">任何其他参数，如 <code class="docutils"><span class="calibre4">beta</span></code> 或者 <code class="docutils"><span class="calibre4">labels</span></code> 在 函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a> 。</li>
</ul>
<p class="calibre10">以下是建立 custom scorers （自定义记分对象）的示例，并使用 <code class="docutils"><span class="calibre4">greater_is_better</span></code> 参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">my_custom_loss_func</span><span class="calibre4">(</span><span class="calibre4">ground_truth</span><span class="calibre4">,</span> <span class="calibre4">predictions</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">diff</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">abs</span><span class="calibre4">(</span><span class="calibre4">ground_truth</span> <span class="calibre4">-</span> <span class="calibre4">predictions</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">max</span><span class="calibre4">()</span>
<span class="calibre4">... </span>    <span class="calibre4">return</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">log</span><span class="calibre4">(</span><span class="calibre4">1</span> <span class="calibre4">+</span> <span class="calibre4">diff</span><span class="calibre4">)</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># loss_func will negate the return value of my_custom_loss_func,</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">#  which will be np.log(2), 0.693, given the values for ground_truth</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">#  and predictions defined below.</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">loss</span>  <span class="calibre4">=</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">my_custom_loss_func</span><span class="calibre4">,</span> <span class="calibre4">greater_is_better</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">score</span> <span class="calibre4">=</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">my_custom_loss_func</span><span class="calibre4">,</span> <span class="calibre4">greater_is_better</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ground_truth</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">predictions</span>  <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.dummy</span> <span class="calibre4">import</span> <span class="calibre4">DummyClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">DummyClassifier</span><span class="calibre4">(</span><span class="calibre4">strategy</span><span class="calibre4">=</span><span class="calibre4">'most_frequent'</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">ground_truth</span><span class="calibre4">,</span> <span class="calibre4">predictions</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">loss</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span><span class="calibre4">ground_truth</span><span class="calibre4">,</span> <span class="calibre4">predictions</span><span class="calibre4">)</span> 
<span class="calibre4">-0.69...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span><span class="calibre4">ground_truth</span><span class="calibre4">,</span> <span class="calibre4">predictions</span><span class="calibre4">)</span> 
<span class="calibre4">0.69...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-744">
<span id="calibre_link-745" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.1.3. 实现自己的记分对象</h3>
<p class="calibre2">您可以通过从头开始构建自己的 scoring object （记分对象），而不使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer" title="sklearn.metrics.make_scorer"><code class="docutils"><span class="calibre4">make_scorer</span></code></a> factory 来生成更加灵活的 model scorers （模型记分对象）。
对于被叫做 scorer 来说，它需要符合以下两个规则所指定的协议:</p>
<ul class="calibre6">
<li class="toctree-l">可以使用参数 <code class="docutils"><span class="calibre4">(estimator,</span> <span class="calibre4">X,</span> <span class="calibre4">y)</span></code> 来调用它，其中 <code class="docutils"><span class="calibre4">estimator</span></code> 是要被评估的模型，<code class="docutils"><span class="calibre4">X</span></code> 是验证数据， <code class="docutils"><span class="calibre4">y</span></code> 是 <code class="docutils"><span class="calibre4">X</span></code> (在有监督情况下) 或 <code class="docutils"><span class="calibre4">None</span></code> (在无监督情况下) 已经被标注的真实数据目标。</li>
<li class="toctree-l">它返回一个浮点数，用于对 <code class="docutils"><span class="calibre4">X</span></code> 进行量化 <code class="docutils"><span class="calibre4">estimator</span></code> 的预测质量，参考 <code class="docutils"><span class="calibre4">y</span></code> 。
再次，按照惯例，更高的数字更好，所以如果你的 scorer 返回 loss ，那么这个值应该被 negated 。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-746">
<span id="calibre_link-747" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.1.4. 使用多个指数评估</h3>
<p class="calibre2">Scikit-learn 还允许在 <code class="docutils"><span class="calibre4">GridSearchCV</span></code>, <code class="docutils"><span class="calibre4">RandomizedSearchCV</span></code> 和 <code class="docutils"><span class="calibre4">cross_validate</span></code> 中评估 multiple metric （多个指数）。</p>
<p class="calibre10">为 <code class="docutils"><span class="calibre4">scoring</span></code> 参数指定多个评分指标有两种方法:</p>
<ul class="calibre6">
<li class="toctree-l"><dl class="first">
<dt class="calibre18">As an iterable of string metrics（作为 string metrics 的迭代）::</dt>
<dd class="calibre19"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scoring</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'accuracy'</span><span class="calibre4">,</span> <span class="calibre4">'precision'</span><span class="calibre4">]</span>
</pre>
</div>
</div>
</dd>
</dl>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">As a <code class="docutils"><span class="calibre4">dict</span></code> mapping the scorer name to the scoring function（作为 <code class="docutils"><span class="calibre4">dict</span></code> ，将 scorer 名称映射到 scoring 函数）::</dt>
<dd class="calibre19"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">accuracy_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">make_scorer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scoring</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'accuracy'</span><span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">accuracy_score</span><span class="calibre4">),</span>
<span class="calibre4">... </span>           <span class="calibre4">'prec'</span><span class="calibre4">:</span> <span class="calibre4">'precision'</span><span class="calibre4">}</span>
</pre>
</div>
</div>
</dd>
</dl>
</li>
</ul>
<p class="calibre10">请注意， dict 值可以是 scorer functions （记分函数）或者 predefined metric strings （预定义 metric 字符串）之一。</p>
<p class="calibre10">目前，只有那些返回 single score （单一分数）的 scorer functions （记分函数）才能在 dict 内传递。不允许返回多个值的 Scorer functions （Scorer 函数），并且需要一个 wrapper 才能返回 single metric（单个指标）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_validate</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">confusion_matrix</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># A sample toy binary classification dataset</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">make_classification</span><span class="calibre4">(</span><span class="calibre4">n_classes</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm</span> <span class="calibre4">=</span> <span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">tp</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">):</span> <span class="calibre4">return</span> <span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">tn</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">):</span> <span class="calibre4">return</span> <span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">fp</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">):</span> <span class="calibre4">return</span> <span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">fn</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">):</span> <span class="calibre4">return</span> <span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scoring</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'tp'</span> <span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">tp</span><span class="calibre4">),</span> <span class="calibre4">'tn'</span> <span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">tn</span><span class="calibre4">),</span>
<span class="calibre4">... </span>           <span class="calibre4">'fp'</span> <span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">fp</span><span class="calibre4">),</span> <span class="calibre4">'fn'</span> <span class="calibre4">:</span> <span class="calibre4">make_scorer</span><span class="calibre4">(</span><span class="calibre4">fn</span><span class="calibre4">)}</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cv_results</span> <span class="calibre4">=</span> <span class="calibre4">cross_validate</span><span class="calibre4">(</span><span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">),</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">scoring</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Getting the test set true positive scores</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">cv_results</span><span class="calibre4">[</span><span class="calibre4">'test_tp'</span><span class="calibre4">])</span>          
<span class="calibre4">[12 13 15]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Getting the test set false negative scores</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">cv_results</span><span class="calibre4">[</span><span class="calibre4">'test_fn'</span><span class="calibre4">])</span>          
<span class="calibre4">[5 4 1]</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-169">
<span id="calibre_link-748" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.2. 分类指标</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="docutils"><span class="calibre4">sklearn.metrics</span></code></a> 模块实现了几个 loss, score, 和 utility 函数来衡量 classification （分类）性能。
某些 metrics （指标）可能需要 positive class （正类），confidence values（置信度值）或 binary decisions values （二进制决策值）的概率估计。
大多数的实现允许每个样本通过 <code class="docutils"><span class="calibre4">sample_weight</span></code> 参数为 overall score （总分）提供 weighted contribution （加权贡献）。</p>
<p class="calibre10">其中一些仅限于二分类案例:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="docutils"><span class="calibre4">precision_recall_curve</span></code></a>(y_true,&nbsp;probas_pred)</td>
<td class="label1">Compute precision-recall pairs for different probability thresholds</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="docutils"><span class="calibre4">roc_curve</span></code></a>(y_true,&nbsp;y_score[,&nbsp;pos_label,&nbsp;…])</td>
<td class="label1">Compute Receiver operating characteristic (ROC)</td>
</tr>
</tbody>
</table>
<p class="calibre10">其他也可以在多分类案例中运行:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code class="docutils"><span class="calibre4">cohen_kappa_score</span></code></a>(y1,&nbsp;y2[,&nbsp;labels,&nbsp;weights,&nbsp;…])</td>
<td class="label1">Cohen’s kappa: a statistic that measures inter-annotator agreement.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="docutils"><span class="calibre4">confusion_matrix</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute confusion matrix to evaluate the accuracy of a classification</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="docutils"><span class="calibre4">hinge_loss</span></code></a>(y_true,&nbsp;pred_decision[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Average hinge loss (non-regularized)</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="docutils"><span class="calibre4">matthews_corrcoef</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td class="label1">Compute the Matthews correlation coefficient (MCC)</td>
</tr>
</tbody>
</table>
<p class="calibre10">有些还可以在 multilabel case （多重案例）中工作:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="docutils"><span class="calibre4">accuracy_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…])</td>
<td class="label1">Accuracy classification score.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code class="docutils"><span class="calibre4">classification_report</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td class="label1">Build a text report showing the main classification metrics</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the F1 score, also known as balanced F-score or F-measure</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="docutils"><span class="calibre4">fbeta_score</span></code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the F-beta score</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="docutils"><span class="calibre4">hamming_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the average Hamming loss.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code class="docutils"><span class="calibre4">jaccard_similarity_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;…])</td>
<td class="label1">Jaccard similarity coefficient score</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="docutils"><span class="calibre4">log_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;eps,&nbsp;normalize,&nbsp;…])</td>
<td class="label1">Log loss, aka logistic loss or cross-entropy loss.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="docutils"><span class="calibre4">precision_recall_fscore_support</span></code></a>(y_true,&nbsp;y_pred)</td>
<td class="label1">Compute precision, recall, F-measure and support for each class</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="docutils"><span class="calibre4">precision_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the precision</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="docutils"><span class="calibre4">recall_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the recall</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="docutils"><span class="calibre4">zero_one_loss</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;normalize,&nbsp;…])</td>
<td class="label1">Zero-one classification loss.</td>
</tr>
</tbody>
</table>
<p class="calibre10">一些通常用于 ranking:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.dcg_score.html#sklearn.metrics.dcg_score" title="sklearn.metrics.dcg_score"><code class="docutils"><span class="calibre4">dcg_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;k])</td>
<td class="label1">Discounted cumulative gain (DCG) at rank K.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.ndcg_score.html#sklearn.metrics.ndcg_score" title="sklearn.metrics.ndcg_score"><code class="docutils"><span class="calibre4">ndcg_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;k])</td>
<td class="label1">Normalized discounted cumulative gain (NDCG) at rank K.</td>
</tr>
</tbody>
</table>
<p class="calibre10">有些工作与 binary 和 multilabel （但不是多类）的问题:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;…])</td>
<td class="label1">Compute average precision (AP) from prediction scores</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">roc_auc_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;average,&nbsp;…])</td>
<td class="label1">Compute Area Under the Curve (AUC) from prediction scores</td>
</tr>
</tbody>
</table>
<p class="calibre10">在以下小节中，我们将介绍每个这些功能，前面是一些关于通用 API 和 metric 定义的注释。</p>
<div class="toctree-wrapper" id="calibre_link-749">
<h3 class="sigil_not_in_toc1">3.3.2.1. 从二分到多分类和 multilabel</h3>
<p class="calibre2">一些 metrics 基本上是为 binary classification tasks （二分类任务）定义的 (例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">roc_auc_score</span></code></a>) 。在这些情况下，默认情况下仅评估 positive label （正标签），假设默认情况下，positive label （正类）标记为 <code class="docutils"><span class="calibre4">1</span></code> （尽管可以通过 <code class="docutils"><span class="calibre4">pos_label</span></code> 参数进行配置）。</p>
<p id="calibre_link-175" class="calibre10">将 binary metric （二分指标）扩展为 multiclass （多类）或 multilabel （多标签）问题时，数据将被视为二分问题的集合，每个类都有一个。
然后可以使用多种方法在整个类中 average binary metric calculations （平均二分指标计算），每种类在某些情况下可能会有用。
如果可用，您应该使用 <code class="docutils"><span class="calibre4">average</span></code> 参数来选择它们。</p>
<ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">"macro（宏）"</span></code> 简单地计算 binary metrics （二分指标）的平均值，赋予每个类别相同的权重。在不常见的类别重要的问题上，macro-averaging （宏观平均）可能是突出表现的一种手段。另一方面，所有类别同样重要的假设通常是不真实的，因此 macro-averaging （宏观平均）将过度强调不频繁类的典型的低性能。</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">"weighted（加权）"</span></code> 通过计算其在真实数据样本中的存在来对每个类的 score 进行加权的 binary metrics （二分指标）的平均值来计算类不平衡。</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">"micro（微）"</span></code> 给每个 sample-class pair （样本类对）对 overall metric （总体指数）（sample-class 权重的结果除外） 等同的贡献。除了对每个类别的 metric 进行求和之外，这个总和构成每个类别度量的 dividends （除数）和 divisors （除数）计算一个整体商。
在 multilabel settings （多标签设置）中，Micro-averaging 可能是优先选择的，包括要忽略 majority class （多数类）的 multiclass classification （多类分类）。</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">"samples（样本）"</span></code> 仅适用于 multilabel problems （多标签问题）。它 does not calculate a per-class measure （不计算每个类别的 measure），而是计算 evaluation data （评估数据）中的每个样本的 true and predicted classes （真实和预测类别）的 metric （指标），并返回 (<code class="docutils"><span class="calibre4">sample_weight</span></code>-weighted) 加权平均。</li>
<li class="toctree-l">选择 <code class="docutils"><span class="calibre4">average=None</span></code> 将返回一个 array 与每个类的 score 。</li>
</ul>
<p class="calibre10">虽然将 multiclass data （多类数据）提供给 metric ，如 binary targets （二分类目标），作为 array of class labels （类标签的数组），multilabel data （多标签数据）被指定为 indicator matrix（指示符矩阵），其中 cell <code class="docutils"><span class="calibre4">[i,</span> <span class="calibre4">j]</span></code> 具有值 1，如果样本 <code class="docutils"><span class="calibre4">i</span></code> 具有标号 <code class="docutils"><span class="calibre4">j</span></code> ，否则为值 0 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-750">
<span id="calibre_link-751" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.2. 精确度得分</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="docutils"><span class="calibre4">accuracy_score</span></code></a> 函数计算 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Accuracy_and_precision">accuracy</a>, 正确预测的分数（默认）或计数 (normalize=False)。</p>
<p class="calibre10">在 multilabel classification （多标签分类）中，函数返回 subset accuracy（子集精度）。如果样本的 entire set of predicted labels （整套预测标签）与真正的标签组合匹配，则子集精度为 1.0; 否则为 0.0 。</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 个样本的预测值，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是相应的真实值，则 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000857.jpg" alt="n_\text{samples}" /> 上的正确预测的分数被定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000438.jpg" alt="\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000388.jpg" alt="1(x)" /> 是 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指示函数）</a>.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">accuracy_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">accuracy_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.5</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">accuracy_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">normalize</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">2</span>
</pre>
</div>
</div>
<p class="calibre10">In the multilabel case with binary label indicators（在具有二分标签指示符的多标签情况下）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">accuracy_score</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]),</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)))</span>
<span class="calibre4">0.5</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_permutation_test_for_classification.html#sphx-glr-auto-examples-feature-selection-plot-permutation-test-for-classification-py"><span class="calibre4">Test with permutations the significance of a classification score</span></a>
例如使用数据集排列的 accuracy score （精度分数）。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-752">
<span id="calibre_link-753" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.3. Cohen’s kappa</h3>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score" title="sklearn.metrics.cohen_kappa_score"><code class="docutils"><span class="calibre4">cohen_kappa_score</span></code></a> 计算 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Cohen’s kappa</a> statistic（统计）。
这个 measure （措施）旨在比较不同人工标注者的标签，而不是 classifier （分类器）与 ground truth （真实数据）。</p>
<p class="calibre10">kappa score （参阅 docstring ）是 -1 和 1 之间的数字。
.8 以上的 scores 通常被认为是很好的 agreement （协议）;
0 或者 更低表示没有 agreement （实际上是 random labels （随机标签））。</p>
<p class="calibre10">Kappa scores 可以计算 binary or multiclass （二分或者多分类）问题，但不能用于 multilabel problems （多标签问题）（除了手动计算 per-label score （每个标签分数）），而不是两个以上的 annotators （注释器）。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">cohen_kappa_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cohen_kappa_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.4285714285714286</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-754">
<span id="calibre_link-755" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.4. 混淆矩阵</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="docutils"><span class="calibre4">confusion_matrix</span></code></a> 函数通过计算 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix（混淆矩阵）</a> 来 evaluates classification accuracy （评估分类的准确性）。</p>
<p class="calibre10">根据定义，confusion matrix （混淆矩阵）中的 entry（条目） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000874.jpg" alt="i, j" />，是实际上在 group <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 中的 observations （观察数），但预测在 group <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000457.jpg" alt="j" /> 中。这里是一个示例:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">confusion_matrix</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">array([[2, 0, 0],</span>
<span class="calibre4">       [0, 0, 1],</span>
<span class="calibre4">       [1, 0, 2]])</span>
</pre>
</div>
</div>
<p class="calibre10">这是一个这样的 confusion matrix （混淆矩阵）的可视化表示 （这个数字来自于 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="calibre4">Confusion matrix</span></a>）:</p>
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_confusion_matrix.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_confusion_matrix_0011.png" class="calibre27" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000283.jpg" /></a>
<p class="calibre10">对于 binary problems （二分类问题），我们可以得到 true negatives（真 negatives）, false positives（假 positives）, false negatives（假 negatives） 和 true positives（真 positives） 的数量如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tn</span><span class="calibre4">,</span> <span class="calibre4">fp</span><span class="calibre4">,</span> <span class="calibre4">fn</span><span class="calibre4">,</span> <span class="calibre4">tp</span> <span class="calibre4">=</span> <span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">ravel</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tn</span><span class="calibre4">,</span> <span class="calibre4">fp</span><span class="calibre4">,</span> <span class="calibre4">fn</span><span class="calibre4">,</span> <span class="calibre4">tp</span>
<span class="calibre4">(2, 1, 2, 3)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py"><span class="calibre4">Confusion matrix</span></a>
例如使用 confusion matrix （混淆矩阵）来评估 classifier （分类器）的输出质量。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="calibre4">Recognizing hand-written digits</span></a>
例如使用 confusion matrix （混淆矩阵）来分类手写数字。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a>
例如使用 confusion matrix （混淆矩阵）对文本文档进行分类。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-756">
<span id="calibre_link-757" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.5. 分类报告</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" title="sklearn.metrics.classification_report"><code class="docutils"><span class="calibre4">classification_report</span></code></a> 函数构建一个显示 main classification metrics （主分类指标）的文本报告。这是一个小例子，其中包含自定义的 <code class="docutils"><span class="calibre4">target_names</span></code> 和 inferred labels （推断标签）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">classification_report</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">target_names</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'class 0'</span><span class="calibre4">,</span> <span class="calibre4">'class 1'</span><span class="calibre4">,</span> <span class="calibre4">'class 2'</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">classification_report</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">target_names</span><span class="calibre4">=</span><span class="calibre4">target_names</span><span class="calibre4">))</span>
<span class="calibre4">             precision    recall  f1-score   support</span>

<span class="calibre4">    class 0       0.67      1.00      0.80         2</span>
<span class="calibre4">    class 1       0.00      0.00      0.00         1</span>
<span class="calibre4">    class 2       1.00      0.50      0.67         2</span>

<span class="calibre4">avg / total       0.67      0.60      0.59         5</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="calibre4">Recognizing hand-written digits</span></a>
作为手写数字的分类报告的使用示例。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a>
作为文本文档的分类报告使用的示例。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="calibre4">Parameter estimation using grid search with cross-validation</span></a>
例如使用 grid search with nested cross-validation （嵌套交叉验证进行网格搜索）的分类报告。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-758">
<span id="calibre_link-759" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.6. 汉明损失</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="docutils"><span class="calibre4">hamming_loss</span></code></a> 计算两组样本之间的 average Hamming loss （平均汉明损失）或者 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance（汉明距离）</a> 。</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000563.jpg" alt="\hat{y}_j" /> 是给定样本的第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000457.jpg" alt="j" /> 个标签的预测值，则 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000432.jpg" alt="y_j" /> 是相应的真实值，而 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000004.jpg" alt="n_\text{labels}" /> 是 classes or labels （类或者标签）的数量，则两个样本之间的 Hamming loss （汉明损失） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000313.jpg" alt="L_{Hamming}" /> 定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000380.jpg" alt="L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{labels}} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_j \not= y_j)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000388.jpg" alt="1(x)" /> 是 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function（指标函数）</a>.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">hamming_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hamming_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.25</span>
</pre>
</div>
</div>
<p class="calibre10">在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hamming_loss</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]),</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">zeros</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)))</span>
<span class="calibre4">0.75</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">在 multiclass classification （多类分类）中， Hamming loss （汉明损失）对应于 <code class="docutils"><span class="calibre4">y_true</span></code> 和 <code class="docutils"><span class="calibre4">y_pred</span></code> 之间的 Hamming distance（汉明距离），它类似于 <a class="calibre3 pcalibre" href="#calibre_link-174"><span class="calibre4">零一损失</span></a> 函数。然而， zero-one loss penalizes （0-1损失惩罚）不严格匹配真实集合的预测集，Hamming loss （汉明损失）惩罚 individual labels （独立标签）。因此，Hamming loss（汉明损失）高于 zero-one loss（0-1 损失），总是在 0 和 1 之间，包括 0 和 1;预测真正的标签的正确的 subset or superset （子集或超集）将给出 0 和 1 之间的 Hamming loss（汉明损失）。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-760">
<span id="calibre_link-761" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.7. Jaccard 相似系数 score</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score" title="sklearn.metrics.jaccard_similarity_score"><code class="docutils"><span class="calibre4">jaccard_similarity_score</span></code></a> 函数计算 pairs of label sets （标签组对）之间的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard similarity coefficients</a> 也称作 Jaccard index 的平均值（默认）或总和。</p>
<p class="calibre10">将第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 个样本的 Jaccard similarity coefficient 与 被标注过的真实数据的标签集 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 和 predicted label set （预测标签集）:math:<cite class="calibre13">hat{y}_i</cite> 定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000526.jpg" alt="J(y_i, \hat{y}_i) = \frac{|y_i \cap \hat{y}_i|}{|y_i \cup \hat{y}_i|}." class="math" /></p>
</div>
<p class="calibre10">在 binary and multiclass classification （二分和多类分类）中，Jaccard similarity coefficient score 等于 classification accuracy（分类精度）。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">jaccard_similarity_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">jaccard_similarity_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.5</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">jaccard_similarity_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">normalize</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">2</span>
</pre>
</div>
</div>
<p class="calibre10">在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">jaccard_similarity_score</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]),</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)))</span>
<span class="calibre4">0.75</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-762">
<span id="calibre_link-763" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.8. 精准，召回和 F-measures</h3>
<p class="calibre2">直观地来理解，<a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Precision_and_recall#Precision">precision</a> 是 the ability of the classifier not to label as positive a sample that is negative （classifier （分类器）的标签不能被标记为正的样本为负的能力），并且 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall">recall</a> 是 classifier （分类器）查找所有 positive samples （正样本）的能力。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/F1_score">F-measure</a> (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000192.jpg" alt="F_\beta" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000389.jpg" alt="F_1" /> measures) 可以解释为 precision （精度）和 recall （召回）的 weighted harmonic mean （加权调和平均值）。 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000192.jpg" alt="F_\beta" /> measure 值达到其最佳值 1 ，其最差分数为 0 。与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000070.jpg" alt="\beta = 1" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000192.jpg" alt="F_\beta" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000389.jpg" alt="F_1" /> 是等价的， recall （召回）和 precision （精度）同样重要。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="docutils"><span class="calibre4">precision_recall_curve</span></code></a> 通过改变 decision threshold （决策阈值）从 ground truth label （被标记的真实数据标签） 和 score given by the classifier （分类器给出的分数）计算 precision-recall curve （精确召回曲线）。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a> 函数根据 prediction scores （预测分数）计算出 average precision (AP)（平均精度）。该分数对应于 precision-recall curve （精确召回曲线）下的面积。该值在 0 和 1 之间，并且越高越好。通过 random predictions （随机预测）， AP 是 fraction of positive samples （正样本的分数）。</p>
<p class="calibre10">几个函数可以让您 analyze the precision （分析精度），recall（召回） 和 F-measures 得分:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a>(y_true,&nbsp;y_score[,&nbsp;…])</td>
<td class="label1">Compute average precision (AP) from prediction scores</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the F1 score, also known as balanced F-score or F-measure</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="docutils"><span class="calibre4">fbeta_score</span></code></a>(y_true,&nbsp;y_pred,&nbsp;beta[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the F-beta score</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="docutils"><span class="calibre4">precision_recall_curve</span></code></a>(y_true,&nbsp;probas_pred)</td>
<td class="label1">Compute precision-recall pairs for different probability thresholds</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="docutils"><span class="calibre4">precision_recall_fscore_support</span></code></a>(y_true,&nbsp;y_pred)</td>
<td class="label1">Compute precision, recall, F-measure and support for each class</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="docutils"><span class="calibre4">precision_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the precision</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="docutils"><span class="calibre4">recall_score</span></code></a>(y_true,&nbsp;y_pred[,&nbsp;labels,&nbsp;…])</td>
<td class="label1">Compute the recall</td>
</tr>
</tbody>
</table>
<p class="calibre10">请注意，<a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="docutils"><span class="calibre4">precision_recall_curve</span></code></a> 函数仅限于 binary case （二分情况）。 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a> 函数只适用于 binary classification and multilabel indicator format （二分类和多标签指示器格式）。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a>
例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a> 用于分类文本文档的用法。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py"><span class="calibre4">Parameter estimation using grid search with cross-validation</span></a>
例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="docutils"><span class="calibre4">precision_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="docutils"><span class="calibre4">recall_score</span></code></a> 用于 using grid search with nested cross-validation （使用嵌套交叉验证的网格搜索）来估计参数。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"><span class="calibre4">Precision-Recall</span></a>
例如 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="docutils"><span class="calibre4">precision_recall_curve</span></code></a> 用于 evaluate classifier output quality（评估分类器输出质量）。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-764">
<h4 class="sigil_not_in_toc1">3.3.2.8.1. 二分类</h4>
<p class="calibre2">在二分类任务中，术语 ‘’positive（正）’’ 和 ‘’negative（负）’’ 是指 classifier’s prediction （分类器的预测），术语 ‘’true（真）’’ 和 ‘’false（假）’’ 是指该预测是否对应于 external judgment （外部判断）（有时被称为 ‘’observation（观测值）’‘）。给出这些定义，我们可以指定下表:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="29%" class="label"></col>
<col width="32%" class="label"></col>
<col width="39%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">&nbsp;</td>
<td colspan="2" class="label1">Actual class (observation)</td>
</tr>
<tr class="row-odd"><td rowspan="2" class="label1">Predicted class
(expectation)</td>
<td class="label1">tp (true positive)
Correct result</td>
<td class="label1">fp (false positive)
Unexpected result</td>
</tr>
<tr class="calibre23"><td class="label1">fn (false negative)
Missing result</td>
<td class="label1">tn (true negative)
Correct absence of result</td>
</tr>
</tbody>
</table>
<p class="calibre10">在这种情况下，我们可以定义 precision（精度）, recall（召回） 和 F-measure 的概念:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000570.jpg" alt="\text{precision} = \frac{tp}{tp + fp}," class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000273.jpg" alt="\text{recall} = \frac{tp}{tp + fn}," class="math" /></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000022.jpg" alt="F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}." class="math" /></p>
</div>
<p class="calibre10">以下是 binary classification （二分类）中的一些小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">precision_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">recall_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.5</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">f1_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.66...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fbeta_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">)</span>  
<span class="calibre4">0.83...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fbeta_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>  
<span class="calibre4">0.66...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fbeta_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span> 
<span class="calibre4">0.55...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">precision_recall_fscore_support</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">)</span>  
<span class="calibre4">(array([ 0.66...,  1.        ]), array([ 1. ,  0.5]), array([ 0.71...,  0.83...]), array([2, 2]...))</span>


<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">precision_recall_curve</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">average_precision_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_scores</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.4</span><span class="calibre4">,</span> <span class="calibre4">0.35</span><span class="calibre4">,</span> <span class="calibre4">0.8</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">precision</span><span class="calibre4">,</span> <span class="calibre4">recall</span><span class="calibre4">,</span> <span class="calibre4">threshold</span> <span class="calibre4">=</span> <span class="calibre4">precision_recall_curve</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_scores</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">precision</span>  
<span class="calibre4">array([ 0.66...,  0.5       ,  1.        ,  1.        ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">recall</span>
<span class="calibre4">array([ 1. ,  0.5,  0.5,  0. ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">threshold</span>
<span class="calibre4">array([ 0.35,  0.4 ,  0.8 ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">average_precision_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_scores</span><span class="calibre4">)</span>  
<span class="calibre4">0.83...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-765">
<h4 class="sigil_not_in_toc1">3.3.2.8.2. 多类和多标签分类</h4>
<p class="calibre2">在 multiclass and multilabel classification task（多类和多标签分类任务）中，precision（精度）, recall（召回）, and F-measures 的概念可以独立地应用于每个标签。
有以下几种方法 combine results across labels （将结果跨越标签组合），由 <code class="docutils"><span class="calibre4">average</span></code> 参数指定为 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a> （仅用于 multilabel）， <a class="calibre3 pcalibre" href="generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score" title="sklearn.metrics.f1_score"><code class="docutils"><span class="calibre4">f1_score</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="docutils"><span class="calibre4">fbeta_score</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="docutils"><span class="calibre4">precision_recall_fscore_support</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score" title="sklearn.metrics.precision_score"><code class="docutils"><span class="calibre4">precision_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="docutils"><span class="calibre4">recall_score</span></code></a> 函数，如上 <a class="calibre3 pcalibre" href="#calibre_link-175"><span class="calibre4">above</span></a> 所述。请注意，对于在包含所有标签的多类设置中进行 “micro”-averaging （”微”平均），将产生相等的 precision（精度）， recall（召回）和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000208.jpg" alt="F" /> ，而 “weighted（加权）” averaging（平均）可能会产生 precision（精度）和 recall（召回）之间的 F-score 。</p>
<p class="calibre10">为了使这一点更加明确，请考虑以下 notation （符号）:</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> <em class="calibre13">predicted（预测）</em> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000342.jpg" alt="(sample, label)" /> 对</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000505.jpg" alt="\hat{y}" /> <em class="calibre13">true（真）</em> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000342.jpg" alt="(sample, label)" /> 对</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000635.jpg" alt="L" /> labels 集合</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000248.jpg" alt="S" /> samples 集合</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000067.jpg" alt="y_s" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 的子集与样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000242.jpg" alt="s" />, 即 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000890.jpg" alt="y_s := \left\{(s&apos;, l) \in y | s&apos; = s\right\}" /></li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000651.jpg" alt="y_l" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 的子集与 label <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000896.jpg" alt="l" /></li>
<li class="toctree-l">类似的, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000062.jpg" alt="\hat{y}_s" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000069.jpg" alt="\hat{y}_l" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000505.jpg" alt="\hat{y}" /> 的子集</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000664.jpg" alt="P(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}" /></li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000522.jpg" alt="R(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}" />
(Conventions （公约）在处理 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000390.jpg" alt="B = \emptyset" /> 有所不同; 这个实现使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000168.jpg" alt="R(A, B):=0" />, 与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000138.jpg" alt="P" /> 类似.)</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000780.jpg" alt="F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}" /></li>
</ul>
<p class="calibre10">然后将 metrics （指标）定义为:</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="4%" class="label"></col>
<col width="32%" class="label"></col>
<col width="32%" class="label"></col>
<col width="33%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head"><code class="docutils"><span class="calibre4">average</span></code></th>
<th class="head">Precision</th>
<th class="head">Recall</th>
<th class="head">F_beta</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><code class="docutils"><span class="calibre4">"micro"</span></code></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000545.jpg" alt="P(y, \hat{y})" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000688.jpg" alt="R(y, \hat{y})" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000561.jpg" alt="F_\beta(y, \hat{y})" /></td>
</tr>
<tr class="row-odd"><td class="label1"><code class="docutils"><span class="calibre4">"samples"</span></code></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000885.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000430.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000119.jpg" alt="\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)" /></td>
</tr>
<tr class="calibre23"><td class="label1"><code class="docutils"><span class="calibre4">"macro"</span></code></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000387.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000502.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000420.jpg" alt="\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)" /></td>
</tr>
<tr class="row-odd"><td class="label1"><code class="docutils"><span class="calibre4">"weighted"</span></code></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000823.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| P(y_l, \hat{y}_l)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000400.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| R(y_l, \hat{y}_l)" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000591.jpg" alt="\frac{1}{\sum_{l \in L} \left|\hat{y}_l\right|} \sum_{l \in L} \left|\hat{y}_l\right| F_\beta(y_l, \hat{y}_l)" /></td>
</tr>
<tr class="calibre23"><td class="label1"><code class="docutils"><span class="calibre4">None</span></code></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000405.jpg" alt="\langle P(y_l, \hat{y}_l) | l \in L \rangle" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000869.jpg" alt="\langle R(y_l, \hat{y}_l) | l \in L \rangle" /></td>
<td class="label1"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000144.jpg" alt="\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle" /></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">precision_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)</span>  
<span class="calibre4">0.22...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">recall_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'micro'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">0.33...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">f1_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'weighted'</span><span class="calibre4">)</span>  
<span class="calibre4">0.26...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">fbeta_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">)</span>  
<span class="calibre4">0.23...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">precision_recall_fscore_support</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">beta</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">(array([ 0.66...,  0.        ,  0.        ]), array([ 1.,  0.,  0.]), array([ 0.71...,  0.        ,  0.        ]), array([2, 2, 2]...))</span>
</pre>
</div>
</div>
<p class="calibre10">For multiclass classification with a “negative class”, it is possible to exclude some labels:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">recall_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'micro'</span><span class="calibre4">)</span>
<span class="calibre4">... </span><span class="calibre4"># excluding 0, no labels were correctly recalled</span>
<span class="calibre4">0.0</span>
</pre>
</div>
</div>
<p class="calibre10">Similarly, labels not present in the data sample may be accounted for in macro-averaging.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">precision_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">0.166...</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-766">
<span id="calibre_link-767" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.9. Hinge loss</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="docutils"><span class="calibre4">hinge_loss</span></code></a> 函数使用 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Hinge_loss">hinge loss</a> 计算模型和数据之间的 average distance （平均距离），这是一种只考虑 prediction errors （预测误差）的 one-sided metric （单向指标）。（Hinge loss 用于最大边界分类器，如支持向量机）</p>
<p class="calibre10">如果标签用 +1 和 -1 编码，则 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" />: 是真实值，并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000588.jpg" alt="w" /> 是由 <code class="docutils"><span class="calibre4">decision_function</span></code> 输出的 predicted decisions （预测决策），则 hinge loss 定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000454.jpg" alt="L_\text{Hinge}(y, w) = \max\left\{1 - wy, 0\right\} = \left|1 - wy\right|_+" class="math" /></p>
</div>
<p class="calibre10">如果有两个以上的标签， <a class="calibre3 pcalibre" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="docutils"><span class="calibre4">hinge_loss</span></code></a> 由于 Crammer &amp; Singer 而使用了 multiclass variant （多类型变体）。
<a class="calibre3 pcalibre" href="http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf">Here</a> 是描述它的论文。</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000807.jpg" alt="y_w" /> 是真实标签的 predicted decision （预测决策），并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000188.jpg" alt="y_t" /> 是所有其他标签的预测决策的最大值，其中预测决策由 decision function （决策函数）输出，则 multiclass hinge loss 定义如下:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000672.jpg" alt="L_\text{Hinge}(y_w, y_t) = \max\left\{1 + y_t - y_w, 0\right\}" class="math" /></p>
</div>
<p class="calibre10">这里是一个小例子，演示了在 binary class （二类）问题中使用了具有 svm classifier （svm 的分类器）的 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="docutils"><span class="calibre4">hinge_loss</span></code></a> 函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">hinge_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">est</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">LinearSVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="calibre4">     intercept_scaling=1, loss='squared_hinge', max_iter=1000,</span>
<span class="calibre4">     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,</span>
<span class="calibre4">     verbose=0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred_decision</span> <span class="calibre4">=</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0.5</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred_decision</span>  
<span class="calibre4">array([-2.18...,  2.36...,  0.09...])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hinge_loss</span><span class="calibre4">([</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">pred_decision</span><span class="calibre4">)</span>  
<span class="calibre4">0.3...</span>
</pre>
</div>
</div>
<p class="calibre10">这里是一个示例，演示了在 multiclass problem （多类问题）中使用了具有 svm 分类器的 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.hinge_loss.html#sklearn.metrics.hinge_loss" title="sklearn.metrics.hinge_loss"><code class="docutils"><span class="calibre4">hinge_loss</span></code></a> 函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">est</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">LinearSVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">Y</span><span class="calibre4">)</span>
<span class="calibre4">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="calibre4">     intercept_scaling=1, loss='squared_hinge', max_iter=1000,</span>
<span class="calibre4">     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,</span>
<span class="calibre4">     verbose=0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred_decision</span> <span class="calibre4">=</span> <span class="calibre4">est</span><span class="calibre4">.</span><span class="calibre4">decision_function</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hinge_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">pred_decision</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">)</span>  
<span class="calibre4">0.56...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-768">
<span id="calibre_link-769" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.10. Log 损失</h3>
<p class="calibre2">Log loss，又被称为 logistic regression loss（logistic 回归损失）或者 cross-entropy loss（交叉熵损失） 定义在 probability estimates （概率估计）。它通常用于 (multinomial) logistic regression （（多项式）logistic 回归）和 neural networks （神经网络）以及 expectation-maximization （期望最大化）的一些变体中，并且可用于评估分类器的 probability outputs （概率输出）（<code class="docutils"><span class="calibre4">predict_proba</span></code>）而不是其 discrete predictions （离散预测）。</p>
<p class="calibre10">对于具有真实标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000314.jpg" alt="y \in \{0,1\}" /> 的 binary classification （二分类）和 probability estimate （概率估计） <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000898.jpg" alt="p = \operatorname{Pr}(y = 1)" />, 每个样本的 log loss 是给定的分类器的 negative log-likelihood 真正的标签:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000136.jpg" alt="L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))" class="math" /></p>
</div>
<p class="calibre10">这扩展到 multiclass case （多类案例）如下。
让一组样本的真实标签被编码为 1-of-K binary indicator matrix <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000206.jpg" alt="Y" />, 即 如果样本 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 具有取自一组 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> 个标签的标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> ，则 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000858.jpg" alt="y_{i,k} = 1" /> 。令 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000138.jpg" alt="P" /> 为 matrix of probability estimates （概率估计矩阵）， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000445.jpg" alt="p_{i,k} = \operatorname{Pr}(t_{i,k} = 1)" /> 。那么整套的 log loss 就是</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000170.jpg" alt="L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}^{N-1} \sum_{k=0}^{K-1} y_{i,k} \log p_{i,k}" class="math" /></p>
</div>
<p class="calibre10">为了看这这里如何 generalizes （推广）上面给出的 binary log loss （二分 log loss），请注意，在 binary case （二分情况下），<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000623.jpg" alt="p_{i,0} = 1 - p_{i,1}" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000877.jpg" alt="y_{i,0} = 1 - y_{i,1}" /> ，因此扩展 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000147.jpg" alt="y_{i,k} \in \{0,1\}" /> 的 inner sum （内部和），给出 binary log loss （二分 log loss）。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss" title="sklearn.metrics.log_loss"><code class="docutils"><span class="calibre4">log_loss</span></code></a> 函数计算出一个 a list of ground-truth labels （已标注的真实数据的标签的列表）和一个 probability matrix （概率矩阵） 的 log loss，由 estimator （估计器）的 <code class="docutils"><span class="calibre4">predict_proba</span></code> 方法返回。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">log_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">.</span><span class="calibre4">9</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">7</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">01</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">99</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">log_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>    
<span class="calibre4">0.1738...</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">y_pred</span></code> 中的第一个 <code class="docutils"><span class="calibre4">[.9,</span> <span class="calibre4">.1]</span></code> 表示第一个样本具有标签 0 的 90% 概率。log loss 是非负数。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-770">
<span id="calibre_link-771" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.11. 马修斯相关系数</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="docutils"><span class="calibre4">matthews_corrcoef</span></code></a> 函数用于计算 binary classes （二分类）的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Matthew’s correlation coefficient (MCC)</a> 引用自 Wikipedia:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">“Matthews correlation coefficient（马修斯相关系数）用于机器学习，作为 binary (two-class) classifications （二分类）分类质量的度量。它考虑到 true and false positives and negatives （真和假的 positives 和 negatives），通常被认为是可以使用的 balanced measure（平衡措施），即使 classes are of very different sizes （类别大小不同）。MCC 本质上是 -1 和 +1 之间的相关系数值。系数 +1 表示完美预测，0 表示平均随机预测， -1 表示反向预测。statistic （统计量）也称为 phi coefficient （phi）系数。”</div>
</blockquote>
<p class="calibre10">在 binary (two-class) （二分类）情况下，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000226.jpg" alt="tp" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000247.jpg" alt="tn" />, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000717.jpg" alt="fp" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000847.jpg" alt="fn" /> 分别是 true positives, true negatives, false positives 和 false negatives 的数量，MCC 定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000748.jpg" alt="MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}." class="math" /></p>
</div>
<p class="calibre10">在 multiclass case （多类的情况）下， Matthews correlation coefficient（马修斯相关系数） 可以根据 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> classes （类）的 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="docutils"><span class="calibre4">confusion_matrix</span></code></a> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000125.jpg" alt="C" /> 定义 <a class="calibre3 pcalibre" href="http://rk.kvl.dk/introduction/index.html">defined</a> 。为了简化定义，考虑以下中间变量:</p>
<ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000266.jpg" alt="t_k=\sum_{i}^{K} C_{ik}" /> 真正发生了 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 类的次数,</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000017.jpg" alt="p_k=\sum_{i}^{K} C_{ki}" /> <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 类被预测的次数,</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000784.jpg" alt="c=\sum_{k}^{K} C_{kk}" /> 正确预测的样本总数,</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000530.jpg" alt="s=\sum_{i}^{K} \sum_{j}^{K} C_{ij}" /> 样本总数.</li>
</ul>
<p class="calibre10">然后 multiclass MCC 定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000235.jpg" alt="MCC = \frac{     c \times s - \sum_{k}^{K} p_k \times t_k }{\sqrt{     (s^2 - \sum_{k}^{K} p_k^2) \times     (s^2 - \sum_{k}^{K} t_k^2) }}" class="math" /></p>
</div>
<p class="calibre10">当有两个以上的标签时， MCC 的值将不再在 -1 和 +1 之间。相反，根据已经标注的真实数据的数量和分布情况，最小值将介于 -1 和 0 之间。最大值始终为 +1 。</p>
<p class="calibre10">这是一个小例子，说明了使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.matthews_corrcoef.html#sklearn.metrics.matthews_corrcoef" title="sklearn.metrics.matthews_corrcoef"><code class="docutils"><span class="calibre4">matthews_corrcoef</span></code></a> 函数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">matthews_corrcoef</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">+</span><span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">matthews_corrcoef</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">-0.33...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-772">
<span id="calibre_link-773" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.12. Receiver operating characteristic (ROC)</h3>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="docutils"><span class="calibre4">roc_curve</span></code></a> 计算 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic curve, or ROC curve</a>.
引用 Wikipedia :</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">“A receiver operating characteristic (ROC), 或者简单的 ROC 曲线，是一个图形图，说明了 binary classifier （二分分类器）系统的性能，因为 discrimination threshold （鉴别阈值）是变化的。它是通过在不同的阈值设置下，从 true positives out of the positives (TPR = true positive 比例) 与 false positives out of the negatives (FPR = false positive 比例) 绘制 true positive 的比例来创建的。 TPR 也称为 sensitivity（灵敏度），FPR 是减去 specificity（特异性） 或 true negative 比例。”</div>
</blockquote>
<p class="calibre10">该函数需要真正的 binar value （二分值）和 target scores（目标分数），这可以是 positive class 的 probability estimates （概率估计），confidence values（置信度值）或 binary decisions（二分决策）。
这是一个如何使用 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="docutils"><span class="calibre4">roc_curve</span></code></a> 函数的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">roc_curve</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.4</span><span class="calibre4">,</span> <span class="calibre4">0.35</span><span class="calibre4">,</span> <span class="calibre4">0.8</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">fpr</span><span class="calibre4">,</span> <span class="calibre4">tpr</span><span class="calibre4">,</span> <span class="calibre4">thresholds</span> <span class="calibre4">=</span> <span class="calibre4">roc_curve</span><span class="calibre4">(</span><span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">scores</span><span class="calibre4">,</span> <span class="calibre4">pos_label</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">fpr</span>
<span class="calibre4">array([ 0. ,  0.5,  0.5,  1. ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tpr</span>
<span class="calibre4">array([ 0.5,  0.5,  1. ,  1. ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">thresholds</span>
<span class="calibre4">array([ 0.8 ,  0.4 ,  0.35,  0.1 ])</span>
</pre>
</div>
</div>
<p class="calibre10">该图显示了这样的 ROC 曲线的示例:</p>
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_roc.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_roc_0011.png" class="calibre27" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000564.jpg" /></a>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">roc_auc_score</span></code></a> 函数计算 receiver operating characteristic (ROC) 曲线下的面积，也由 AUC 和 AUROC 表示。通过计算 roc 曲线下的面积，曲线信息总结为一个数字。
有关更多的信息，请参阅 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">Wikipedia article on AUC</a> .</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">roc_auc_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_scores</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.4</span><span class="calibre4">,</span> <span class="calibre4">0.35</span><span class="calibre4">,</span> <span class="calibre4">0.8</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">roc_auc_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_scores</span><span class="calibre4">)</span>
<span class="calibre4">0.75</span>
</pre>
</div>
</div>
<p class="calibre10">在 multi-label classification （多标签分类）中， <a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">roc_auc_score</span></code></a> 函数通过在标签上进行平均来扩展 <a class="calibre3 pcalibre" href="#calibre_link-175"><span class="calibre4">above</span></a> .</p>
<p class="calibre10">与诸如 subset accuracy （子集精确度），Hamming loss（汉明损失）或 F1 score 的 metrics（指标）相比， ROC 不需要优化每个标签的阈值。<a class="calibre3 pcalibre" href="generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="docutils"><span class="calibre4">roc_auc_score</span></code></a> 函数也可以用于 multi-class classification （多类分类），如果预测的输出被 binarized （二分化）。</p>
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_roc.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_roc_0021.png" class="calibre27" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000382.jpg" /></a>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py"><span class="calibre4">Receiver Operating Characteristic (ROC)</span></a>
例如使用 ROC 来评估分类器输出的质量。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py"><span class="calibre4">Receiver Operating Characteristic (ROC) with cross validation</span></a>
例如使用 ROC 来评估分类器输出质量，使用 cross-validation （交叉验证）。</li>
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_species_distribution_modeling.html#sphx-glr-auto-examples-applications-plot-species-distribution-modeling-py"><span class="calibre4">Species distribution modeling</span></a>
例如使用 ROC 来 model species distribution 模拟物种分布。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-174">
<span id="calibre_link-774" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.13. 零一损失</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="docutils"><span class="calibre4">zero_one_loss</span></code></a> 函数通过 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 计算 0-1 classification loss (<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000151.jpg" alt="L_{0-1}" />) 的 sum （和）或 average （平均值）。默认情况下，函数在样本上 normalizes （标准化）。要获得 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000151.jpg" alt="L_{0-1}" /> 的总和，将 <code class="docutils"><span class="calibre4">normalize</span></code> 设置为 <code class="docutils"><span class="calibre4">False</span></code>。</p>
<p class="calibre10">在 multilabel classification （多标签分类）中，如果零标签与标签严格匹配，则 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="docutils"><span class="calibre4">zero_one_loss</span></code></a> 将一个子集作为一个子集，如果有任何错误，则为零。默认情况下，函数返回不完全预测子集的百分比。为了得到这样的子集的计数，将 <code class="docutils"><span class="calibre4">normalize</span></code> 设置为 <code class="docutils"><span class="calibre4">False</span></code> 。</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是第 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" /> 个样本的预测值，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是相应的真实值，则 0-1 loss <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000151.jpg" alt="L_{0-1}" /> 定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000218.jpg" alt="L_{0-1}(y_i, \hat{y}_i) = 1(\hat{y}_i \not= y_i)" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000388.jpg" alt="1(x)" /> 是 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Indicator_function">indicator function</a>.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">zero_one_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">zero_one_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.25</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">zero_one_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">normalize</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">1</span>
</pre>
</div>
</div>
<p class="calibre10">在具有 binary label indicators （二分标签指示符）的 multilabel （多标签）情况下，第一个标签集 [0,1] 有错误:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">zero_one_loss</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]),</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)))</span>
<span class="calibre4">0.5</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">zero_one_loss</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]),</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">ones</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)),</span>  <span class="calibre4">normalize</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">1</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">参阅 <a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"><span class="calibre4">Recursive feature elimination with cross-validation</span></a>
例如 zero one loss 使用以通过 cross-validation （交叉验证）执行递归特征消除。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-775">
<span id="calibre_link-776" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.2.14. Brier 分数损失</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss" title="sklearn.metrics.brier_score_loss"><code class="docutils"><span class="calibre4">brier_score_loss</span></code></a> 函数计算二进制类的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Brier_score">Brier 分数</a> 。引用维基百科：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">“Brier 分数是一个特有的分数函数，用于衡量概率预测的准确性。它适用于预测必须将概率分配给一组相互排斥的离散结果的任务。”</div>
</blockquote>
<p class="calibre10">该函数返回的是 实际结果与可能结果 的预测概率之间均方差的得分。 实际结果必须为1或0（真或假），而实际结果的预测概率可以是0到1之间的值。</p>
<p class="calibre10">Brier 分数损失也在0到1之间，分数越低（均方差越小），预测越准确。它可以被认为是对一组概率预测的 “校准” 的度量。</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000141.jpg" alt="BS = \frac{1}{N} \sum_{t=1}^{N}(f_t - o_t)^2" class="math" /></p>
</div>
<p class="calibre10">其中: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 是预测的总数， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000714.jpg" alt="f_t" /> 是实际结果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000345.jpg" alt="o_t" /> 的预测概率。</p>
<p class="calibre10">这是一个使用这个函数的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">brier_score_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true_categorical</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">"spam"</span><span class="calibre4">,</span> <span class="calibre4">"ham"</span><span class="calibre4">,</span> <span class="calibre4">"ham"</span><span class="calibre4">,</span> <span class="calibre4">"spam"</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_prob</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.9</span><span class="calibre4">,</span> <span class="calibre4">0.8</span><span class="calibre4">,</span> <span class="calibre4">0.4</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">brier_score_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_prob</span><span class="calibre4">)</span>
<span class="calibre4">0.055</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">brier_score_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">-</span><span class="calibre4">y_prob</span><span class="calibre4">,</span> <span class="calibre4">pos_label</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">0.055</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">brier_score_loss</span><span class="calibre4">(</span><span class="calibre4">y_true_categorical</span><span class="calibre4">,</span> <span class="calibre4">y_prob</span><span class="calibre4">,</span> <span class="calibre4">pos_label</span><span class="calibre4">=</span><span class="calibre4">"ham"</span><span class="calibre4">)</span>
<span class="calibre4">0.055</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">brier_score_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_prob</span> <span class="calibre4">&gt;</span> <span class="calibre4">0.5</span><span class="calibre4">)</span>
<span class="calibre4">0.0</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">请参阅分类器的概率校准 <a class="calibre3 pcalibre" href="../auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py"><span class="calibre4">Probability calibration of classifiers</span></a> ，通过 Brier 分数损失使用示例 来执行分类器的概率校准。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><ol class="first3" start="7">
<li class="toctree-l">Brier, <a class="calibre3 pcalibre" href="http://docs.lib.noaa.gov/rescue/mwr/078/mwr-078-01-0001.pdf">以概率表示的预测验证</a> , 月度天气评估78.1（1950）</li>
</ol>
</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-170">
<span id="calibre_link-777" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.3. 多标签排名指标</h2>
<p class="calibre2">在多分类学习中，每个样本可以具有与其相关联的任何数量的真实标签。目标是给予高分，更好地评价真实标签。</p>
<div class="toctree-wrapper" id="calibre_link-778">
<span id="calibre_link-779" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.3.1. 覆盖误差</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error" title="sklearn.metrics.coverage_error"><code class="docutils"><span class="calibre4">coverage_error</span></code></a> 函数计算必须包含在最终预测中的标签的平均数，以便预测所有真正的标签。
如果您想知道有多少 top 评分标签，您必须通过平均来预测，而不会丢失任何真正的标签，这很有用。
因此，此指标的最佳价值是真正标签的平均数量。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">我们的实现的分数比 Tsoumakas 等人在2010年的分数大1。
这扩展了它来处理一个具有0个真实标签实例的退化情况。</p>
</div>
<p class="calibre10">正式地，给定真实标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000451.jpg" alt="y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}" /> 的二进制指示矩阵和与每个标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000768.jpg" alt="\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}" /> 相关联的分数，覆盖范围被定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000317.jpg" alt="coverage(y, \hat{f}) = \frac{1}{n_{\text{samples}}}   \sum_{i=0}^{n_{\text{samples}} - 1} \max_{j:y_{ij} = 1} \text{rank}_{ij}" class="math" /></p>
</div>
<p class="calibre10">与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000496.jpg" alt="\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|" /> 。给定等级定义，通过给出将被分配给所有绑定值的最大等级， <code class="docutils"><span class="calibre4">y_scores</span></code> 中的关系会被破坏。</p>
<p class="calibre10">这是一个使用这个函数的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">coverage_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_score</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0.75</span><span class="calibre4">,</span> <span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">coverage_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_score</span><span class="calibre4">)</span>
<span class="calibre4">2.5</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-780">
<span id="calibre_link-781" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.3.2. 标签排名平均精度</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score" title="sklearn.metrics.label_ranking_average_precision_score"><code class="docutils"><span class="calibre4">label_ranking_average_precision_score</span></code></a> 函数实现标签排名平均精度（LRAP）。
该度量值与 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="docutils"><span class="calibre4">average_precision_score</span></code></a> 函数相关联，但是基于标签排名的概念，而不是精确度和召回。</p>
<p class="calibre10">标签排名平均精度（LRAP）是分配给每个样本的每个真实标签的平均值，真实对总标签与较低分数的比率。
如果能够为每个样本相关标签提供更好的排名，这个指标就会产生更好的分数。
获得的得分总是严格大于0，最佳值为1。如果每个样本只有一个相关标签，则标签排名平均精度等于 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">平均倒数等级</a> 。</p>
<p class="calibre10">正式地，给定真实标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000249.jpg" alt="y \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}" /> 的二进制指示矩阵和与每个标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000510.jpg" alt="\hat{f} \in \mathcal{R}^{n_\text{samples} \times n_\text{labels}}" /> 相关联的得分，平均精度被定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000792.jpg" alt="LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}   \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|}   \sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}" class="math" /></p>
</div>
<p class="calibre10">与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000105.jpg" alt="\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}" />，
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000496.jpg" alt="\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000113.jpg" alt="|\cdot|" /> 是集合的 l0 范数或基数。</p>
<p class="calibre10">这是一个使用这个函数的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">label_ranking_average_precision_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_score</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0.75</span><span class="calibre4">,</span> <span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">label_ranking_average_precision_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_score</span><span class="calibre4">)</span> 
<span class="calibre4">0.416...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-782">
<span id="calibre_link-783" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.3.3. 排序损失</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss" title="sklearn.metrics.label_ranking_loss"><code class="docutils"><span class="calibre4">label_ranking_loss</span></code></a> 函数计算在样本上平均排序错误的标签对数量的排序损失，即真实标签的分数低于假标签，由虚假和真实标签的倒数加权。最低可实现的排名损失为零。</p>
<p class="calibre10">正式地，给定真相标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000451.jpg" alt="y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}" /> 的二进制指示矩阵和与每个标签 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000768.jpg" alt="\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}" /> 相关联的得分，排序损失被定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000410.jpg" alt="\text{ranking\_loss}(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}   \sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{|y_i|(n_\text{labels} - |y_i|)}   \left|\left\{(k, l): \hat{f}_{ik} &lt; \hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \right\}\right|" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000113.jpg" alt="|\cdot|" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000695.jpg" alt="\ell_0" /> 范数或集合的基数。</p>
<p class="calibre10">这是一个使用这个函数的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">label_ranking_loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_score</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0.75</span><span class="calibre4">,</span> <span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">label_ranking_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_score</span><span class="calibre4">)</span> 
<span class="calibre4">0.75...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># With the following prediction, we have perfect and minimal loss</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_score</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1.0</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.2</span><span class="calibre4">,</span> <span class="calibre4">0.9</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">label_ranking_loss</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_score</span><span class="calibre4">)</span>
<span class="calibre4">0.0</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010). 挖掘多标签数据。在数据挖掘和知识发现手册（第667-685页）。美国 Springer.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-171">
<span id="calibre_link-784" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.4. 回归指标</h2>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="docutils"><span class="calibre4">sklearn.metrics</span></code></a> 模块实现了一些 loss, score 以及 utility 函数以测量 regression（回归）的性能.
其中一些已经被加强以处理多个输出的场景: <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="docutils"><span class="calibre4">mean_squared_error</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="docutils"><span class="calibre4">mean_absolute_error</span></code></a>, <a class="calibre3 pcalibre" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="docutils"><span class="calibre4">explained_variance_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">r2_score</span></code></a>.</p>
<p class="calibre10">这些函数有 <code class="docutils"><span class="calibre4">multioutput</span></code> 这样一个 keyword（关键的）参数, 它指定每一个目标的 score（得分）或 loss（损失）的平均值的方式.
默认是 <code class="docutils"><span class="calibre4">'uniform_average'</span></code>, 其指定了输出时一致的权重均值.
如果一个 <code class="docutils"><span class="calibre4">ndarray</span></code> 的 shape <code class="docutils"><span class="calibre4">(n_outputs,)</span></code> 被传递, 则其中的 entries（条目）将被解释为权重，并返回相应的加权平均值.
如果 <code class="docutils"><span class="calibre4">multioutput</span></code> 指定了 <code class="docutils"><span class="calibre4">'raw_values'</span></code> , 则所有未改变的部分 score（得分）或 loss（损失）将以 <code class="docutils"><span class="calibre4">(n_outputs,)</span></code> 形式的数组返回.</p>
<p class="calibre10">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">r2_score</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="docutils"><span class="calibre4">explained_variance_score</span></code></a> 函数接受一个额外的值 <code class="docutils"><span class="calibre4">'variance_weighted'</span></code> 用于 <code class="docutils"><span class="calibre4">multioutput</span></code> 参数.
该选项通过相应目标变量的方差使得每个单独的 score 进行加权.
该设置量化了全局捕获的未缩放方差.
如果目标变量的大小不一样, 则该 score 更好地解释了较高的方差变量.
<code class="docutils"><span class="calibre4">multioutput='variance_weighted'</span></code> 是 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">r2_score</span></code></a> 的默认值以向后兼容.
以后该值会被改成 <code class="docutils"><span class="calibre4">uniform_average</span></code>.</p>
<div class="toctree-wrapper" id="calibre_link-785">
<span id="calibre_link-786" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.1. 解释方差得分</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="docutils"><span class="calibre4">explained_variance_score</span></code></a> 函数计算了 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Explained_variation">explained variance
regression score（解释的方差回归得分）</a>.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000505.jpg" alt="\hat{y}" /> 是预估的目标输出, <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 是相应（正确的）目标输出, 并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000407.jpg" alt="Var" /> is <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Variance">方差</a>, 标准差的平方, 那么解释的方差预估如下:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000629.jpg" alt="\texttt{explained\_{}variance}(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}" class="math" /></p>
</div>
<p class="calibre10">最好的得分是 1.0, 值越低越差.</p>
<p class="calibre10">下面是一下有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score" title="sklearn.metrics.explained_variance_score"><code class="docutils"><span class="calibre4">explained_variance_score</span></code></a> 函数使用的一些例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">explained_variance_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">explained_variance_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.957...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">5</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">explained_variance_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">'raw_values'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">array([ 0.967...,  1.        ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">explained_variance_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.3</span><span class="calibre4">,</span> <span class="calibre4">0.7</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">0.990...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-787">
<span id="calibre_link-788" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.2. 平均绝对误差</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="docutils"><span class="calibre4">mean_absolute_error</span></code></a> 函数计算了 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Mean_absolute_error">平均绝对误差</a>, 一个对应绝对误差损失预期值或者 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000508.jpg" alt="l1" />-norm 损失的风险度量.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />-th 样本的预测值,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是对应的真实值, 则平均绝对误差 (MAE) 预估的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 定义如下</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000653.jpg" alt="\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|." class="math" /></p>
</div>
<p class="calibre10">下面是一个有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error" title="sklearn.metrics.mean_absolute_error"><code class="docutils"><span class="calibre4">mean_absolute_error</span></code></a> 函数用法的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">mean_absolute_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_absolute_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.5</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">5</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_absolute_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.75</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_absolute_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">'raw_values'</span><span class="calibre4">)</span>
<span class="calibre4">array([ 0.5,  1. ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_absolute_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.3</span><span class="calibre4">,</span> <span class="calibre4">0.7</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">0.849...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-789">
<span id="calibre_link-790" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.3. 均方误差</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="docutils"><span class="calibre4">mean_squared_error</span></code></a> 函数计算了 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Mean_squared_error">均方误差</a>, 一个对应于平方（二次）误差或损失的预期值的风险度量.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />-th 样本的预测值,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是对应的真实值, 则均方误差（MSE）预估的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 定义如下</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000897.jpg" alt="\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (y_i - \hat{y}_i)^2." class="math" /></p>
</div>
<p class="calibre10">下面是一个有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="docutils"><span class="calibre4">mean_squared_error</span></code></a> 函数用法的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">mean_squared_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.375</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">5</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.7083...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Examples:</p>
<ul class="calibre6">
<li class="toctree-l">点击 <a class="calibre3 pcalibre" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="calibre4">Gradient Boosting regression</span></a>
查看均方误差用于梯度上升（gradient boosting）回归的使用例子。</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-791">
<span id="calibre_link-792" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.4. 均方误差对数</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="docutils"><span class="calibre4">mean_squared_log_error</span></code></a> 函数计算了一个对应平方对数（二次）误差或损失的预估值风险度量.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />-th 样本的预测值,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是对应的真实值, 则均方误差对数（MSLE）预估的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 定义如下</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000483.jpg" alt="\text{MSLE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )^2." class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000353.jpg" alt="\log_e (x)" /> 表示 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 的自然对数.
当目标具有指数增长的趋势时, 该指标最适合使用, 例如人口数量, 跨年度商品的平均销售额等.
请注意, 该指标会对低于预测的估计值进行估计.</p>
<p class="calibre10">下面是一个有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error" title="sklearn.metrics.mean_squared_log_error"><code class="docutils"><span class="calibre4">mean_squared_log_error</span></code></a> 函数用法的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">mean_squared_log_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_log_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.039...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2.5</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mean_squared_log_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.044...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-793">
<span id="calibre_link-794" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.5. 中位绝对误差</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="docutils"><span class="calibre4">median_absolute_error</span></code></a> 函数尤其有趣, 因为它的离群值很强.
通过取目标和预测之间的所有绝对差值的中值来计算损失.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />-th 样本的预测值,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是对应的真实值, 则中位绝对误差（MedAE）预估的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 定义如下</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000034.jpg" alt="\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid)." class="math" /></p>
</div>
<p class="calibre10">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="docutils"><span class="calibre4">median_absolute_error</span></code></a> 函数不支持多输出.</p>
<p class="calibre10">下面是一个有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error" title="sklearn.metrics.median_absolute_error"><code class="docutils"><span class="calibre4">median_absolute_error</span></code></a> 函数用法的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">median_absolute_error</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">median_absolute_error</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>
<span class="calibre4">0.5</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-795">
<span id="calibre_link-796" class="calibre4"></span><h3 class="sigil_not_in_toc1">3.3.4.6. R² score, 可决系数</h3>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">r2_score</span></code></a> 函数计算了 computes R², 即 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">可决系数</a>.
它提供了将来样本如何可能被模型预测的估量.
最佳分数为 1.0, 可以为负数（因为模型可能会更糟）.
总是预测 y 的预期值，不考虑输入特征的常数模型将得到 R^2 得分为 0.0.</p>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000560.jpg" alt="\hat{y}_i" /> 是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000654.jpg" alt="i" />-th 样本的预测值,
并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000114.jpg" alt="y_i" /> 是对应的真实值, 则 R² 得分预估的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000010.jpg" alt="n_{\text{samples}}" /> 定义如下</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000675.jpg" alt="R^2(y, \hat{y}) = 1 - \frac{\sum_{i=0}^{n_{\text{samples}} - 1} (y_i - \hat{y}_i)^2}{\sum_{i=0}^{n_\text{samples} - 1} (y_i - \bar{y})^2}" class="math" /></p>
</div>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000589.jpg" alt="\bar{y} =  \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1} y_i" />.</p>
<p class="calibre10">下面是一个有关 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score" title="sklearn.metrics.r2_score"><code class="docutils"><span class="calibre4">r2_score</span></code></a> 函数用法的小例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">r2_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">7</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">2.5</span><span class="calibre4">,</span> <span class="calibre4">0.0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">8</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">r2_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">)</span>  
<span class="calibre4">0.948...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">5</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">r2_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">'variance_weighted'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">0.938...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_true</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">8</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">5</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">r2_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">'uniform_average'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">0.936...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">r2_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">'raw_values'</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">array([ 0.965...,  0.908...])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">r2_score</span><span class="calibre4">(</span><span class="calibre4">y_true</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">multioutput</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.3</span><span class="calibre4">,</span> <span class="calibre4">0.7</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">0.925...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l">点击 <a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="calibre4">Lasso and Elastic Net for Sparse Signals</span></a>
查看关于R²用于评估在Lasso and Elastic Net on sparse signals上的使用.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-172">
<span id="calibre_link-797" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.5. 聚类指标</h2>
<p class="calibre2">该 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics" title="sklearn.metrics"><code class="docutils"><span class="calibre4">sklearn.metrics</span></code></a> 模块实现了一些 loss, score 和 utility 函数.
更多信息请参阅 <a class="calibre3 pcalibre" href="clustering.html#clustering-evaluation"><span class="calibre4">聚类性能度量</span></a> 部分, 例如聚类, 以及用于二分聚类的 <a class="calibre3 pcalibre" href="biclustering.html#biclustering-evaluation"><span class="calibre4">Biclustering 评测</span></a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-173">
<span id="calibre_link-798" class="calibre4"></span><h2 class="sigil_not_in_toc">3.3.6. 虚拟估计</h2>
<p class="calibre2">在进行监督学习的过程中，简单的 sanity check（理性检查）包括将人的估计与简单的经验法则进行比较.
<a class="calibre3 pcalibre" href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code class="docutils"><span class="calibre4">DummyClassifier</span></code></a> 实现了几种简单的分类策略:</p>
<ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">stratified</span></code> 通过在训练集类分布方面来生成随机预测.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">most_frequent</span></code> 总是预测训练集中最常见的标签.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">prior</span></code> always predicts the class that maximizes the class prior
(like <code class="docutils"><span class="calibre4">most_frequent`)</span> <span class="calibre4">and</span> <span class="calibre4">``predict_proba</span></code> returns the class prior.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">uniform</span></code> 随机产生预测.</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18"><code class="docutils"><span class="calibre4">constant</span></code> 总是预测用户提供的常量标签.</dt>
<dd class="calibre19">A major motivation of this method is F1-scoring, when the positive class
is in the minority.
这种方法的主要动机是 F1-scoring, 当 positive class（正类）较少时.</dd>
</dl>
</li>
</ul>
<p class="calibre10">请注意, 这些所有的策略, <code class="docutils"><span class="calibre4">predict</span></code> 方法彻底的忽略了输入数据!</p>
<p class="calibre10">为了说明 <a class="calibre3 pcalibre" href="generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="sklearn.dummy.DummyClassifier"><code class="docutils"><span class="calibre4">DummyClassifier</span></code></a>, 首先让我们创建一个 imbalanced dataset:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">train_test_split</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">y</span> <span class="calibre4">!=</span> <span class="calibre4">1</span><span class="calibre4">]</span> <span class="calibre4">=</span> <span class="calibre4">-</span><span class="calibre4">1</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">train_test_split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">接下来, 让我们比较一下 <code class="docutils"><span class="calibre4">SVC</span></code> 和  <code class="docutils"><span class="calibre4">most_frequent</span></code> 的准确性.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.dummy</span> <span class="calibre4">import</span> <span class="calibre4">DummyClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span> 
<span class="calibre4">0.63...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">DummyClassifier</span><span class="calibre4">(</span><span class="calibre4">strategy</span><span class="calibre4">=</span><span class="calibre4">'most_frequent'</span><span class="calibre4">,</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">DummyClassifier(constant=None, random_state=0, strategy='most_frequent')</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span>  
<span class="calibre4">0.57...</span>
</pre>
</div>
</div>
<p class="calibre10">我们看到 <code class="docutils"><span class="calibre4">SVC</span></code> 没有比一个 dummy classifier（虚拟分类器）好很多.
现在, 让我们来更改一下 kernel:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">)</span>  
<span class="calibre4">0.97...</span>
</pre>
</div>
</div>
<p class="calibre10">我们看到准确率提升到将近 100%.
建议采用交叉验证策略, 以更好地估计精度, 如果不是太耗 CPU 的话.
更多信息请参阅 <a class="calibre3 pcalibre" href="cross_validation.html#cross-validation"><span class="calibre4">交叉验证：评估估算器的表现</span></a> 部分.
此外，如果要优化参数空间，强烈建议您使用适当的方法;
更多详情请参阅 <a class="calibre3 pcalibre" href="grid_search.html#grid-search"><span class="calibre4">调整估计器的超参数</span></a> 部分.</p>
<p class="calibre10">通常来说，当分类器的准确度太接近随机情况时，这可能意味着出现了一些问题: 特征没有帮助, 超参数没有正确调整, class 不平衡造成分类器有问题等…</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.dummy.DummyRegressor.html#sklearn.dummy.DummyRegressor" title="sklearn.dummy.DummyRegressor"><code class="docutils"><span class="calibre4">DummyRegressor</span></code></a> 还实现了四个简单的经验法则来进行回归:</p>
<ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">mean</span></code> 总是预测训练目标的平均值.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">median</span></code> 总是预测训练目标的中位数.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">quantile</span></code> 总是预测用户提供的训练目标的 quantile（分位数）.</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">constant</span></code> 总是预测由用户提供的常数值.</li>
</ul>
<p class="calibre10">在以上所有的策略中, <code class="docutils"><span class="calibre4">predict</span></code> 方法完全忽略了输入数据.</p>
</div>
</div>


<div class="calibre" id="calibre_link-51">
<span id="calibre_link-799" class="calibre4"></span><h1 class="calibre5">3.4. 模型持久化</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@正版乔</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" />     
    </div>
<p class="calibre10">在训练完 scikit-learn 模型之后, 最好有一种方法来将模型持久化以备将来使用，而无需重新训练.
以下部分为您提供了有关如何使用 pickle 来持久化模型的示例.
在使用 pickle 序列化时，我们还将回顾一些安全性和可维护性方面的问题.</p>
<div class="toctree-wrapper" id="calibre_link-800">
<h2 class="sigil_not_in_toc">3.4.1. 持久化示例</h2>
<p class="calibre2">可以通过使用 Python 的内置持久化模型将训练好的模型保存在 scikit 中, 它名为 <a class="calibre3 pcalibre" href="https://docs.python.org/2/library/pickle.html">pickle</a>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">    tol=0.001, verbose=False)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">pickle</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">s</span> <span class="calibre4">=</span> <span class="calibre4">pickle</span><span class="calibre4">.</span><span class="calibre4">dumps</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">pickle</span><span class="calibre4">.</span><span class="calibre4">loads</span><span class="calibre4">(</span><span class="calibre4">s</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">:</span><span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">array([0])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">0</span>
</pre>
</div>
</div>
<p class="calibre10">在这个 scikit 的特殊示例中，使用 joblib 来替换 pickle (<code class="docutils"><span class="calibre4">joblib.dump</span></code> &amp; <code class="docutils"><span class="calibre4">joblib.load</span></code>) 可能会更有意思, 这对于内部带有大量数组的对象来说更为高效, 通常情况下适合 scikit-learn estimators（预估器）, but can only pickle to the disk and not to a string:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.externals</span> <span class="calibre4">import</span> <span class="calibre4">joblib</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">joblib</span><span class="calibre4">.</span><span class="calibre4">dump</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">'filename.pkl'</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<p class="calibre10">之后你可以使用以下方式加载 pickled model（可能在另一个 Python 进程中）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">joblib</span><span class="calibre4">.</span><span class="calibre4">load</span><span class="calibre4">(</span><span class="calibre4">'filename.pkl'</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">joblib.dump</span></code> 和 <code class="docutils"><span class="calibre4">joblib.load</span></code> 函数也接收类似 file 的对象而不是文件名.
有关使用 Joblib 来持久化数据的更多信息可以参阅 <a class="calibre3 pcalibre" href="https://pythonhosted.org/joblib/persistence.html">这里</a>.</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-801">
<span id="calibre_link-802" class="calibre4"></span><h2 class="sigil_not_in_toc">3.4.2. 安全性和可维护性的局限性</h2>
<p class="calibre2">pickle (和通过扩展的 joblib), 在安全性和可维护性方面存在一些问题.
由于以下原因,</p>
<ul class="calibre6">
<li class="toctree-l">不要打开不受信任的数据, 因为它可能导致恶意代码在加载时执行.</li>
<li class="toctree-l">虽然使用一个版本的 scikit-learn 保存的模型可能会在其他版本中加载，但这完全不受支持并且也不合适.
还应该记住, 对这些数据执行的操作可能会产生不同和意想不到的结果.</li>
</ul>
<p class="calibre10">为了用将来版本的 scikit-learn 来重构类似的模型, 额外的元数据应该随着 pickled model 一起被保存：</p>
<ul class="calibre6">
<li class="toctree-l">训练数据, 例如. 引用不可变的快照</li>
<li class="toctree-l">用于生成模型更多 python 源代码</li>
<li class="toctree-l">scikit-learn 以及它的 dependencies 的版本</li>
<li class="toctree-l">在训练数据的基础上获得的交叉验证得分</li>
</ul>
<p class="calibre10">这样可以检查交叉验证得分是否与以前的范围相同.</p>
<p class="calibre10">由于模型内部表示可能在两种不同架构上不一样, 因此不支持在一个架构上转储模型并将其加载到另一个体系架构上.</p>
<p class="calibre10">如果您想要了解关于这些 issues 以及浏览其它可能的序列化方法的更多详情，请参阅这个
<a class="calibre3 pcalibre" href="http://pyvideo.org/video/2566/pickles-are-for-delis-not-software">Alex Gaynor 的演讲</a>.</p>
</div>
</div>


<div class="calibre" id="calibre_link-249">
<span id="calibre_link-803" class="calibre4"></span><h1 class="calibre5">3.5. 验证曲线: 绘制分数以评估模型</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@正版乔</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Xi</a><br class="calibre9" /> 
    </div>
<p class="calibre10">每种估计器都有其优势和缺陷。它的泛化误差可以用偏差、方差和噪声来分解。估计值的偏差
<strong class="calibre14">bias</strong> 是不同训练集的平均误差。估计值的方差 <strong class="calibre14">variance</strong> 用来表示它对训练集的
变化有多敏感。噪声是数据的一个属性。</p>
<p class="calibre10">在下面的图中，我们可以看到一个函数 <a href="#calibre_link-250" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-804">:math:`f(x) = \cos (\frac{3}{2} \pi x)`_</span></a> 和一些来自该函数的的带噪声数据的样本。 我们用三个不同的估计来拟合函数：多项式特征为 1,4 和
15 的线性回归。我们看到，第一个估计最多只能为样本和真正的函数提供一个很差的拟合
，因为它太简单了(高偏差），第二个估计几乎完全近似，最后一个估计完全接近训练数据，
但不能很好地拟合真实的函数，即对训练数据的变化（高方差）非常敏感。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_underfitting_overfitting.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_underfitting_overfitting_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000553.jpg" class="calibre57" /></a>
</div>
<p class="calibre10">偏差和方差是估计的固有属性，我们通常必须选择合适的学习算法和超参数，以使偏差和
方差都尽可能的低（参见偏差方差困境 <a href="#calibre_link-251" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-805">`Bias-variance dilemma&lt;https://en.wikipedia.org/wiki/Bias-variance_dilemma&gt;`_</span></a>）。
减少模型方差的另一种方法是使用更多的训练数据。 如果真实函数过于复杂，不能用一个方
差较小的估计来近似，则只能去收集更多的训练数据。</p>
<p class="calibre10">在示例里简单的一维问题中，我们可以很容易看出估计是否存在偏差或方差。 然而，在高维空间中，
模型可能变得非常难以可视化。 因此，使用下面所描述的工具通常是有帮助的。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py"><span class="calibre4">Underfitting vs. Overfitting</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py"><span class="calibre4">Plotting Validation Curves</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py"><span class="calibre4">Plotting Learning Curves</span></a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-806">
<span id="calibre_link-807" class="calibre4"></span><h2 class="sigil_not_in_toc">3.5.1. 验证曲线</h2>
<p class="calibre2">为了验证一个模型，我们需要一个评分函数（参见模型评估：<a class="calibre3 pcalibre" href="model_evaluation.html#model-evaluation"><span class="calibre4">模型评估: 量化预测的质量</span></a>），
例如分类器的准确性。 选择估计器的多个超参数的正确方法当然是网格搜索或类似方法
（参见调整估计的超参数 <a class="calibre3 pcalibre" href="grid_search.html#grid-search"><span class="calibre4">调整估计器的超参数</span></a> ），其选择一个或多个验证集上的分数最高的超参数。
请注意，如果我们基于验证分数优化了超参数，则验证分数就有偏差了，并且不再是一个良好的泛化估计。
为了得到正确的泛化估计，我们必须在另一个测试集上计算得分。</p>
<p class="calibre10">然而，绘制单个超参数对训练分数和验证分数的影响,有时有助于发现该估计是否因为某些超参数
而出现过拟合或欠拟合。</p>
<p class="calibre10">本例中,下面的方程 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve" title="sklearn.model_selection.validation_curve"><code class="docutils"><span class="calibre4">validation_curve</span></code></a> 能起到如下作用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">validation_curve</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">Ridge</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">seed</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">indices</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">y</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">shuffle</span><span class="calibre4">(</span><span class="calibre4">indices</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">],</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">train_scores</span><span class="calibre4">,</span> <span class="calibre4">valid_scores</span> <span class="calibre4">=</span> <span class="calibre4">validation_curve</span><span class="calibre4">(</span><span class="calibre4">Ridge</span><span class="calibre4">(),</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">"alpha"</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                              <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">logspace</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">train_scores</span>           
<span class="calibre4">array([[ 0.94...,  0.92...,  0.92...],</span>
<span class="calibre4">       [ 0.94...,  0.92...,  0.92...],</span>
<span class="calibre4">       [ 0.47...,  0.45...,  0.42...]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">valid_scores</span>           
<span class="calibre4">array([[ 0.90...,  0.92...,  0.94...],</span>
<span class="calibre4">       [ 0.90...,  0.92...,  0.94...],</span>
<span class="calibre4">       [ 0.44...,  0.39...,  0.45...]])</span>
</pre>
</div>
</div>
<p class="calibre10">如果训练得分和验证得分都很低，则估计器是欠拟合的。 如果训练得分高，验证得分低，则估计器过拟合，
否则估计会拟合得很好。 通常不可能有较低的训练得分和较高的验证得分。 所有三种情况都可以
在下面的图中找到，其中我们改变了数字数据集上 SVM 的参数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000566.jpg" alt="\gamma" /> 。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_validation_curve.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_validation_curve_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000243.jpg" class="calibre11" /></a>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-808">
<span id="calibre_link-809" class="calibre4"></span><h2 class="sigil_not_in_toc">3.5.2. 学习曲线</h2>
<p class="calibre2">学习曲线显示了对于不同数量的训练样本的估计器的验证和训练评分。它可以帮助我们发现从增加更多的训
练数据中能获益多少，以及估计是否受方差误差或偏差误差的影响更大。如果验证分数和训练
分数都收敛到一个相对于增加的训练集大小来说过低的值，那么我们将不能从更多的训练数据
中获益。在下面的图中看到一个例子：朴素贝叶斯大致收敛到一个较低的分数。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_learning_curve.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_learning_curve_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000507.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">我们可能需要使用可以学习更复杂概念（即具有较低偏差）的当前估计的一个估计值或参数化。
如果训练样本的最大时,训练分数比验证分数得分大得多，那么增加训练样本很可能会增加泛化能力。
在下面的图中，可以看到支持向量机可以从更多的训练样本中获益。</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/model_selection/plot_learning_curve.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_learning_curve_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000488.jpg" class="calibre11" /></a>
</div>
<p class="calibre10">我们可以使用函数 <a class="calibre3 pcalibre" href="generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve" title="sklearn.model_selection.learning_curve"><code class="docutils"><span class="calibre4">learning_curve</span></code></a> 绘制这样的学习曲线（已使用的样本数，训练集
上的平均分数和验证集上的平均分数）所需的值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">learning_curve</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">train_sizes</span><span class="calibre4">,</span> <span class="calibre4">train_scores</span><span class="calibre4">,</span> <span class="calibre4">valid_scores</span> <span class="calibre4">=</span> <span class="calibre4">learning_curve</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">),</span> <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">train_sizes</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">80</span><span class="calibre4">,</span> <span class="calibre4">110</span><span class="calibre4">],</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">train_sizes</span>            
<span class="calibre4">array([ 50, 80, 110])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">train_scores</span>           
<span class="calibre4">array([[ 0.98...,  0.98 ,  0.98...,  0.98...,  0.98...],</span>
<span class="calibre4">       [ 0.98...,  1.   ,  0.98...,  0.98...,  0.98...],</span>
<span class="calibre4">       [ 0.98...,  1.   ,  0.98...,  0.98...,  0.99...]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">valid_scores</span>           
<span class="calibre4">array([[ 1. ,  0.93...,  1. ,  1. ,  0.96...],</span>
<span class="calibre4">       [ 1. ,  0.96...,  1. ,  1. ,  0.96...],</span>
<span class="calibre4">       [ 1. ,  0.96...,  1. ,  1. ,  0.96...]])</span>
</pre>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-66">
<span id="calibre_link-810" class="calibre4"></span><h1 class="calibre5">4. 数据集转换</h1>
<p class="calibre2">scikit-learn 提供了一个用于转换数据集的库, 它也许会 clean（清理）（请参阅
<a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing"><span class="calibre4">预处理数据</span></a>）, reduce（减少）（请参阅 <a class="calibre3 pcalibre" href="modules/unsupervised_reduction.html#data-reduction"><span class="calibre4">无监督降维</span></a>）, expand（扩展）（请参阅
<a class="calibre3 pcalibre" href="modules/kernel_approximation.html#kernel-approximation"><span class="calibre4">内核近似</span></a>）或 generate（生成）（请参阅 <a class="calibre3 pcalibre" href="modules/feature_extraction.html#feature-extraction"><span class="calibre4">特征提取</span></a>）
feature representations（特征表示）.</p>
<p class="calibre10">像其它预估计一样, 它们由具有 <code class="docutils"><span class="calibre4">fit</span></code> 方法的类来表示,
该方法从训练集学习模型参数（例如, 归一化的平均值和标准偏差）以及将该转换模型应用于 <code class="docutils"><span class="calibre4">transform</span></code> 方法到不可见数据.
同时 <code class="docutils"><span class="calibre4">fit_transform</span></code> 可以更方便和有效地建模与转换训练数据.</p>
<p class="calibre10">将 <a class="calibre3 pcalibre" href="modules/pipeline.html#combining-estimators"><span class="calibre4">Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</span></a> 中 transformers（转换）使用并行的或者串联的方式合并到一起.
<a class="calibre3 pcalibre" href="modules/metrics.html#metrics"><span class="calibre4">成对的矩阵, 类别和核函数</span></a> 涵盖将特征空间转换为 affinity matrices（亲和矩阵）,
而  <a class="calibre3 pcalibre" href="modules/preprocessing_targets.html#preprocessing-targets"><span class="calibre4">预测目标 (y) 的转换</span></a> 考虑在 scikit-learn 中使用目标空间的转换（例如. 标签分类）.</p>
<div class="toctree-wrapper">
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#pipeline">4.1.1. Pipeline: 链式评估器</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#id8">4.1.1.1. 用法</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#id9">4.1.1.2. 注意点</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#pipeline-cache">4.1.1.3. 缓存转换器：避免重复计算</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#featureunion">4.1.2. FeatureUnion（特征联合）: 个特征层面</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/pipeline.html#id11">4.1.2.1. 用法</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html">4.2. 特征提取</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#dict-feature-extraction">4.2.1. 从字典类型加载特征</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#feature-hashing">4.2.2. 特征哈希（相当于一种降维技巧）</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id4">4.2.2.1. 实现细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#text-feature-extraction">4.2.3. 文本特征提取</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id7">4.2.3.1. 话语表示</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id8">4.2.3.2. 稀疏</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#vectorizer">4.2.3.3. 常用 Vectorizer 使用</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#tfidf">4.2.3.4. Tf&ndash;idf 项加权</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id10">4.2.3.5. 解码文本文件</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id11">4.2.3.6. 应用和实例</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id12">4.2.3.7. 词语表示的限制</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#hashing-vectorizer">4.2.3.8. 用哈希技巧矢量化大文本语料库</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#hashingvectorizer">4.2.3.9. 使用 HashingVectorizer 执行外核缩放</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id14">4.2.3.10. 自定义矢量化器类</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#image-feature-extraction">4.2.4. 图像特征提取</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id16">4.2.4.1. 补丁提取</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/feature_extraction.html#id17">4.2.4.2. 图像的连接图</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html">4.3. 预处理数据</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing-scaler">4.3.1. 标准化，也称去均值和方差按比例缩放</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#id3">4.3.1.1. 将特征缩放至特定范围内</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#id4">4.3.1.2. 缩放稀疏（矩阵）数据</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#id5">4.3.1.3. 缩放有离群值的数据</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#kernel-centering">4.3.1.4. 核矩阵的中心化</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing-transformer">4.3.2. 非线性转换</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing-normalization">4.3.3. 归一化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing-binarization">4.3.4. 二值化</a><ul class="calibre8">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#id10">4.3.4.1. 特征二值化</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#preprocessing-categorical-features">4.3.5. 分类特征编码</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#imputation">4.3.6. 缺失值插补</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#polynomial-features">4.3.7. 生成多项式特征</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing.html#function-transformer">4.3.8. 自定义转换器</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/unsupervised_reduction.html">4.4. 无监督降维</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/unsupervised_reduction.html#pca">4.4.1. PCA: 主成份分析</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/unsupervised_reduction.html#id2">4.4.2. 随机投影</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/unsupervised_reduction.html#id3">4.4.3. 特征聚集</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/random_projection.html">4.5. 随机投影</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/random_projection.html#johnson-lindenstrauss">4.5.1. Johnson-Lindenstrauss 辅助定理</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/random_projection.html#gaussian-random-matrix">4.5.2. 高斯随机投影</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/random_projection.html#sparse-random-matrix">4.5.3. 稀疏随机矩阵</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html">4.6. 内核近似</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html#nystroem">4.6.1. 内核近似的 Nystroem 方法</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html#rbf-kernel-approx">4.6.2. 径向基函数内核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html#additive-chi-kernel-approx">4.6.3. 加性卡方核</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html#skewed-chi-squared-kernel">4.6.4. Skewed Chi Squared Kernel (偏斜卡方核?暂译)</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/kernel_approximation.html#id8">4.6.5. 数学方面的细节</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html">4.7. 成对的矩阵, 类别和核函数</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#cosine-similarity">4.7.1. 余弦相似度</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#linear-kernel">4.7.2. 线性核函数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#polynomial-kernel">4.7.3. 多项式核函数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#sigmoid">4.7.4. Sigmoid 核函数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#rbf">4.7.5. RBF 核函数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#laplacian-kernel">4.7.6. 拉普拉斯核函数</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/metrics.html#chi2-kernel">4.7.7. 卡方核函数</a></li>
</ul>
</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing_targets.html">4.8. 预测目标 (<code class="docutils"><span class="calibre4">y</span></code>) 的转换</a><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing_targets.html#id1">4.8.1. 标签二值化</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="modules/preprocessing_targets.html#id2">4.8.2. 标签编码</a></li>
</ul>
</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-76">
<span id="calibre_link-811" class="calibre4"></span><h1 class="calibre5">4.1. Pipeline（管道）和 FeatureUnion（特征联合）: 合并的评估器</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Sehriff</a><br class="calibre9" /> 
    </div>
<div class="toctree-wrapper" id="calibre_link-812">
<span id="calibre_link-813" class="calibre4"></span><h2 class="sigil_not_in_toc">4.1.1. Pipeline: 链式评估器</h2>
<dl class="calibre10">
<dt class="calibre18"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 可以把多个评估器链接成一个。这个是很有用的，因为处理数据的步骤一般都是固定的，例如特征选择、标准化和分类。</dt>
<dd class="calibre19"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 主要有两个目的:</dd>
<dt class="calibre18">便捷性和封装性</dt>
<dd class="calibre19">你只要对数据调用 <a href="#calibre_link-77" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-814">``</span></a>fit``和 <a href="#calibre_link-78" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-815">``</span></a>predict``一次来适配所有的一系列评估器。</dd>
<dt class="calibre18">联合的参数选择</dt>
<dd class="calibre19">你可以一次 :ref:<a href="#calibre_link-79" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-816">`</span></a>grid search &lt;grid_search&gt;`管道中所有评估器的参数。</dd>
<dt class="calibre18">安全性</dt>
<dd class="calibre19">训练转换器和预测器使用的是相同样本，管道有助于防止来自测试数据的统计数据泄露到交叉验证的训练模型中。</dd>
</dl>
<p class="calibre10">管道中的所有评估器，除了最后一个评估器，管道的所有评估器必须是转换器。
(例如，必须有 <code class="docutils"><span class="calibre4">transform</span></code> 方法).
最后一个评估器的类型不限（转换器、分类器等等）</p>
<div class="toctree-wrapper" id="calibre_link-817">
<h3 class="sigil_not_in_toc1">4.1.1.1. 用法</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 使用一系列 <code class="docutils"><span class="calibre4">(key,</span> <span class="calibre4">value)</span></code> 键值对来构建,其中 <code class="docutils"><span class="calibre4">key</span></code> 是你给这个步骤起的名字， <code class="docutils"><span class="calibre4">value</span></code> 是一个评估器对象:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">Pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">PCA</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimators</span> <span class="calibre4">=</span> <span class="calibre4">[(</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">,</span> <span class="calibre4">PCA</span><span class="calibre4">()),</span> <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">SVC</span><span class="calibre4">())]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span> 
<span class="calibre4">Pipeline(memory=None,</span>
<span class="calibre4">         steps=[('reduce_dim', PCA(copy=True,...)),</span>
<span class="calibre4">                ('clf', SVC(C=1.0,...))])</span>
</pre>
</div>
</div>
</div>
</blockquote>
<p class="calibre10">功能函数 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline" title="sklearn.pipeline.make_pipeline"><code class="docutils"><span class="calibre4">make_pipeline</span></code></a> 是构建管道的缩写;
它接收多个评估器并返回一个管道，自动填充评估器名:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">make_pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.naive_bayes</span> <span class="calibre4">import</span> <span class="calibre4">MultinomialNB</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">Binarizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">make_pipeline</span><span class="calibre4">(</span><span class="calibre4">Binarizer</span><span class="calibre4">(),</span> <span class="calibre4">MultinomialNB</span><span class="calibre4">())</span> 
<span class="calibre4">Pipeline(memory=None,</span>
<span class="calibre4">         steps=[('binarizer', Binarizer(copy=True, threshold=0.0)),</span>
<span class="calibre4">                ('multinomialnb', MultinomialNB(alpha=1.0,</span>
<span class="calibre4">                                                class_prior=None,</span>
<span class="calibre4">                                                fit_prior=True))])</span>
</pre>
</div>
</div>
<p class="calibre10">管道中的评估器作为一个列表保存在 <code class="docutils"><span class="calibre4">steps</span></code> 属性内:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">steps</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,</span>
<span class="calibre4">  svd_solver='auto', tol=0.0, whiten=False))</span>
</pre>
</div>
</div>
<p class="calibre10">并作为 <code class="docutils"><span class="calibre4">dict</span></code> 保存在 <code class="docutils"><span class="calibre4">named_steps</span></code>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">[</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">]</span>
<span class="calibre4">PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,</span>
<span class="calibre4">  svd_solver='auto', tol=0.0, whiten=False)</span>
</pre>
</div>
</div>
<p class="calibre10">管道中的评估器参数可以通过 <code class="docutils"><span class="calibre4">&lt;estimator&gt;__&lt;parameter&gt;</span></code> 语义来访问:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">clf__C</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">)</span> 
<span class="calibre4">Pipeline(memory=None,</span>
<span class="calibre4">         steps=[('reduce_dim', PCA(copy=True, iterated_power='auto',...)),</span>
<span class="calibre4">                ('clf', SVC(C=10, cache_size=200, class_weight=None,...))])</span>
</pre>
</div>
</div>
<p class="calibre10">named_steps 的属性映射到多个值,在交互环境支持 tab 补全:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">.</span><span class="calibre4">reduce_dim</span> <span class="calibre4">is</span> <span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">[</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">]</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<p class="calibre10">这对网格搜索尤其重要:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">param_grid</span> <span class="calibre4">=</span> <span class="calibre4">dict</span><span class="calibre4">(</span><span class="calibre4">reduce_dim__n_components</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                  <span class="calibre4">clf__C</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">grid_search</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">pipe</span><span class="calibre4">,</span> <span class="calibre4">param_grid</span><span class="calibre4">=</span><span class="calibre4">param_grid</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">单独的步骤可以用多个参数替换，除了最后步骤，其他步骤都可以设置为 <code class="docutils"><span class="calibre4">None</span></code> 来跳过</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">LogisticRegression</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">param_grid</span> <span class="calibre4">=</span> <span class="calibre4">dict</span><span class="calibre4">(</span><span class="calibre4">reduce_dim</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">None</span><span class="calibre4">,</span> <span class="calibre4">PCA</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">),</span> <span class="calibre4">PCA</span><span class="calibre4">(</span><span class="calibre4">10</span><span class="calibre4">)],</span>
<span class="calibre4">... </span>                  <span class="calibre4">clf</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">SVC</span><span class="calibre4">(),</span> <span class="calibre4">LogisticRegression</span><span class="calibre4">()],</span>
<span class="calibre4">... </span>                  <span class="calibre4">clf__C</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">grid_search</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">pipe</span><span class="calibre4">,</span> <span class="calibre4">param_grid</span><span class="calibre4">=</span><span class="calibre4">param_grid</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/feature_selection/plot_feature_selection_pipeline.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-pipeline-py"><span class="calibre4">Pipeline Anova SVM</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="calibre4">Sample pipeline for text feature extraction and evaluation</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_digits_pipe.html#sphx-glr-auto-examples-plot-digits-pipe-py"><span class="calibre4">Pipelining: chaining a PCA and a logistic regression</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py"><span class="calibre4">Explicit feature map approximation for RBF kernels</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/svm/plot_svm_anova.html#sphx-glr-auto-examples-svm-plot-svm-anova-py"><span class="calibre4">SVM-Anova: SVM with univariate feature selection</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_compare_reduction.html#sphx-glr-auto-examples-plot-compare-reduction-py"><span class="calibre4">Selecting dimensionality reduction with Pipeline and GridSearchCV</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">也可以参阅:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="grid_search.html#grid-search"><span class="calibre4">调整估计器的超参数</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-818">
<h3 class="sigil_not_in_toc1">4.1.1.2. 注意点</h3>
<p class="calibre2">对管道调用 <code class="docutils"><span class="calibre4">fit</span></code> 方法的效果跟依次对每个评估器调用 <code class="docutils"><span class="calibre4">fit</span></code> 方法一样, 都是``transform`` 输入并传递给下个步骤。
管道中最后一个评估器的所有方法，管道都有,例如，如果最后的评估器是一个分类器， <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 可以当做分类器来用。如果最后一个评估器是转换器，管道也一样可以。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-819">
<span id="calibre_link-820" class="calibre4"></span><h3 class="sigil_not_in_toc1">4.1.1.3. 缓存转换器：避免重复计算</h3>
<p class="calibre2">适配转换器是很耗费计算资源的。设置了``memory`` 参数， <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 将会在调用``fit``方法后缓存每个转换器。
如果参数和输入数据相同，这个特征用于避免重复计算适配的转换器。典型的例子是网格搜索转换器，该转化器只要适配一次就可以多次使用。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><code class="docutils"><span class="calibre4">memory</span></code> 参数用于缓存转换器。</div>
</blockquote>
<p class="calibre10"><code class="docutils"><span class="calibre4">memory</span></code> 可以是包含要缓存的转换器的目录的字符串或一个 <a class="calibre3 pcalibre" href="https://pythonhosted.org/joblib/memory.html">joblib.Memory</a>
对象:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">tempfile</span> <span class="calibre4">import</span> <span class="calibre4">mkdtemp</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">shutil</span> <span class="calibre4">import</span> <span class="calibre4">rmtree</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">PCA</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">Pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimators</span> <span class="calibre4">=</span> <span class="calibre4">[(</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">,</span> <span class="calibre4">PCA</span><span class="calibre4">()),</span> <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">SVC</span><span class="calibre4">())]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cachedir</span> <span class="calibre4">=</span> <span class="calibre4">mkdtemp</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">,</span> <span class="calibre4">memory</span><span class="calibre4">=</span><span class="calibre4">cachedir</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span> 
<span class="calibre4">Pipeline(...,</span>
<span class="calibre4">         steps=[('reduce_dim', PCA(copy=True,...)),</span>
<span class="calibre4">                ('clf', SVC(C=1.0,...))])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Clear the cache directory when you don't need it anymore</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rmtree</span><span class="calibre4">(</span><span class="calibre4">cachedir</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10"><strong class="calibre14">Side effect of caching transfomers</strong></p>
<p class="calibre10">使用 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 而不开启缓存功能,还是可以通过查看原始实例的，例如:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_digits</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pca1</span> <span class="calibre4">=</span> <span class="calibre4">PCA</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm1</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([(</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">,</span> <span class="calibre4">pca1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">svm1</span><span class="calibre4">)])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pipe</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">Pipeline(memory=None,</span>
<span class="calibre4">         steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># The pca instance can be inspected directly</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">pca1</span><span class="calibre4">.</span><span class="calibre4">components_</span><span class="calibre4">)</span> 
<span class="calibre4">    [[ -1.77484909e-19  ... 4.07058917e-18]]</span>
</pre>
</div>
</div>
<p class="calibre10">开启缓存会在适配前触发转换器的克隆。因此，管道的转换器实例不能被直接查看。
在下面例子中， 访问 <code class="docutils"><span class="calibre4">PCA</span></code> 实例 <code class="docutils"><span class="calibre4">pca2</span></code>
将会引发 <code class="docutils"><span class="calibre4">AttributeError</span></code> 因为 <code class="docutils"><span class="calibre4">pca2</span></code> 是一个未适配的转换器。
这时应该使用属性 <code class="docutils"><span class="calibre4">named_steps</span></code> 来检查管道的评估器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cachedir</span> <span class="calibre4">=</span> <span class="calibre4">mkdtemp</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pca2</span> <span class="calibre4">=</span> <span class="calibre4">PCA</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm2</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cached_pipe</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([(</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">,</span> <span class="calibre4">pca2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">svm2</span><span class="calibre4">)],</span>
<span class="calibre4">... </span>                       <span class="calibre4">memory</span><span class="calibre4">=</span><span class="calibre4">cachedir</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cached_pipe</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4"> Pipeline(memory=...,</span>
<span class="calibre4">          steps=[('reduce_dim', PCA(...)), ('clf', SVC(...))])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">cached_pipe</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">[</span><span class="calibre4">'reduce_dim'</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">components_</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">    [[ -1.77484909e-19  ... 4.07058917e-18]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Remove the cache directory</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rmtree</span><span class="calibre4">(</span><span class="calibre4">cachedir</span><span class="calibre4">)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_compare_reduction.html#sphx-glr-auto-examples-plot-compare-reduction-py"><span class="calibre4">Selecting dimensionality reduction with Pipeline and GridSearchCV</span></a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-821">
<span id="calibre_link-822" class="calibre4"></span><h2 class="sigil_not_in_toc">4.1.2. FeatureUnion（特征联合）: 个特征层面</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" title="sklearn.pipeline.FeatureUnion"><code class="docutils"><span class="calibre4">FeatureUnion</span></code></a> 合并了多个转换器对象形成一个新的转换器，该转换器合并了他们的输出。一个 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" title="sklearn.pipeline.FeatureUnion"><code class="docutils"><span class="calibre4">FeatureUnion</span></code></a> 可以接收多个转换器对象。在适配期间，每个转换器都单独的和数据适配。
对于转换数据，转换器可以并发使用，且输出的样本向量被连接成更大的向量。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" title="sklearn.pipeline.FeatureUnion"><code class="docutils"><span class="calibre4">FeatureUnion</span></code></a> 功能与 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 一样-
便捷性和联合参数的估计和验证。</p>
<p class="calibre10">可以结合:class:<cite class="calibre13">FeatureUnion</cite> 和 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">Pipeline</span></code></a> 来创造出复杂模型。</p>
<p class="calibre10">(一个 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" title="sklearn.pipeline.FeatureUnion"><code class="docutils"><span class="calibre4">FeatureUnion</span></code></a> 没办法检查两个转换器是否会产出相同的特征。它仅仅在特征集合不相关时产生联合并确认是调用者的职责。)</p>
<div class="toctree-wrapper" id="calibre_link-823">
<h3 class="sigil_not_in_toc1">4.1.2.1. 用法</h3>
<p class="calibre2">一个 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" title="sklearn.pipeline.FeatureUnion"><code class="docutils"><span class="calibre4">FeatureUnion</span></code></a> 是通过一系列 <code class="docutils"><span class="calibre4">(key,</span> <span class="calibre4">value)</span></code> 键值对来构建的,其中的 <code class="docutils"><span class="calibre4">key</span></code> 给转换器指定的名字
(一个绝对的字符串; 他只是一个代号)， <code class="docutils"><span class="calibre4">value</span></code> 是一个评估器对象:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">FeatureUnion</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">PCA</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">KernelPCA</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimators</span> <span class="calibre4">=</span> <span class="calibre4">[(</span><span class="calibre4">'linear_pca'</span><span class="calibre4">,</span> <span class="calibre4">PCA</span><span class="calibre4">()),</span> <span class="calibre4">(</span><span class="calibre4">'kernel_pca'</span><span class="calibre4">,</span> <span class="calibre4">KernelPCA</span><span class="calibre4">())]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">combined</span> <span class="calibre4">=</span> <span class="calibre4">FeatureUnion</span><span class="calibre4">(</span><span class="calibre4">estimators</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">combined</span> 
<span class="calibre4">FeatureUnion(n_jobs=1,</span>
<span class="calibre4">             transformer_list=[('linear_pca', PCA(copy=True,...)),</span>
<span class="calibre4">                               ('kernel_pca', KernelPCA(alpha=1.0,...))],</span>
<span class="calibre4">             transformer_weights=None)</span>
</pre>
</div>
</div>
<p class="calibre10">跟管道一样，特征联合有一个精简版的构造器叫做:func:<cite class="calibre13">make_union</cite> ，该构造器不需要显式给每个组价起名字。</p>
<p class="calibre10">正如 <code class="docutils"><span class="calibre4">Pipeline</span></code>, 单独的步骤可能用``set_params``替换 ,并设置为 <a href="#calibre_link-80" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-824">``</span></a>None``来跳过:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">combined</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">kernel_pca</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">)</span>
<span class="calibre4">... </span>
<span class="calibre4">FeatureUnion(n_jobs=1,</span>
<span class="calibre4">             transformer_list=[('linear_pca', PCA(copy=True,...)),</span>
<span class="calibre4">                               ('kernel_pca', None)],</span>
<span class="calibre4">             transformer_weights=None)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_feature_stacker.html#sphx-glr-auto-examples-plot-feature-stacker-py"><span class="calibre4">Concatenating multiple feature extraction methods</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py"><span class="calibre4">Feature Union with Heterogeneous Data Sources</span></a></li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-62">
<span id="calibre_link-825" class="calibre4"></span><h1 class="calibre5">4.2. 特征提取</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@if only</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />  
    </div>
<p class="calibre10">模块 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code class="docutils"><span class="calibre4">sklearn.feature_extraction</span></code></a> 可用于提取符合机器学习算法支持的特征，比如文本和图片。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">特征特征提取与 <span class="calibre4">特征选择</span> 有很大的不同：前者包括将任意数据（如文本或图像）转换为可用于机器学习的数值特征。后者是将这些特征应用到机器学习中。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-826">
<span id="calibre_link-827" class="calibre4"></span><h2 class="sigil_not_in_toc">4.2.1. 从字典类型加载特征</h2>
<p class="calibre2">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="docutils"><span class="calibre4">DictVectorizer</span></code></a> 可用于将标准的Python字典（dict）对象列表的要素数组转换为 scikit-learn 估计器使用的 NumPy/SciPy 表示形式。</p>
<p class="calibre10">虽然 Python 的处理速度不是特别快，但 Python 的 <code class="docutils"><span class="calibre4">dict</span></code> 优点是使用方便，稀疏（不需要存储的特征），并且除了值之外还存储特征名称。</p>
<p class="calibre10">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="docutils"><span class="calibre4">DictVectorizer</span></code></a> 实现了 “one-of-K” 或 “one-hot” 编码，用于分类（也称为标称，离散）特征。分类功能是 “属性值” 对，其中该值被限制为不排序的可能性的离散列表（例如主题标识符，对象类型，标签，名称…）。</p>
<p class="calibre10">在下面的例子，”城市” 是一个分类属性，而 “温度” 是传统的数字特征:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">measurements</span> <span class="calibre4">=</span> <span class="calibre4">[</span>
<span class="calibre4">... </span>    <span class="calibre4">{</span><span class="calibre4">'city'</span><span class="calibre4">:</span> <span class="calibre4">'Dubai'</span><span class="calibre4">,</span> <span class="calibre4">'temperature'</span><span class="calibre4">:</span> <span class="calibre4">33.</span><span class="calibre4">},</span>
<span class="calibre4">... </span>    <span class="calibre4">{</span><span class="calibre4">'city'</span><span class="calibre4">:</span> <span class="calibre4">'London'</span><span class="calibre4">,</span> <span class="calibre4">'temperature'</span><span class="calibre4">:</span> <span class="calibre4">12.</span><span class="calibre4">},</span>
<span class="calibre4">... </span>    <span class="calibre4">{</span><span class="calibre4">'city'</span><span class="calibre4">:</span> <span class="calibre4">'San Francisco'</span><span class="calibre4">,</span> <span class="calibre4">'temperature'</span><span class="calibre4">:</span> <span class="calibre4">18.</span><span class="calibre4">},</span>
<span class="calibre4">... </span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction</span> <span class="calibre4">import</span> <span class="calibre4">DictVectorizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vec</span> <span class="calibre4">=</span> <span class="calibre4">DictVectorizer</span><span class="calibre4">()</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vec</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">measurements</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[  1.,   0.,   0.,  33.],</span>
<span class="calibre4">       [  0.,   1.,   0.,  12.],</span>
<span class="calibre4">       [  0.,   0.,   1.,  18.]])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vec</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span>
<span class="calibre4">['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']</span>
</pre>
</div>
</div>
<p class="calibre10">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="docutils"><span class="calibre4">DictVectorizer</span></code></a> 也是对自然语言处理模型中训练序列分类器的有用的表示变换，通常通过提取围绕感兴趣的特定的词的特征窗口来工作。</p>
<p class="calibre10">例如，假设我们具有提取我们想要用作训练序列分类器（例如：块）的互补标签的部分语音（PoS）标签的第一算法。以下 dict 可以是在 “坐在垫子上的猫” 的句子，围绕 “sat” 一词提取的这样一个特征窗口:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pos_window</span> <span class="calibre4">=</span> <span class="calibre4">[</span>
<span class="calibre4">... </span>    <span class="calibre4">{</span>
<span class="calibre4">... </span>        <span class="calibre4">'word-2'</span><span class="calibre4">:</span> <span class="calibre4">'the'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>        <span class="calibre4">'pos-2'</span><span class="calibre4">:</span> <span class="calibre4">'DT'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>        <span class="calibre4">'word-1'</span><span class="calibre4">:</span> <span class="calibre4">'cat'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>        <span class="calibre4">'pos-1'</span><span class="calibre4">:</span> <span class="calibre4">'NN'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>        <span class="calibre4">'word+1'</span><span class="calibre4">:</span> <span class="calibre4">'on'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>        <span class="calibre4">'pos+1'</span><span class="calibre4">:</span> <span class="calibre4">'PP'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">},</span>
<span class="calibre4">... </span>    <span class="calibre4"># in a real application one would extract many such dictionaries</span>
<span class="calibre4">... </span><span class="calibre4">]</span>
</pre>
</div>
</div>
<p class="calibre10">该描述可以被矢量化为适合于呈递分类器的稀疏二维矩阵（可能在被管道 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">text.TfidfTransformer</span></code></a> 进行归一化之后）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vec</span> <span class="calibre4">=</span> <span class="calibre4">DictVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pos_vectorized</span> <span class="calibre4">=</span> <span class="calibre4">vec</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">pos_window</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pos_vectorized</span>                
<span class="calibre4">&lt;1x6 sparse matrix of type '&lt;... 'numpy.float64'&gt;'</span>
<span class="calibre4">    with 6 stored elements in Compressed Sparse ... format&gt;</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pos_vectorized</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[ 1.,  1.,  1.,  1.,  1.,  1.]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vec</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span>
<span class="calibre4">['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']</span>
</pre>
</div>
</div>
<p class="calibre10">你可以想象，如果一个文本语料库的每一个单词都提取了这样一个上下文，那么所得的矩阵将会非常宽（许多 one-hot-features），其中大部分通常将会是0。
为了使结果数据结构能够适应内存，该类``DictVectorizer`` 的 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 默认使用一个矩阵而不是一个 <code class="docutils"><span class="calibre4">numpy.ndarray</span></code>。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-63">
<span id="calibre_link-828" class="calibre4"></span><h2 class="sigil_not_in_toc">4.2.2. 特征哈希（相当于一种降维技巧）</h2>
<p class="calibre2">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 是一种高速，低内存消耗的向量化方法，它使用了`特征散列 feature hashing &lt;<a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Feature_hashing">https://en.wikipedia.org/wiki/Feature_hashing</a>&gt;`_ 技术 ，或可称为 “散列法” （hashing trick）的技术。
代替在构建训练中遇到的特征的哈希表，如向量化所做的那样 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 将哈希函数应用于特征，以便直接在样本矩阵中确定它们的列索引。
结果是以牺牲可检测性为代价，提高速度和减少内存的使用; 哈希表不记得输入特性是什么样的，没有 <code class="docutils"><span class="calibre4">inverse_transform</span></code> 办法。</p>
<p class="calibre10">由于散列函数可能导致（不相关）特征之间的冲突，因此使用带符号散列函数，并且散列值的符号确定存储在特征的输出矩阵中的值的符号。
这样，碰撞可能会抵消而不是累积错误，并且任何输出要素的值的预期平均值为零。默认情况下，此机制将使用 <code class="docutils"><span class="calibre4">alternate_sign=True</span></code> 启用，对于小型哈希表大小（<code class="docutils"><span class="calibre4">n_features</span> <span class="calibre4">&lt;</span> <span class="calibre4">10000</span></code>）特别有用。
对于大的哈希表大小，可以禁用它，以便将输出传递给估计器，如 <a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="docutils"><span class="calibre4">sklearn.naive_bayes.MultinomialNB</span></code></a> 或 <a class="calibre3 pcalibre" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="docutils"><span class="calibre4">sklearn.feature_selection.chi2</span></code></a> 特征选择器，这些特征选项器可以使用非负输入。</p>
<p class="calibre10">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 接受映射（如 Python 的 <code class="docutils"><span class="calibre4">dict</span></code> 及其在 <code class="docutils"><span class="calibre4">collections</span></code> 模块中的变体），使用键值对 <code class="docutils"><span class="calibre4">(feature,</span> <span class="calibre4">value)</span></code> 或字符串，具体取决于构造函数参数 <code class="docutils"><span class="calibre4">input_type</span></code>。
映射被视为 <code class="docutils"><span class="calibre4">(feature,</span> <span class="calibre4">value)</span></code> 对的列表，而单个字符串的隐含值为1，因此 <code class="docutils"><span class="calibre4">['feat1',</span> <span class="calibre4">'feat2',</span> <span class="calibre4">'feat3']</span></code> 被解释为 <code class="docutils"><span class="calibre4">[('feat1',</span> <span class="calibre4">1),</span> <span class="calibre4">('feat2',</span> <span class="calibre4">1),</span> <span class="calibre4">('feat3',</span> <span class="calibre4">1)]</span></code>。
如果单个特征在样本中多次出现，相关值将被求和（所以 <code class="docutils"><span class="calibre4">('feat',</span> <span class="calibre4">2)</span></code> 和 <code class="docutils"><span class="calibre4">('feat',</span> <span class="calibre4">3.5)</span></code> 变为 <code class="docutils"><span class="calibre4">('feat',</span> <span class="calibre4">5.5)</span></code>）。 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 的输出始终是 CSR 格式的 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 矩阵。</p>
<p class="calibre10">特征散列可以在文档分类中使用，但与 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">text.CountVectorizer</span></code></a> 不同，<a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 不执行除 Unicode 或 UTF-8 编码之外的任何其他预处理;
请参阅下面的哈希技巧向量化大文本语料库，用于组合的 tokenizer/hasher。</p>
<p class="calibre10">例如，有一个词级别的自然语言处理任务，需要从 <code class="docutils"><span class="calibre4">(token,</span> <span class="calibre4">part_of_speech)</span></code> 键值对中提取特征。可以使用 Python 生成器函数来提取功能:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">def</span> <span class="calibre4">token_features</span><span class="calibre4">(</span><span class="calibre4">token</span><span class="calibre4">,</span> <span class="calibre4">part_of_speech</span><span class="calibre4">):</span>
    <span class="calibre4">if</span> <span class="calibre4">token</span><span class="calibre4">.</span><span class="calibre4">isdigit</span><span class="calibre4">():</span>
        <span class="calibre4">yield</span> <span class="calibre4">"numeric"</span>
    <span class="calibre4">else</span><span class="calibre4">:</span>
        <span class="calibre4">yield</span> <span class="calibre4">"token=</span><span class="calibre4">{}</span><span class="calibre4">"</span><span class="calibre4">.</span><span class="calibre4">format</span><span class="calibre4">(</span><span class="calibre4">token</span><span class="calibre4">.</span><span class="calibre4">lower</span><span class="calibre4">())</span>
        <span class="calibre4">yield</span> <span class="calibre4">"token,pos=</span><span class="calibre4">{}</span><span class="calibre4">,</span><span class="calibre4">{}</span><span class="calibre4">"</span><span class="calibre4">.</span><span class="calibre4">format</span><span class="calibre4">(</span><span class="calibre4">token</span><span class="calibre4">,</span> <span class="calibre4">part_of_speech</span><span class="calibre4">)</span>
    <span class="calibre4">if</span> <span class="calibre4">token</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">isupper</span><span class="calibre4">():</span>
        <span class="calibre4">yield</span> <span class="calibre4">"uppercase_initial"</span>
    <span class="calibre4">if</span> <span class="calibre4">token</span><span class="calibre4">.</span><span class="calibre4">isupper</span><span class="calibre4">():</span>
        <span class="calibre4">yield</span> <span class="calibre4">"all_uppercase"</span>
    <span class="calibre4">yield</span> <span class="calibre4">"pos=</span><span class="calibre4">{}</span><span class="calibre4">"</span><span class="calibre4">.</span><span class="calibre4">format</span><span class="calibre4">(</span><span class="calibre4">part_of_speech</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">然后， <code class="docutils"><span class="calibre4">raw_X</span></code>  为了可以传入 <code class="docutils"><span class="calibre4">FeatureHasher.transform</span></code> 可以通过如下方式构造:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">raw_X</span> <span class="calibre4">=</span> <span class="calibre4">(</span><span class="calibre4">token_features</span><span class="calibre4">(</span><span class="calibre4">tok</span><span class="calibre4">,</span> <span class="calibre4">pos_tagger</span><span class="calibre4">(</span><span class="calibre4">tok</span><span class="calibre4">))</span> <span class="calibre4">for</span> <span class="calibre4">tok</span> <span class="calibre4">in</span> <span class="calibre4">corpus</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">并传入一个 hasher:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">hasher</span> <span class="calibre4">=</span> <span class="calibre4">FeatureHasher</span><span class="calibre4">(</span><span class="calibre4">input_type</span><span class="calibre4">=</span><span class="calibre4">'string'</span><span class="calibre4">)</span>
<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">hasher</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">raw_X</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">得到一个 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 类型的矩阵 <code class="docutils"><span class="calibre4">X</span></code>。</p>
<p class="calibre10">注意使用发生器的理解，它将懒惰引入到特征提取中：词令牌（token）只能根据需要从哈希值进行处理。</p>
<div class="toctree-wrapper" id="calibre_link-829">
<h3 class="sigil_not_in_toc1">4.2.2.1. 实现细节</h3>
<p class="calibre2">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">FeatureHasher</span></code></a> 使用签名的 32-bit 变体的 MurmurHash3。
因此导致（并且由于限制 <code class="docutils"><span class="calibre4">scipy.sparse</span></code>），当前支持的功能的最大数量 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000586.jpg" alt="2^{31} - 1" />.</p>
<p class="calibre10">特征哈希的原始形式源于Weinberger et al，使用两个单独的哈希函数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000836.jpg" alt="h" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000751.jpg" alt="\xi" /> 分别确定特征的列索引和符号。
现有的实现是基于假设：MurmurHash3的符号位与其他位独立。</p>
<p class="calibre10">由于使用简单的模数将哈希函数转换为列索引，建议使用2次幂作为 <code class="docutils"><span class="calibre4">n_features</span></code> 参数; 否则特征不会均匀的分布到列中。</p>
<p class="calibre10">参考文献：
Kilian Weinberger，Anirban Dasgupta，John Langford，Alex Smola和Josh Attenberg（2009）。用于大规模多任务学习的特征散列。PROC。ICML。
MurmurHash3。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l">Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and
Josh Attenberg (2009). <a class="calibre3 pcalibre" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">用于大规模多任务学习的特征散列</a>. Proc. ICML.</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://github.com/aappleby/smhasher">MurmurHash3</a>.</li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-830">
<span id="calibre_link-831" class="calibre4"></span><h2 class="sigil_not_in_toc">4.2.3. 文本特征提取</h2>
<div class="toctree-wrapper" id="calibre_link-832">
<h3 class="sigil_not_in_toc1">4.2.3.1. 话语表示</h3>
<p class="calibre2">文本分析是机器学习算法的主要应用领域。
然而，原始数据，符号文字序列不能直接传递给算法，因为它们大多数要求具有固定长度的数字矩阵特征向量，而不是具有可变长度的原始文本文档。</p>
<p class="calibre10">为解决这个问题，scikit-learn提供了从文本内容中提取数字特征的最常见方法，即：</p>
<ul class="calibre6">
<li class="toctree-l"><strong class="calibre14">令牌化（tokenizing）</strong> 对每个可能的词令牌分成字符串并赋予整数形的id，例如通过使用空格和标点符号作为令牌分隔符。</li>
<li class="toctree-l"><strong class="calibre14">统计（counting）</strong> 每个词令牌在文档中的出现次数。</li>
<li class="toctree-l"><strong class="calibre14">标准化（normalizing）</strong> 在大多数的文档 / 样本中，可以减少重要的次令牌的出现次数的权重。。</li>
</ul>
<p class="calibre10">在该方案中，特征和样本定义如下：</p>
<ul class="calibre6">
<li class="toctree-l">每个**单独的令牌发生频率**（归一化或不归零）被视为一个**特征**。</li>
<li class="toctree-l">给定**文档**中所有的令牌频率向量被看做一个多元sample**样本**。</li>
</ul>
<p class="calibre10">因此，文本的集合可被表示为矩阵形式，每行对应一条文本，每列对应每个文本中出现的词令牌(如单个词)。</p>
<p class="calibre10">我们称**向量化**是将文本文档集合转换为数字集合特征向量的普通方法。
这种特殊思想（令牌化，计数和归一化）被称为 <strong class="calibre14">Bag of Words</strong> 或 “Bag of n-grams” 模型。
文档由单词出现来描述，同时完全忽略文档中单词的相对位置信息。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-833">
<h3 class="sigil_not_in_toc1">4.2.3.2. 稀疏</h3>
<p class="calibre2">由于大多数文本文档通常只使用文本词向量全集中的一个小子集，所以得到的矩阵将具有许多特征值为零（通常大于99％）。</p>
<p class="calibre10">例如，10,000 个短文本文档（如电子邮件）的集合将使用总共100,000个独特词的大小的词汇，而每个文档将单独使用100到1000个独特的单词。</p>
<p class="calibre10">为了能够将这样的矩阵存储在存储器中，并且还可以加速代数的矩阵/向量运算，实现通常将使用诸如 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 包中的稀疏实现。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-834">
<h3 class="sigil_not_in_toc1">4.2.3.3. 常用 Vectorizer 使用</h3>
<p class="calibre2">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a>  在单个类中实现了令牌化和出现频数统计:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">CountVectorizer</span>
</pre>
</div>
</div>
<p class="calibre10">这个模型有很多参数，但参数的默认初始值是相当合理的（请参阅 <a class="calibre3 pcalibre" href="classes.html#text-feature-extraction-ref"><span class="calibre4">参考文档</span></a> 了解详细信息）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span>                     
<span class="calibre4">CountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',</span>
<span class="calibre4">        dtype=&lt;... 'numpy.int64'&gt;, encoding=...'utf-8', input=...'content',</span>
<span class="calibre4">        lowercase=True, max_df=1.0, max_features=None, min_df=1,</span>
<span class="calibre4">        ngram_range=(1, 1), preprocessor=None, stop_words=None,</span>
<span class="calibre4">        strip_accents=None, token_pattern=...'(?u)\\b\\w\\w+\\b',</span>
<span class="calibre4">        tokenizer=None, vocabulary=None)</span>
</pre>
</div>
</div>
<p class="calibre10">让我们使用它来进行简单文本全集令牌化，并统计词频:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">corpus</span> <span class="calibre4">=</span> <span class="calibre4">[</span>
<span class="calibre4">... </span>    <span class="calibre4">'This is the first document.'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">'This is the second second document.'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">'And the third one.'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">'Is this the first document?'</span><span class="calibre4">,</span>
<span class="calibre4">... </span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">corpus</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span>                              
<span class="calibre4">&lt;4x9 sparse matrix of type '&lt;... 'numpy.int64'&gt;'</span>
<span class="calibre4">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre>
</div>
</div>
<p class="calibre10">初始设定是，令牌化字符串，提取至少两个字母的词默认配置，做这一步的函数可以显式的被调用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">analyze</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">build_analyzer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">analyze</span><span class="calibre4">(</span><span class="calibre4">"This is a text document to analyze."</span><span class="calibre4">)</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">'this'</span><span class="calibre4">,</span> <span class="calibre4">'is'</span><span class="calibre4">,</span> <span class="calibre4">'text'</span><span class="calibre4">,</span> <span class="calibre4">'document'</span><span class="calibre4">,</span> <span class="calibre4">'to'</span><span class="calibre4">,</span> <span class="calibre4">'analyze'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<p class="calibre10">analyzer 在拟合期间发现的每个项都被分配一个与所得矩阵中的列对应的唯一整数索引。列的这种解释可以检索如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">'and'</span><span class="calibre4">,</span> <span class="calibre4">'document'</span><span class="calibre4">,</span> <span class="calibre4">'first'</span><span class="calibre4">,</span> <span class="calibre4">'is'</span><span class="calibre4">,</span> <span class="calibre4">'one'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>     <span class="calibre4">'second'</span><span class="calibre4">,</span> <span class="calibre4">'the'</span><span class="calibre4">,</span> <span class="calibre4">'third'</span><span class="calibre4">,</span> <span class="calibre4">'this'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>           
<span class="calibre4">array([[0, 1, 1, 1, 0, 0, 1, 0, 1],</span>
<span class="calibre4">       [0, 1, 0, 1, 0, 2, 1, 0, 1],</span>
<span class="calibre4">       [1, 0, 0, 0, 1, 0, 1, 1, 0],</span>
<span class="calibre4">       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)</span>
</pre>
</div>
</div>
<p class="calibre10">从列标到特征名的反转映射储存在向量化类 vectorizer 的属性  <code class="docutils"><span class="calibre4">vocabulary_</span></code> 中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">vocabulary_</span><span class="calibre4">.</span><span class="calibre4">get</span><span class="calibre4">(</span><span class="calibre4">'document'</span><span class="calibre4">)</span>
<span class="calibre4">1</span>
</pre>
</div>
</div>
<p class="calibre10">因此，在将来的调用转换方法中，在训练语料库中未出现的词将被完全忽略:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([</span><span class="calibre4">'Something completely new.'</span><span class="calibre4">])</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">... </span>                          
<span class="calibre4">array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)</span>
</pre>
</div>
</div>
<p class="calibre10">请注意，在上一个语料库中，第一个和最后一个文档具有完全相同的单词，因此被编码成相同的向量。
特别是最后一个字符是询问形式时我们丢失了他的信息。为了防止词组顺序颠倒，除了提取一元模型 1-grams（个别词）之外，我们还可以提取 2-grams 的单词:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">bigram_vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">(</span><span class="calibre4">ngram_range</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">),</span>
<span class="calibre4">... </span>                                    <span class="calibre4">token_pattern</span><span class="calibre4">=</span><span class="calibre4">r</span><span class="calibre4">'\b\w+\b'</span><span class="calibre4">,</span> <span class="calibre4">min_df</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">analyze</span> <span class="calibre4">=</span> <span class="calibre4">bigram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">build_analyzer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">analyze</span><span class="calibre4">(</span><span class="calibre4">'Bi-grams are cool!'</span><span class="calibre4">)</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">'bi'</span><span class="calibre4">,</span> <span class="calibre4">'grams'</span><span class="calibre4">,</span> <span class="calibre4">'are'</span><span class="calibre4">,</span> <span class="calibre4">'cool'</span><span class="calibre4">,</span> <span class="calibre4">'bi grams'</span><span class="calibre4">,</span> <span class="calibre4">'grams are'</span><span class="calibre4">,</span> <span class="calibre4">'are cool'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<p class="calibre10">矢量化提取的词因此变得很大，同时可以在定位模式时消歧义:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_2</span> <span class="calibre4">=</span> <span class="calibre4">bigram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">corpus</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_2</span>
<span class="calibre4">... </span>                          
<span class="calibre4">array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],</span>
<span class="calibre4">       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],</span>
<span class="calibre4">       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],</span>
<span class="calibre4">       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)</span>
</pre>
</div>
</div>
<p class="calibre10">特别是 “Is this” 的疑问形式只出现在最后一个文档中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">feature_index</span> <span class="calibre4">=</span> <span class="calibre4">bigram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">vocabulary_</span><span class="calibre4">.</span><span class="calibre4">get</span><span class="calibre4">(</span><span class="calibre4">'is this'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_2</span><span class="calibre4">[:,</span> <span class="calibre4">feature_index</span><span class="calibre4">]</span>     
<span class="calibre4">array([0, 0, 0, 1]...)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-835">
<span id="calibre_link-836" class="calibre4"></span><h3 class="sigil_not_in_toc1">4.2.3.4. Tf&ndash;idf 项加权</h3>
<p class="calibre2">在一个大的文本语料库中，一些单词将出现很多次（例如 “the”, “a”, “is” 是英文），因此对文档的实际内容没有什么有意义的信息。
如果我们将直接计数数据直接提供给分类器，那么这些频繁词组会掩盖住那些我们关注但很少出现的词。</p>
<p class="calibre10">为了为了重新计算特征权重，并将其转化为适合分类器使用的浮点值，因此使用 tf-idf 变换是非常常见的。</p>
<p class="calibre10">Tf表示**术语频率**，而 tf-idf 表示术语频率乘以**转制文档频率**:
<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000148.jpg" alt="\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}" />.</p>
<p class="calibre10">使用 <code class="docutils"><span class="calibre4">TfidfTransformer</span></code> 的默认设置，<code class="docutils"><span class="calibre4">TfidfTransformer(norm='l2',</span> <span class="calibre4">use_idf=True,</span> <span class="calibre4">smooth_idf=True,</span> <span class="calibre4">sublinear_tf=False)</span></code> 术语频率，一个术语在给定文档中出现的次数乘以 idf 组件， 计算为</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000134.jpg" alt="\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1" />,</p>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000549.jpg" alt="n_d" /> 是文档的总数，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000294.jpg" alt="\text{df}(d,t)" /> 是包含术语 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000180.jpg" alt="t" /> 的文档数。
然后，所得到的 tf-idf 向量通过欧几里得范数归一化：</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000391.jpg" alt="v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 + v{_2}^2 + \dots + v{_n}^2}}" />.</p>
<p class="calibre10">它源于一个词权重的信息检索方式(作为搜索引擎结果的评级函数)，同时也在文档分类和聚类中表现良好。</p>
<p class="calibre10">以下部分包含进一步说明和示例，说明如何精确计算 tf-idfs 以及如何在 scikit-learn 中计算 tf-idfs， <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">TfidfTransformer</span></code></a> 并 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="docutils"><span class="calibre4">TfidfVectorizer</span></code></a> 与定义 idf 的标准教科书符号略有不同</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000815.jpg" alt="\text{idf}(t) = log{\frac{n_d}{1+\text{df}(d,t)}}." /></p>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">TfidfTransformer</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="docutils"><span class="calibre4">TfidfVectorizer</span></code></a> 中 <code class="docutils"><span class="calibre4">smooth_idf=False</span></code>，将 “1” 计数添加到 idf 而不是 idf 的分母:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000421.jpg" alt="\text{idf}(t) = log{\frac{n_d}{\text{df}(d,t)}} + 1" /></p>
<p class="calibre10">该归一化由类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">TfidfTransformer</span></code></a> 实现:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">TfidfTransformer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">(</span><span class="calibre4">smooth_idf</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span>   
<span class="calibre4">TfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,</span>
<span class="calibre4">                 use_idf=True)</span>
</pre>
</div>
</div>
<p class="calibre10">有关所有参数的详细信息，请参阅 <a class="calibre3 pcalibre" href="classes.html#text-feature-extraction-ref"><span class="calibre4">参考文档</span></a>。</p>
<p class="calibre10">让我们以下方的词频为例。第一个次在任何时间都是100％出现，因此不是很有重要。另外两个特征只占不到50％的比例，因此可能更具有代表性:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">counts</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span>
<span class="calibre4">... </span>          <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
<span class="calibre4">... </span>          <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
<span class="calibre4">... </span>          <span class="calibre4">[</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
<span class="calibre4">... </span>          <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
<span class="calibre4">... </span>          <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]]</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tfidf</span> <span class="calibre4">=</span> <span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">counts</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tfidf</span>                         
<span class="calibre4">&lt;6x3 sparse matrix of type '&lt;... 'numpy.float64'&gt;'</span>
<span class="calibre4">    with 9 stored elements in Compressed Sparse ... format&gt;</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tfidf</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>                        
<span class="calibre4">array([[ 0.81940995,  0.        ,  0.57320793],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 0.47330339,  0.88089948,  0.        ],</span>
<span class="calibre4">       [ 0.58149261,  0.        ,  0.81355169]])</span>
</pre>
</div>
</div>
<p class="calibre10">每行都被正则化，使其适应欧几里得标准:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000391.jpg" alt="v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 + v{_2}^2 + \dots + v{_n}^2}}" /></p>
<p class="calibre10">例如，我们可以计算`计数`数组中第一个文档中第一个项的 tf-idf ，如下所示:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000845.jpg" alt="n_{d, {\text{term1}}} = 6" /></p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000289.jpg" alt="\text{df}(d, t)_{\text{term1}} = 6" /></p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000886.jpg" alt="\text{idf}(d, t)_{\text{term1}} = log \frac{n_d}{\text{df}(d, t)} + 1 = log(1)+1 = 1" /></p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000752.jpg" alt="\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3" /></p>
<p class="calibre10">现在，如果我们对文档中剩下的2个术语重复这个计算，我们得到:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000116.jpg" alt="\text{tf-idf}_{\text{term2}} = 0 \times log(6/1)+1 = 0" /></p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000129.jpg" alt="\text{tf-idf}_{\text{term3}} = 1 \times log(6/2)+1 \approx 2.0986" /></p>
<p class="calibre10">和原始 tf-idfs 的向量:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000809.jpg" alt="\text{tf-idf}_raw = [3, 0, 2.0986]." /></p>
<p class="calibre10">然后，应用欧几里德（L2）规范，我们获得文档1的以下 tf-idfs:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000831.jpg" alt="\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}} = [ 0.819,  0,  0.573]." /></p>
<p class="calibre10">此外，默认参数 <code class="docutils"><span class="calibre4">smooth_idf=True</span></code> 将 “1” 添加到分子和分母，就好像一个额外的文档被看到一样包含集合中的每个术语，这样可以避免零分割:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000134.jpg" alt="\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1" /></p>
<p class="calibre10">使用此修改，文档1中第三项的 tf-idf 更改为 1.8473:</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000068.jpg" alt="\text{tf-idf}_{\text{term3}} = 1 \times log(7/3)+1 \approx 1.8473" /></p>
<p class="calibre10">而 L2 标准化的 tf-idf 变为</p>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000384.jpg" alt="\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}} = [0.8515, 0, 0.5243]" />:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">counts</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[ 0.85151335,  0.        ,  0.52433293],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 1.        ,  0.        ,  0.        ],</span>
<span class="calibre4">       [ 0.55422893,  0.83236428,  0.        ],</span>
<span class="calibre4">       [ 0.63035731,  0.        ,  0.77630514]])</span>
</pre>
</div>
</div>
<p class="calibre10">通过 <code class="docutils"><span class="calibre4">拟合</span></code> 方法调用计算的每个特征的权重存储在模型属性中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">idf_</span>                       
<span class="calibre4">array([ 1. ...,  2.25...,  1.84...])</span>
</pre>
</div>
</div>
<p class="calibre10">由于 tf-idf 经常用于文本特征，所以还有一个类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="docutils"><span class="calibre4">TfidfVectorizer</span></code></a> ，它将 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a>
和 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">TfidfTransformer</span></code></a> 的所有选项组合在一个单例模型中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">TfidfVectorizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">corpus</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                               
<span class="calibre4">&lt;4x9 sparse matrix of type '&lt;... 'numpy.float64'&gt;'</span>
<span class="calibre4">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre>
</div>
</div>
<p class="calibre10">虽然tf-idf标准化通常非常有用，但是可能有一种情况是二元变量显示会提供更好的特征。
这可以使用类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 的 <code class="docutils"><span class="calibre4">二进制</span></code> 参数来实现。
特别地，一些估计器，诸如 <a class="calibre3 pcalibre" href="naive_bayes.html#bernoulli-naive-bayes"><span class="calibre4">伯努利朴素贝叶斯</span></a> 显式的使用离散的布尔随机变量。
而且，非常短的文本很可能影响 tf-idf 值，而二进制出现信息更稳定。</p>
<p class="calibre10">通常情况下，调整特征提取参数的最佳方法是使用基于网格搜索的交叉验证，例如通过将特征提取器与分类器进行流水线化:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">用于文本特征提取和评估的样本管道 <a class="calibre3 pcalibre" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="calibre4">Sample pipeline for text feature extraction and evaluation</span></a></li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-837">
<h3 class="sigil_not_in_toc1">4.2.3.5. 解码文本文件</h3>
<p class="calibre2">文本由字符组成，但文件由字节组成。字节转化成字符依照一定的编码(encoding)方式。
为了在Python中的使用文本文档，这些字节必须被 <em class="calibre13">解码</em> 为 Unicode 的字符集。
常用的编码方式有 ASCII，Latin-1（西欧），KOI8-R（俄语）和通用编码 UTF-8 和 UTF-16。还有许多其他的编码存在</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">编码也可以称为 ‘字符集’, 但是这个术语不太准确: 单个字符集可能存在多个编码。</p>
</div>
<p class="calibre10">scikit-learn 中的文本提取器知道如何解码文本文件，
但只有当您告诉他们文件的编码的情况下才行， <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 才需要一个 <code class="docutils"><span class="calibre4">encoding</span></code> 参数。
对于现代文本文件，正确的编码可能是 UTF-8，因此它也是默认解码方式 (<code class="docutils"><span class="calibre4">encoding="utf-8"</span></code>).</p>
<p class="calibre10">如果正在加载的文本不是使用UTF-8进行编码，则会得到 <code class="docutils"><span class="calibre4">UnicodeDecodeError</span></code>.
矢量化的方式可以通过设定 <code class="docutils"><span class="calibre4">decode_error</span></code> 参数设置为  <code class="docutils"><span class="calibre4">"ignore"</span></code> 或 <code class="docutils"><span class="calibre4">"replace"``来避免抛出解码错误。</span>
<span class="calibre4">有关详细信息，请参阅Python函数</span> <span class="calibre4">``bytes.decode</span></code> 的文档（在Python提示符下键入 <code class="docutils"><span class="calibre4">help(bytes.decode)</span></code> ）。</p>
<p class="calibre10">如果您在解码文本时遇到问题，请尝试以下操作:</p>
<ul class="calibre6">
<li class="toctree-l">了解文本的实际编码方式。该文件可能带有标题或 README，告诉您编码，或者可能有一些标准编码，您可以根据文本的来源来推断编码方式。</li>
<li class="toctree-l">您可能可以使用 UNIX 命令 <code class="docutils"><span class="calibre4">file</span></code> 找出它一般使用什么样的编码。 Python <code class="docutils"><span class="calibre4">chardet</span></code> 模块附带一个名为 <code class="docutils"><span class="calibre4">chardetect.py</span></code> 的脚本，它会猜测具体的编码，尽管你不能依靠它的猜测是正确的。</li>
<li class="toctree-l">你可以尝试 UTF-8 并忽略错误。您可以使用 <code class="docutils"><span class="calibre4">bytes.decode(errors='replace')</span></code> 对字节字符串进行解码，用无意义字符替换所有解码错误，或在向量化器中设置 <code class="docutils"><span class="calibre4">decode_error='replace'</span></code>. 这可能会损坏您的功能的有用性。</li>
<li class="toctree-l">真实文本可能来自各种使用不同编码的来源，或者甚至以与编码的编码不同的编码进行粗略解码。这在从 Web 检索的文本中是常见的。Python 包 <a class="calibre3 pcalibre" href="https://github.com/LuminosoInsight/python-ftfy">ftfy</a> 可以自动排序一些解码错误类，所以您可以尝试将未知文本解码为 <code class="docutils"><span class="calibre4">latin-1</span></code>，然后使用 <code class="docutils"><span class="calibre4">ftfy</span></code> 修复错误。</li>
<li class="toctree-l">如果文本的编码的混合，那么它很难整理分类（20个新闻组数据集的情况），您可以把它们回到简单的单字节编码，如 <code class="docutils"><span class="calibre4">latin-1</span></code>。某些文本可能显示不正确，但至少相同的字节序列将始终代表相同的功能。</li>
</ul>
<p class="calibre10">例如，以下代码段使用 <code class="docutils"><span class="calibre4">chardet</span></code> （未附带 scikit-learn，必须单独安装）来计算出编码方式。然后，它将文本向量化并打印学习的词汇（特征）。输出在下方给出。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">chardet</span>    
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text1</span> <span class="calibre4">=</span> <span class="calibre4">b</span><span class="calibre4">"Sei mir gegr</span><span class="calibre4">\xc3\xbc\xc3\x9f</span><span class="calibre4">t mein Sauerkraut"</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text2</span> <span class="calibre4">=</span> <span class="calibre4">b</span><span class="calibre4">"holdselig sind deine Ger</span><span class="calibre4">\xfc</span><span class="calibre4">che"</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text3</span> <span class="calibre4">=</span> <span class="calibre4">b</span><span class="calibre4">"</span><span class="calibre4">\xff\xfe</span><span class="calibre4">A</span><span class="calibre4">\x00</span><span class="calibre4">u</span><span class="calibre4">\x00</span><span class="calibre4">f</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">F</span><span class="calibre4">\x00</span><span class="calibre4">l</span><span class="calibre4">\x00\xfc\x00</span><span class="calibre4">g</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">l</span><span class="calibre4">\x00</span><span class="calibre4">n</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">d</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">s</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">G</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">s</span><span class="calibre4">\x00</span><span class="calibre4">a</span><span class="calibre4">\x00</span><span class="calibre4">n</span><span class="calibre4">\x00</span><span class="calibre4">g</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">s</span><span class="calibre4">\x00</span><span class="calibre4">,</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">H</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">r</span><span class="calibre4">\x00</span><span class="calibre4">z</span><span class="calibre4">\x00</span><span class="calibre4">l</span><span class="calibre4">\x00</span><span class="calibre4">i</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">b</span><span class="calibre4">\x00</span><span class="calibre4">c</span><span class="calibre4">\x00</span><span class="calibre4">h</span><span class="calibre4">\x00</span><span class="calibre4">e</span><span class="calibre4">\x00</span><span class="calibre4">n</span><span class="calibre4">\x00</span><span class="calibre4">,</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">t</span><span class="calibre4">\x00</span><span class="calibre4">r</span><span class="calibre4">\x00</span><span class="calibre4">a</span><span class="calibre4">\x00</span><span class="calibre4">g</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">i</span><span class="calibre4">\x00</span><span class="calibre4">c</span><span class="calibre4">\x00</span><span class="calibre4">h</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">d</span><span class="calibre4">\x00</span><span class="calibre4">i</span><span class="calibre4">\x00</span><span class="calibre4">c</span><span class="calibre4">\x00</span><span class="calibre4">h</span><span class="calibre4">\x00</span><span class="calibre4"> </span><span class="calibre4">\x00</span><span class="calibre4">f</span><span class="calibre4">\x00</span><span class="calibre4">o</span><span class="calibre4">\x00</span><span class="calibre4">r</span><span class="calibre4">\x00</span><span class="calibre4">t</span><span class="calibre4">\x00</span><span class="calibre4">"</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">decoded</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">x</span><span class="calibre4">.</span><span class="calibre4">decode</span><span class="calibre4">(</span><span class="calibre4">chardet</span><span class="calibre4">.</span><span class="calibre4">detect</span><span class="calibre4">(</span><span class="calibre4">x</span><span class="calibre4">)[</span><span class="calibre4">'encoding'</span><span class="calibre4">])</span>
<span class="calibre4">... </span>           <span class="calibre4">for</span> <span class="calibre4">x</span> <span class="calibre4">in</span> <span class="calibre4">(</span><span class="calibre4">text1</span><span class="calibre4">,</span> <span class="calibre4">text2</span><span class="calibre4">,</span> <span class="calibre4">text3</span><span class="calibre4">)]</span>        
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">v</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">decoded</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">vocabulary_</span>    
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">term</span> <span class="calibre4">in</span> <span class="calibre4">v</span><span class="calibre4">:</span> <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">v</span><span class="calibre4">)</span>                           
</pre>
</div>
</div>
<p class="calibre10">（根据 <code class="docutils"><span class="calibre4">chardet</span></code> 的版本，可能会返回第一个值错误的结果。）
有关 Unicode 和字符编码的一般介绍，请参阅Joel Spolsky的 <a class="calibre3 pcalibre" href="http://www.joelonsoftware.com/articles/Unicode.html">绝对最小每个软件开发人员必须了解 Unicode</a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-838">
<h3 class="sigil_not_in_toc1">4.2.3.6. 应用和实例</h3>
<p class="calibre2">词汇表达方式相当简单，但在实践中却非常有用。</p>
<p class="calibre10">特别是在 <strong class="calibre14">监督学习的设置</strong> 中，它能够把快速和可扩展的线性模型组合来训练 <strong class="calibre14">文档分类器</strong>, 例如:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">使用稀疏特征对文本文档进行分类 <a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a></li>
</ul>
</div>
</blockquote>
<p class="calibre10">在 <strong class="calibre14">无监督的设置</strong> 中，可以通过应用诸如 <a class="calibre3 pcalibre" href="clustering.html#k-means"><span class="calibre4">K-means</span></a> 的聚类算法来将相似文档分组在一起：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">使用k-means聚类文本文档 <a class="calibre3 pcalibre" href="../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"><span class="calibre4">Clustering text documents using k-means</span></a></li>
</ul>
</div>
</blockquote>
<p class="calibre10">最后，通过松弛聚类的约束条件，可以通过使用非负矩阵分解（ <a class="calibre3 pcalibre" href="decomposition.html#nmf"><span class="calibre4">非负矩阵分解(NMF 或 NNMF)</span></a> 或NNMF）来发现语料库的主要主题：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">主题提取与非负矩阵分解和潜在Dirichlet分配 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="calibre4">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-839">
<h3 class="sigil_not_in_toc1">4.2.3.7. 词语表示的限制</h3>
<p class="calibre2">一组单词（什么是单词）无法捕获短语和多字表达，有效地忽略任何单词顺序依赖。另外，这个单词模型不包含潜在的拼写错误或词汇导出。</p>
<p class="calibre10">N克抢救！而不是构建一个简单的unigrams集合 (n=1)，可能更喜欢一组二进制 (n=2)，其中计算连续字对。</p>
<p class="calibre10">还可以考虑一个字符 n-gram 的集合，这是一种对拼写错误和派生有弹性的表示。</p>
<p class="calibre10">例如，假设我们正在处理两个文档的语料库： <code class="docutils"><span class="calibre4">['words',</span> <span class="calibre4">'wprds']</span></code>. 第二个文件包含 ‘words’ 一词的拼写错误。
一个简单的单词表示将把这两个视为非常不同的文档，两个可能的特征都是不同的。
然而，一个字符 2-gram 的表示可以找到匹配的文档中的8个特征中的4个，这可能有助于优选的分类器更好地决定:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">(</span><span class="calibre4">analyzer</span><span class="calibre4">=</span><span class="calibre4">'char_wb'</span><span class="calibre4">,</span> <span class="calibre4">ngram_range</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">counts</span> <span class="calibre4">=</span> <span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">([</span><span class="calibre4">'words'</span><span class="calibre4">,</span> <span class="calibre4">'wprds'</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">' w'</span><span class="calibre4">,</span> <span class="calibre4">'ds'</span><span class="calibre4">,</span> <span class="calibre4">'or'</span><span class="calibre4">,</span> <span class="calibre4">'pr'</span><span class="calibre4">,</span> <span class="calibre4">'rd'</span><span class="calibre4">,</span> <span class="calibre4">'s '</span><span class="calibre4">,</span> <span class="calibre4">'wo'</span><span class="calibre4">,</span> <span class="calibre4">'wp'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">counts</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">astype</span><span class="calibre4">(</span><span class="calibre4">int</span><span class="calibre4">)</span>
<span class="calibre4">array([[1, 1, 1, 0, 1, 1, 1, 0],</span>
<span class="calibre4">       [1, 1, 0, 1, 1, 1, 0, 1]])</span>
</pre>
</div>
</div>
<p class="calibre10">在上面的例子中，使用 <code class="docutils"><span class="calibre4">'char_wb</span></code> 分析器’，它只能从字边界内的字符（每侧填充空格）创建 n-gram。 <code class="docutils"><span class="calibre4">'char'</span></code> 分析器可以创建跨越单词的 n-gram:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">(</span><span class="calibre4">analyzer</span><span class="calibre4">=</span><span class="calibre4">'char_wb'</span><span class="calibre4">,</span> <span class="calibre4">ngram_range</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">([</span><span class="calibre4">'jumpy fox'</span><span class="calibre4">])</span>
<span class="calibre4">... </span>                               
<span class="calibre4">&lt;1x4 sparse matrix of type '&lt;... 'numpy.int64'&gt;'</span>
<span class="calibre4">   with 4 stored elements in Compressed Sparse ... format&gt;</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">' fox '</span><span class="calibre4">,</span> <span class="calibre4">' jump'</span><span class="calibre4">,</span> <span class="calibre4">'jumpy'</span><span class="calibre4">,</span> <span class="calibre4">'umpy '</span><span class="calibre4">])</span>
<span class="calibre4">True</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">(</span><span class="calibre4">analyzer</span><span class="calibre4">=</span><span class="calibre4">'char'</span><span class="calibre4">,</span> <span class="calibre4">ngram_range</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">([</span><span class="calibre4">'jumpy fox'</span><span class="calibre4">])</span>
<span class="calibre4">... </span>                               
<span class="calibre4">&lt;1x5 sparse matrix of type '&lt;... 'numpy.int64'&gt;'</span>
<span class="calibre4">    with 5 stored elements in Compressed Sparse ... format&gt;</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ngram_vectorizer</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">()</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">'jumpy'</span><span class="calibre4">,</span> <span class="calibre4">'mpy f'</span><span class="calibre4">,</span> <span class="calibre4">'py fo'</span><span class="calibre4">,</span> <span class="calibre4">'umpy '</span><span class="calibre4">,</span> <span class="calibre4">'y fox'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<p class="calibre10">对于使用白色空格进行单词分离的语言，对于语言边界感知变体 <code class="docutils"><span class="calibre4">char_wb</span></code> 尤其有趣，因为在这种情况下，它会产生比原始 <code class="docutils"><span class="calibre4">char</span></code> 变体显着更少的噪音特征。
对于这样的语言，它可以增加使用这些特征训练的分类器的预测精度和收敛速度，同时保持关于拼写错误和词导出的稳健性。</p>
<p class="calibre10">虽然可以通过提取 n-gram 而不是单独的单词来保存一些本地定位信息，但是包含 n-gram 的单词和袋子可以破坏文档的大部分内部结构，因此破坏了该内部结构的大部分含义。</p>
<p class="calibre10">为了处理自然语言理解的更广泛的任务，因此应考虑到句子和段落的地方结构。因此，许多这样的模型将被称为 “结构化输出” 问题，这些问题目前不在 scikit-learn 的范围之内。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-840">
<span id="calibre_link-841" class="calibre4"></span><h3 class="sigil_not_in_toc1">4.2.3.8. 用哈希技巧矢量化大文本语料库</h3>
<p class="calibre2">上述向量化方案是简单的，但是它存在 <strong class="calibre14">从字符串令牌到整数特征索引的内存映射</strong> （ <code class="docutils"><span class="calibre4">vocabulary_</span></code> 属性），在处理 <strong class="calibre14">大型数据集时会引起几个问题</strong> :</p>
<ul class="calibre6">
<li class="toctree-l">语料库越大，词汇量越大，使用的内存也越大.</li>
<li class="toctree-l">拟合（fitting）需要根据原始数据集的大小等比例分配中间数据结构的大小.</li>
<li class="toctree-l">构建词映射需要完整的传递数据集，因此不可能以严格在线的方式拟合文本分类器.</li>
<li class="toctree-l">pickling和un-pickling vocabulary 很大的向量器会非常慢（通常比pickling/un-pickling单纯数据的结构，比如同等大小的Numpy数组）.</li>
<li class="toctree-l">将向量化任务分隔成并行的子任务很不容易实现，因为 <code class="docutils"><span class="calibre4">vocabulary_</span></code> 属性要共享状态有一个细颗粒度的同步障碍：从标记字符串中映射特征索引与每个标记的首次出现顺序是独立的，因此应该被共享，在这点上并行worker的性能收到了损害，使他们比串行更慢。</li>
</ul>
<p class="calibre10">通过组合由 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">sklearn.feature_extraction.FeatureHasher</span></code></a> 类实现的 “散列技巧” (<a class="calibre3 pcalibre" href="#calibre_link-63"><span class="calibre4">特征哈希（相当于一种降维技巧）</span></a>) 和 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 的文本预处理和标记化功能，可以克服这些限制。</p>
<p class="calibre10">这种组合是在 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="docutils"><span class="calibre4">HashingVectorizer</span></code></a> 中实现的，该类是与 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 大部分 API 兼容的变压器类。 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="docutils"><span class="calibre4">HashingVectorizer</span></code></a> 是无状态的，这意味着您不需要 <code class="docutils"><span class="calibre4">fit</span></code> 它:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">HashingVectorizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hv</span> <span class="calibre4">=</span> <span class="calibre4">HashingVectorizer</span><span class="calibre4">(</span><span class="calibre4">n_features</span><span class="calibre4">=</span><span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hv</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">corpus</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                               
<span class="calibre4">&lt;4x10 sparse matrix of type '&lt;... 'numpy.float64'&gt;'</span>
<span class="calibre4">    with 16 stored elements in Compressed Sparse ... format&gt;</span>
</pre>
</div>
</div>
<p class="calibre10">你可以看到从向量输出中抽取了16个非0特征标记：与之前由CountVectorizer在同一个样本语料库抽取的19个非0特征要少。差异来自哈希方法的冲突，因为较低的n_features参数的值。</p>
<p class="calibre10">在真实世界的环境下，n_features参数可以使用默认值2 ** 20（将近100万可能的特征）。如果内存或者下游模型的大小是一个问题，那么选择一个较小的值比如2 ** 18可能有一些帮助，而不需要为典型的文本分类任务引入太多额外的冲突。</p>
<p class="calibre10">注意维度并不影响CPU的算法训练时间，这部分是在操作CSR指标（LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive），但是，它对CSC matrices (LinearSVC(dual=False), Lasso(), etc)算法有效。</p>
<p class="calibre10">让我们再次尝试使用默认设置:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hv</span> <span class="calibre4">=</span> <span class="calibre4">HashingVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">hv</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">corpus</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                              
<span class="calibre4">&lt;4x1048576 sparse matrix of type '&lt;... 'numpy.float64'&gt;'</span>
<span class="calibre4">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre>
</div>
</div>
<p class="calibre10">冲突没有再出现，但是，代价是输出空间的维度值非常大。当然，这里使用的19词以外的其他词之前仍会有冲突。</p>
<p class="calibre10">类 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="docutils"><span class="calibre4">HashingVectorizer</span></code></a> 还具有以下限制：</p>
<ul class="calibre6">
<li class="toctree-l">不能反转模型（没有inverse_transform方法），也无法访问原始的字符串表征，因为，进行mapping的哈希方法是单向本性。</li>
<li class="toctree-l">没有提供了IDF权重，因为这需要在模型中引入状态。如果需要的话，可以在管道中添加 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="docutils"><span class="calibre4">TfidfTransformer</span></code></a> 。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-842">
<h3 class="sigil_not_in_toc1">4.2.3.9. 使用 HashingVectorizer 执行外核缩放</h3>
<p class="calibre2">使用 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="docutils"><span class="calibre4">HashingVectorizer</span></code></a> 的一个有趣的开发是执行外核 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm">out-of-core</a> 缩放的能力。 这意味着我们可以从无法放入电脑主内存的数据中进行学习。</p>
<p class="calibre10">实现核外扩展的一个策略是将数据以流的方式以一小批提交给评估器。每批的向量化都是用HashingVectorizer这样来保证评估器的输入空间的维度是相等的。因此任何时间使用的内存数都限定在小频次的大小。
尽管用这种方法可以处理的数据没有限制，但是从实用角度学习时间受到想要在这个任务上花费的CPU时间的限制。</p>
<p class="calibre10">对于文本分类任务中的外核缩放的完整示例，请参阅文本文档的外核分类 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="calibre4">Out-of-core classification of text documents</span></a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-843">
<h3 class="sigil_not_in_toc1">4.2.3.10. 自定义矢量化器类</h3>
<p class="calibre2">通过将可调用传递给向量化程序构造函数可以定制行为:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">my_tokenizer</span><span class="calibre4">(</span><span class="calibre4">s</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">return</span> <span class="calibre4">s</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">()</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">(</span><span class="calibre4">tokenizer</span><span class="calibre4">=</span><span class="calibre4">my_tokenizer</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">build_analyzer</span><span class="calibre4">()(</span><span class="calibre4">u</span><span class="calibre4">"Some... punctuation!"</span><span class="calibre4">)</span> <span class="calibre4">==</span> <span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">[</span><span class="calibre4">'some...'</span><span class="calibre4">,</span> <span class="calibre4">'punctuation!'</span><span class="calibre4">])</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
<p class="calibre10">特别是我们命名：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">预处理器</span></code>: 可以将整个文档作为输入（作为单个字符串）的可调用，并返回文档的可能转换的版本，仍然是整个字符串。这可以用于删除HTML标签，小写整个文档等。</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">tokenizer</span></code>: 一个可从预处理器接收输出并将其分成标记的可调用函数，然后返回这些列表。</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">分析器</span></code>: 一个可替代预处理程序和标记器的可调用程序。默认分析仪都会调用预处理器和刻录机，但是自定义分析仪将会跳过这个。
N-gram提取和停止字过滤在分析器级进行，因此定制分析器可能必须重现这些步骤。</li>
</ul>
</div>
</blockquote>
<p class="calibre10">（Lucene 用户可能会识别这些名称，但请注意，scikit-learn 概念可能无法一对一映射到 Lucene 概念上。）</p>
<p class="calibre10">为了使预处理器，标记器和分析器了解模型参数，可以从类派生并覆盖 <code class="docutils"><span class="calibre4">build_preprocessor</span></code>, <code class="docutils"><span class="calibre4">build_tokenizer`</span></code> 和 <code class="docutils"><span class="calibre4">build_analyzer</span></code> 工厂方法，而不是传递自定义函数。</p>
<p class="calibre10">一些提示和技巧:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">如果文档由外部包进行预先标记，则将它们存储在文件（或字符串）中，令牌由空格分隔，并通过 <code class="docutils"><span class="calibre4">analyzer=str.split</span></code></li>
<li class="toctree-l">Fancy 令牌级分析，如词干，词法，复合分割，基于词性的过滤等不包括在 scikit-learn 代码库中，但可以通过定制分词器或分析器来添加。</li>
</ul>
<p class="calibre10">这是一个 <code class="docutils"><span class="calibre4">CountVectorizer</span></code>, 使用 <a class="calibre3 pcalibre" href="http://www.nltk.org">NLTK</a> 的 tokenizer 和 lemmatizer:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>    &gt;&gt;&gt; from nltk import word_tokenize          # doctest: +SKIP
    &gt;&gt;&gt; from nltk.stem import WordNetLemmatizer # doctest: +SKIP
    &gt;&gt;&gt; class LemmaTokenizer(object):
    ...     def __init__(self):
    ...         self.wnl = WordNetLemmatizer()
    ...     def __call__(self, doc):
    ...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
    ...
    &gt;&gt;&gt; vect = CountVectorizer(tokenizer=LemmaTokenizer())  # doctest: +SKIP

（请注意，这不会过滤标点符号。）
例如，以下例子将英国的一些拼写变成美国拼写::

    &gt;&gt;&gt; import re
    &gt;&gt;&gt; def to_british(tokens):
    ...     for t in tokens:
    ...         t = re.sub(r"(...)our$", r"\1or", t)
    ...         t = re.sub(r"([bt])re$", r"\1er", t)
    ...         t = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", t)
    ...         t = re.sub(r"ogue$", "og", t)
    ...         yield t
    ...
    &gt;&gt;&gt; class CustomVectorizer(CountVectorizer):
    ...     def build_tokenizer(self):
    ...         tokenize = super(CustomVectorizer, self).build_tokenizer()
    ...         return lambda doc: list(to_british(tokenize(doc)))
    ...
    &gt;&gt;&gt; print(CustomVectorizer().build_analyzer()(u"color colour")) # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
    [...'color', ...'color']

用于其他样式的预处理; 例子包括 stemming, lemmatization, 或 normalizing numerical tokens, 后者说明如下:

 * :ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`
</pre>
</div>
</div>
</div>
</blockquote>
<p class="calibre10">在处理不使用显式字分隔符（例如空格）的亚洲语言时，自定义向量化器也是有用的。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-844">
<span id="calibre_link-845" class="calibre4"></span><h2 class="sigil_not_in_toc">4.2.4. 图像特征提取</h2>
<div class="toctree-wrapper" id="calibre_link-846">
<h3 class="sigil_not_in_toc1">4.2.4.1. 补丁提取</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="docutils"><span class="calibre4">extract_patches_2d</span></code></a> 函数从存储为二维数组的图像或沿着第三轴的颜色信息三维提取修补程序。
要从其所有补丁重建图像，请使用 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d.html#sklearn.feature_extraction.image.reconstruct_from_patches_2d" title="sklearn.feature_extraction.image.reconstruct_from_patches_2d"><code class="docutils"><span class="calibre4">reconstruct_from_patches_2d</span></code></a>. 例如让我们使用3个彩色通道（例如 RGB 格式）生成一个 4x4 像素的图像:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction</span> <span class="calibre4">import</span> <span class="calibre4">image</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">one_image</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">4</span> <span class="calibre4">*</span> <span class="calibre4">4</span> <span class="calibre4">*</span> <span class="calibre4">3</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">((</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">one_image</span><span class="calibre4">[:,</span> <span class="calibre4">:,</span> <span class="calibre4">0</span><span class="calibre4">]</span>  <span class="calibre4"># R channel of a fake RGB picture</span>
<span class="calibre4">array([[ 0,  3,  6,  9],</span>
<span class="calibre4">       [12, 15, 18, 21],</span>
<span class="calibre4">       [24, 27, 30, 33],</span>
<span class="calibre4">       [36, 39, 42, 45]])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span> <span class="calibre4">=</span> <span class="calibre4">image</span><span class="calibre4">.</span><span class="calibre4">extract_patches_2d</span><span class="calibre4">(</span><span class="calibre4">one_image</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">),</span> <span class="calibre4">max_patches</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(2, 2, 2, 3)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span><span class="calibre4">[:,</span> <span class="calibre4">:,</span> <span class="calibre4">:,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">array([[[ 0,  3],</span>
<span class="calibre4">        [12, 15]],</span>

<span class="calibre4">       [[15, 18],</span>
<span class="calibre4">        [27, 30]]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span> <span class="calibre4">=</span> <span class="calibre4">image</span><span class="calibre4">.</span><span class="calibre4">extract_patches_2d</span><span class="calibre4">(</span><span class="calibre4">one_image</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(9, 2, 2, 3)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span><span class="calibre4">[</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">:,</span> <span class="calibre4">:,</span> <span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">array([[15, 18],</span>
<span class="calibre4">       [27, 30]])</span>
</pre>
</div>
</div>
<p class="calibre10">现在让我们尝试通过在重叠区域进行平均来从补丁重建原始图像:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">reconstructed</span> <span class="calibre4">=</span> <span class="calibre4">image</span><span class="calibre4">.</span><span class="calibre4">reconstruct_from_patches_2d</span><span class="calibre4">(</span><span class="calibre4">patches</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">testing</span><span class="calibre4">.</span><span class="calibre4">assert_array_equal</span><span class="calibre4">(</span><span class="calibre4">one_image</span><span class="calibre4">,</span> <span class="calibre4">reconstructed</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.PatchExtractor.html#sklearn.feature_extraction.image.PatchExtractor" title="sklearn.feature_extraction.image.PatchExtractor"><code class="docutils"><span class="calibre4">PatchExtractor</span></code></a> 以同样的方式类作品 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="docutils"><span class="calibre4">extract_patches_2d</span></code></a>, 只是它支持多种图像作为输入。它被实现为一个估计器，因此它可以在管道中使用。看到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">five_images</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">5</span> <span class="calibre4">*</span> <span class="calibre4">4</span> <span class="calibre4">*</span> <span class="calibre4">4</span> <span class="calibre4">*</span> <span class="calibre4">3</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span> <span class="calibre4">=</span> <span class="calibre4">image</span><span class="calibre4">.</span><span class="calibre4">PatchExtractor</span><span class="calibre4">((</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">))</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">five_images</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">patches</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(45, 2, 2, 3)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-847">
<h3 class="sigil_not_in_toc1">4.2.4.2. 图像的连接图</h3>
<p class="calibre2">scikit-learn 中的几个估计可以使用特征或样本之间的连接信息。
例如，Ward聚类（层次聚类 <a class="calibre3 pcalibre" href="clustering.html#hierarchical-clustering"><span class="calibre4">层次聚类</span></a> ）可以聚集在一起，只有图像的相邻像素，从而形成连续的斑块:</p>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_ward_segmentation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_face_ward_segmentation_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000520.jpg" class="calibre58" /></a>
</div>
<p class="calibre10">为此，估计器使用 ‘连接性’ 矩阵，给出连接的样本。</p>
<p class="calibre10">该函数 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.img_to_graph.html#sklearn.feature_extraction.image.img_to_graph" title="sklearn.feature_extraction.image.img_to_graph"><code class="docutils"><span class="calibre4">img_to_graph</span></code></a> 从2D或3D图像返回这样一个矩阵。类似地，<a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code class="docutils"><span class="calibre4">grid_to_graph</span></code></a> 为给定这些图像的形状的图像构建连接矩阵。</p>
<p class="calibre10">这些矩阵可用于在使用连接信息的估计器中强加连接，如 Ward 聚类（层次聚类 <a class="calibre3 pcalibre" href="clustering.html#hierarchical-clustering"><span class="calibre4">层次聚类</span></a> ），而且还要构建预计算的内核或相似矩阵。</p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10"><strong class="calibre14">示例</strong></p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_face_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-face-ward-segmentation-py"><span class="calibre4">A demo of structured Ward hierarchical clustering on a raccoon face image</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py"><span class="calibre4">Spectral clustering for image segmentation</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="calibre4">Feature agglomeration vs. univariate selection</span></a></li>
</ul>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-82">
<span id="calibre_link-848" class="calibre4"></span><h1 class="calibre5">4.3. 预处理数据</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@if only</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Trembleguy</a><br class="calibre9" />
    </div>
<p class="calibre10"><code class="docutils"><span class="calibre4">sklearn.preprocessing</span></code> 包提供了几个常见的实用功能和变换器类型，用来将原始特征向量更改为更适合机器学习模型的形式。</p>
<p class="calibre10">一般来说，机器学习算法受益于数据集的标准化。如果数据集中存在一些离群值，那么稳定的缩放或转换更合适。不同缩放、转换以及归一在一个包含边缘离群值的数据集中的表现在 <a class="calibre3 pcalibre" href="../auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py"><span class="calibre4">Compare the effect of different scalers on data with outliers</span></a> 中有着重说明。</p>
<div class="toctree-wrapper" id="calibre_link-849">
<span id="calibre_link-850" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.1. 标准化，也称去均值和方差按比例缩放</h2>
<p class="calibre2">数据集的 <strong class="calibre14">标准化</strong> 对scikit-learn中实现的大多数机器学习算法来说是 <strong class="calibre14">常见的要求</strong> 。如果个别特征或多或少看起来不是很像标准正态分布(<strong class="calibre14">具有零均值和单位方差</strong>)，那么它们的表现力可能会较差。</p>
<p class="calibre10">在实际情况中,我们经常忽略特征的分布形状，直接经过去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p>
<p class="calibre10">例如，在机器学习算法的目标函数(例如SVM的RBF内核或线性模型的l1和l2正则化)，许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。</p>
<p class="calibre10">函数 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" title="sklearn.preprocessing.scale"><code class="docutils"><span class="calibre4">scale</span></code></a> 为数组形状的数据集的标准化提供了一个快捷实现:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">preprocessing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">2.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">2.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_scaled</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">scale</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_scaled</span>                                          
<span class="calibre4">array([[ 0.  ..., -1.22...,  1.33...],</span>
<span class="calibre4">       [ 1.22...,  0.  ..., -0.26...],</span>
<span class="calibre4">       [-1.22...,  1.22..., -1.06...]])</span>
</pre>
</div>
</div>
<p class="calibre10">经过缩放后的数据具有零均值以及标准方差:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_scaled</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">array([ 0.,  0.,  0.])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_scaled</span><span class="calibre4">.</span><span class="calibre4">std</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">array([ 1.,  1.,  1.])</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">预处理</span></code> 模块还提供了一个实用类 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">StandardScaler</span></code></a> ，它实现了转化器的API来计算训练集上的平均值和标准偏差，以便以后能够在测试集上重新应用相同的变换。因此，这个类适用于 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline</span></code></a> 的早期步骤:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">StandardScaler</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span>
<span class="calibre4">StandardScaler(copy=True, with_mean=True, with_std=True)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">mean_</span>                                      
<span class="calibre4">array([ 1. ...,  0. ...,  0.33...])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">scale_</span>                                       
<span class="calibre4">array([ 0.81...,  0.81...,  1.24...])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>                           
<span class="calibre4">array([[ 0.  ..., -1.22...,  1.33...],</span>
<span class="calibre4">       [ 1.22...,  0.  ..., -0.26...],</span>
<span class="calibre4">       [-1.22...,  1.22..., -1.06...]])</span>
</pre>
</div>
</div>
<p class="calibre10">缩放类对象可以在新的数据上实现和训练集相同缩放操作:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>                
<span class="calibre4">array([[-2.44...,  1.22..., -0.26...]])</span>
</pre>
</div>
</div>
<p class="calibre10">你也可以通过在构造函数 :class:StandardScaler 中传入参数 <code class="docutils"><span class="calibre4">with_mean=False`</span> <span class="calibre4">或者``with_std=False</span></code> 来取消中心化或缩放操作。</p>
<div class="toctree-wrapper" id="calibre_link-851">
<h3 class="sigil_not_in_toc1">4.3.1.1. 将特征缩放至特定范围内</h3>
<p class="calibre2">一种标准化是将特征缩放到给定的最小值和最大值之间，通常在零和一之间，或者也可以将每个特征的最大绝对值转换至单位大小。可以分别使用 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler"><code class="docutils"><span class="calibre4">MinMaxScaler</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler" title="sklearn.preprocessing.MaxAbsScaler"><code class="docutils"><span class="calibre4">MaxAbsScaler</span></code></a> 实现。</p>
<p class="calibre10">使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素。</p>
<p class="calibre10">以下是一个将简单的数据矩阵缩放到``[0, 1]``的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">2.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">2.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">]])</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">min_max_scaler</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">MinMaxScaler</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_minmax</span> <span class="calibre4">=</span> <span class="calibre4">min_max_scaler</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_minmax</span>
<span class="calibre4">array([[ 0.5       ,  0.        ,  1.        ],</span>
<span class="calibre4">       [ 1.        ,  0.5       ,  0.33333333],</span>
<span class="calibre4">       [ 0.        ,  1.        ,  0.        ]])</span>
</pre>
</div>
</div>
<p class="calibre10">同样的转换实例可以被用与在训练过程中不可见的测试数据:实现和训练数据一致的缩放和移位操作:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span> <span class="calibre4">-</span><span class="calibre4">3.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">4.</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_minmax</span> <span class="calibre4">=</span> <span class="calibre4">min_max_scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_minmax</span>
<span class="calibre4">array([[-1.5       ,  0.        ,  1.66666667]])</span>
</pre>
</div>
</div>
<p class="calibre10">可以检查缩放器（scaler）属性，来观察在训练集中学习到的转换操作的基本性质:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">min_max_scaler</span><span class="calibre4">.</span><span class="calibre4">scale_</span>                             
<span class="calibre4">array([ 0.5       ,  0.5       ,  0.33...])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">min_max_scaler</span><span class="calibre4">.</span><span class="calibre4">min_</span>                               
<span class="calibre4">array([ 0.        ,  0.5       ,  0.33...])</span>
</pre>
</div>
</div>
<p class="calibre10">如果给 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler"><code class="docutils"><span class="calibre4">MinMaxScaler</span></code></a> 提供一个明确的 <code class="docutils"><span class="calibre4">feature_range=(min,</span> <span class="calibre4">max)</span></code> ，完整的公式是:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">X_std</span> <span class="calibre4">=</span> <span class="calibre4">(</span><span class="calibre4">X</span> <span class="calibre4">-</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">min</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span> <span class="calibre4">/</span> <span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">max</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span> <span class="calibre4">-</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">min</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span>

<span class="calibre4">X_scaled</span> <span class="calibre4">=</span> <span class="calibre4">X_std</span> <span class="calibre4">*</span> <span class="calibre4">(</span><span class="calibre4">max</span> <span class="calibre4">-</span> <span class="calibre4">min</span><span class="calibre4">)</span> <span class="calibre4">+</span> <span class="calibre4">min</span>
</pre>
</div>
</div>
<p class="calibre10">类 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler" title="sklearn.preprocessing.MaxAbsScaler"><code class="docutils"><span class="calibre4">MaxAbsScaler</span></code></a> 的工作原理非常相似，但是它只通过除以每个特征的最大值将训练数据特征缩放至 <code class="docutils"><span class="calibre4">[-1,</span> <span class="calibre4">1]</span></code> 范围内，这就意味着，训练数据应该是已经零中心化或者是稀疏数据。 例子::用先前例子的数据实现最大绝对值缩放操作。</p>
<p class="calibre10">以下是使用上例中数据运用这个缩放器的例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">2.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">2.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>                    <span class="calibre4">[</span> <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">]])</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">max_abs_scaler</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">MaxAbsScaler</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_maxabs</span> <span class="calibre4">=</span> <span class="calibre4">max_abs_scaler</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_maxabs</span>                <span class="calibre4"># doctest +NORMALIZE_WHITESPACE^</span>
<span class="calibre4">array([[ 0.5, -1. ,  1. ],</span>
<span class="calibre4">       [ 1. ,  0. ,  0. ],</span>
<span class="calibre4">       [ 0. ,  1. , -0.5]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span> <span class="calibre4">-</span><span class="calibre4">3.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">4.</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_maxabs</span> <span class="calibre4">=</span> <span class="calibre4">max_abs_scaler</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_maxabs</span>                 
<span class="calibre4">array([[-1.5, -1. ,  2. ]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">max_abs_scaler</span><span class="calibre4">.</span><span class="calibre4">scale_</span>         
<span class="calibre4">array([ 2.,  1.,  2.])</span>
</pre>
</div>
</div>
<p class="calibre10">在 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" title="sklearn.preprocessing.scale"><code class="docutils"><span class="calibre4">scale</span></code></a> 模块中进一步提供了方便的功能。当你不想创建对象时，可以使用如 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.minmax_scale.html#sklearn.preprocessing.minmax_scale" title="sklearn.preprocessing.minmax_scale"><code class="docutils"><span class="calibre4">minmax_scale</span></code></a> 以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.maxabs_scale.html#sklearn.preprocessing.maxabs_scale" title="sklearn.preprocessing.maxabs_scale"><code class="docutils"><span class="calibre4">maxabs_scale</span></code></a> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-852">
<h3 class="sigil_not_in_toc1">4.3.1.2. 缩放稀疏（矩阵）数据</h3>
<p class="calibre2">中心化稀疏(矩阵)数据会破坏数据的稀疏结构，因此很少有一个比较明智的实现方式。但是缩放稀疏输入是有意义的，尤其是当几个特征在不同的量级范围时。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler" title="sklearn.preprocessing.MaxAbsScaler"><code class="docutils"><span class="calibre4">MaxAbsScaler</span></code></a> 以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.maxabs_scale.html#sklearn.preprocessing.maxabs_scale" title="sklearn.preprocessing.maxabs_scale"><code class="docutils"><span class="calibre4">maxabs_scale</span></code></a> 是专为缩放数据而设计的，并且是缩放数据的推荐方法。但是， <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" title="sklearn.preprocessing.scale"><code class="docutils"><span class="calibre4">scale</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">StandardScaler</span></code></a> 也能够接受 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 作为输入，只要参数 <code class="docutils"><span class="calibre4">with_mean=False</span></code> 被准确传入它的构造器。否则会出现 <code class="docutils"><span class="calibre4">ValueError</span></code> 的错误，因为默认的中心化会破坏稀疏性，并且经常会因为分配过多的内存而使执行崩溃。 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler" title="sklearn.preprocessing.RobustScaler"><code class="docutils"><span class="calibre4">RobustScaler</span></code></a> 不能适应稀疏输入，但你可以在稀疏输入使用 <code class="docutils"><span class="calibre4">transform</span></code> 方法。</p>
<p class="calibre10">注意，缩放器同时接受压缩的稀疏行和稀疏列(参见 <code class="docutils"><span class="calibre4">scipy.sparse.csr_matrix</span></code> 以及 <code class="docutils"><span class="calibre4">scipy.sparse.csc_matrix</span></code> )。任何其他稀疏输入将会 <strong class="calibre14">转化为压缩稀疏行表示</strong> 。为了避免不必要的内存复制，建议在上游(早期)选择CSR或CSC表示。</p>
<p class="calibre10">最后，最后，如果已经中心化的数据并不是很大，使用 <code class="docutils"><span class="calibre4">toarray</span></code> 方法将输入的稀疏矩阵显式转换为数组是另一种选择。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-853">
<h3 class="sigil_not_in_toc1">4.3.1.3. 缩放有离群值的数据</h3>
<p class="calibre2">如果你的数据包含许多异常值，使用均值和方差缩放可能并不是一个很好的选择。这种情况下，你可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.robust_scale.html#sklearn.preprocessing.robust_scale" title="sklearn.preprocessing.robust_scale"><code class="docutils"><span class="calibre4">robust_scale</span></code></a> 以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler" title="sklearn.preprocessing.RobustScaler"><code class="docutils"><span class="calibre4">RobustScaler</span></code></a> 作为替代品。它们对你的数据的中心和范围使用更有鲁棒性的估计。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<p class="calibre10">更多关于中心化和缩放数据的重要性讨论在此FAQ中提及: <a class="calibre3 pcalibre" href="http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html">Should I normalize/standardize/rescale the data?</a></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Scaling vs Whitening
有时候独立地中心化和缩放数据是不够的，因为下游的机器学习模型能够对特征之间的线性依赖做出一些假设(这对模型的学习过程来说是不利的)。</p>
<p class="calibre10">要解决这个问题，你可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">sklearn.decomposition.PCA</span></code></a> 或 <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.RandomizedPCA.html#sklearn.decomposition.RandomizedPCA" title="sklearn.decomposition.RandomizedPCA"><code class="docutils"><span class="calibre4">sklearn.decomposition.RandomizedPCA</span></code></a> 并指定参数 <code class="docutils"><span class="calibre4">whiten=True</span></code> 来更多移除特征间的线性关联。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">在回归中缩放目标变量</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale" title="sklearn.preprocessing.scale"><code class="docutils"><span class="calibre4">scale</span></code></a> 以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">StandardScaler</span></code></a> 可以直接处理一维数组。在回归中，缩放目标/相应变量时非常有用。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-854">
<span id="calibre_link-855" class="calibre4"></span><h3 class="sigil_not_in_toc1">4.3.1.4. 核矩阵的中心化</h3>
<p class="calibre2">如果你有一个核矩阵 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000444.jpg" alt="K" /> ，它计算由函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000455.jpg" alt="phi" /> 定义的特征空间的点积，那么一个 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.KernelCenterer.html#sklearn.preprocessing.KernelCenterer" title="sklearn.preprocessing.KernelCenterer"><code class="docutils"><span class="calibre4">KernelCenterer</span></code></a> 类能够转化这个核矩阵，通过移除特征空间的平均值，使它包含由函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000455.jpg" alt="phi" /> 定义的内部产物。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-856">
<span id="calibre_link-857" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.2. 非线性转换</h2>
<p class="calibre2">类似于缩放， <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer" title="sklearn.preprocessing.QuantileTransformer"><code class="docutils"><span class="calibre4">QuantileTransformer</span></code></a> 类将每个特征缩放在同样的范围或分布情况下。但是，通过执行一个秩转换能够使异常的分布平滑化，并且能够比缩放更少地受到离群值的影响。但是它的确使特征间及特征内的关联和距离失真了。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer" title="sklearn.preprocessing.QuantileTransformer"><code class="docutils"><span class="calibre4">QuantileTransformer</span></code></a> 类以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.quantile_transform.html#sklearn.preprocessing.quantile_transform" title="sklearn.preprocessing.quantile_transform"><code class="docutils"><span class="calibre4">quantile_transform</span></code></a> 函数提供了一个基于分位数函数的无参数转换，将数据映射到了零到一的均匀分布上:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_iris</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">train_test_split</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">train_test_split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">quantile_transformer</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">QuantileTransformer</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_trans</span> <span class="calibre4">=</span> <span class="calibre4">quantile_transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test_trans</span> <span class="calibre4">=</span> <span class="calibre4">quantile_transformer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">percentile</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">[:,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">25</span><span class="calibre4">,</span> <span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">75</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span> 
<span class="calibre4">array([ 4.3,  5.1,  5.8,  6.5,  7.9])</span>
</pre>
</div>
</div>
<p class="calibre10">这个特征是萼片的厘米单位的长度。一旦应用分位数转换，这些元素就接近于之前定义的百分位数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">percentile</span><span class="calibre4">(</span><span class="calibre4">X_train_trans</span><span class="calibre4">[:,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">25</span><span class="calibre4">,</span> <span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">75</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])</span>
</pre>
</div>
</div>
<p class="calibre10">这可以在具有类似形式的独立测试集上确认:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">percentile</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">[:,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">25</span><span class="calibre4">,</span> <span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">75</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">percentile</span><span class="calibre4">(</span><span class="calibre4">X_test_trans</span><span class="calibre4">[:,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">25</span><span class="calibre4">,</span> <span class="calibre4">50</span><span class="calibre4">,</span> <span class="calibre4">75</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">])</span>
<span class="calibre4">... </span>
<span class="calibre4">array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])</span>
</pre>
</div>
</div>
<p class="calibre10">也可以通过设置 <code class="docutils"><span class="calibre4">output_distribution='normal'</span></code> 将转换后的数据映射到正态分布:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">quantile_transformer</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">QuantileTransformer</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">output_distribution</span><span class="calibre4">=</span><span class="calibre4">'normal'</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_trans</span> <span class="calibre4">=</span> <span class="calibre4">quantile_transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">quantile_transformer</span><span class="calibre4">.</span><span class="calibre4">quantiles_</span> 
<span class="calibre4">array([[ 4.3...,   2...,     1...,     0.1...],</span>
<span class="calibre4">       [ 4.31...,  2.02...,  1.01...,  0.1...],</span>
<span class="calibre4">       [ 4.32...,  2.05...,  1.02...,  0.1...],</span>
<span class="calibre4">       ...,</span>
<span class="calibre4">       [ 7.84...,  4.34...,  6.84...,  2.5...],</span>
<span class="calibre4">       [ 7.87...,  4.37...,  6.87...,  2.5...],</span>
<span class="calibre4">       [ 7.9...,   4.4...,   6.9...,   2.5...]])</span>
</pre>
</div>
</div>
<p class="calibre10">这样，输入的中值称为输出的平均值，并且以0为中心。正常输出被剪切，使得输入的最小和最大值分别对应于1e-7和1-1e-7分位数&mdash;&mdash;在变换下不会变得无限大。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-858">
<span id="calibre_link-859" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.3. 归一化</h2>
<p class="calibre2"><strong class="calibre14">归一化</strong> 是 <strong class="calibre14">缩放单个样本以具有单位范数</strong> 的过程。如果你计划使用二次形式(如点积或任何其他核函数)来量化任何样本间的相似度，则此过程将非常有用。</p>
<p class="calibre10">这个观点基于 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Vector_Space_Model">向量空间模型(Vector Space Model)</a> ，经常在文本分类和内容聚类中使用.</p>
<p class="calibre10">函数 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize" title="sklearn.preprocessing.normalize"><code class="docutils"><span class="calibre4">normalize</span></code></a> 提供了一个快速简单的方法在类似数组的数据集上执行操作，使用 <code class="docutils"><span class="calibre4">l1</span></code> 或 <code class="docutils"><span class="calibre4">l2</span></code> 范式:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">2.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>     <span class="calibre4">[</span> <span class="calibre4">2.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>     <span class="calibre4">[</span> <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_normalized</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">normalize</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">norm</span><span class="calibre4">=</span><span class="calibre4">'l2'</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_normalized</span>                                      
<span class="calibre4">array([[ 0.40..., -0.40...,  0.81...],</span>
<span class="calibre4">       [ 1.  ...,  0.  ...,  0.  ...],</span>
<span class="calibre4">       [ 0.  ...,  0.70..., -0.70...]])</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">preprocessing</span></code> 预处理模块提供的 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer"><code class="docutils"><span class="calibre4">Normalizer</span></code></a> 工具类使用 <code class="docutils"><span class="calibre4">Transformer</span></code> API 实现了相同的操作(即使在这种情况下， <code class="docutils"><span class="calibre4">fit</span></code> 方法是无用的：该类是无状态的，因为该操作独立对待样本).</p>
<p class="calibre10">因此这个类适用于 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline</span></code></a> 的早期步骤:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">normalizer</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">Normalizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>  <span class="calibre4"># fit does nothing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">normalizer</span>
<span class="calibre4">Normalizer(copy=True, norm='l2')</span>
</pre>
</div>
</div>
<p class="calibre10">在这之后归一化实例可以被使用在样本向量中，像任何其他转换器一样:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">normalizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>                            
<span class="calibre4">array([[ 0.40..., -0.40...,  0.81...],</span>
<span class="calibre4">       [ 1.  ...,  0.  ...,  0.  ...],</span>
<span class="calibre4">       [ 0.  ...,  0.70..., -0.70...]])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">normalizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([[</span><span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">0.</span><span class="calibre4">]])</span>             
<span class="calibre4">array([[-0.70...,  0.70...,  0.  ...]])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">稀疏(数据)输入</p>
<p class="calibre10">函数 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize" title="sklearn.preprocessing.normalize"><code class="docutils"><span class="calibre4">normalize</span></code></a> 以及类 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer"><code class="docutils"><span class="calibre4">Normalizer</span></code></a> 接收 <strong class="calibre14">来自scipy.sparse的密集类数组数据和稀疏矩阵</strong> 作为输入。</p>
<p class="calibre10">对于稀疏输入，在被提交给高效Cython例程前，数据被 <strong class="calibre14">转化为压缩的稀疏行形式</strong> (参见 <code class="docutils"><span class="calibre4">scipy.sparse.csr_matrix</span></code> )。为了避免不必要的内存复制，推荐在上游选择CSR表示。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-860">
<span id="calibre_link-861" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.4. 二值化</h2>
<div class="toctree-wrapper" id="calibre_link-862">
<h3 class="sigil_not_in_toc1">4.3.4.1. 特征二值化</h3>
<p class="calibre2"><strong class="calibre14">特征二值化</strong> 是 <strong class="calibre14">将数值特征用阈值过滤得到布尔值</strong> 的过程。这对于下游的概率型模型是有用的，它们假设输入数据是多值 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">伯努利分布(Bernoulli distribution)</a> 。例如这个例子 <a class="calibre3 pcalibre" href="generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM"><code class="docutils"><span class="calibre4">sklearn.neural_network.BernoulliRBM</span></code></a> 。</p>
<p class="calibre10">即使归一化计数(又名术语频率)和TF-IDF值特征在实践中表现稍好一些，文本处理团队也常常使用二值化特征值(这可能会简化概率估计)。</p>
<p class="calibre10">相比于 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer"><code class="docutils"><span class="calibre4">Normalizer</span></code></a> ，实用程序类 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer" title="sklearn.preprocessing.Binarizer"><code class="docutils"><span class="calibre4">Binarizer</span></code></a> 也被用于 <a class="calibre3 pcalibre" href="generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="sklearn.pipeline.Pipeline"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline</span></code></a> 的早期步骤中。因为每个样本被当做是独立于其他样本的，所以 <code class="docutils"><span class="calibre4">fit</span></code> 方法是无用的:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span> <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">,</span>  <span class="calibre4">2.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>     <span class="calibre4">[</span> <span class="calibre4">2.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">0.</span><span class="calibre4">],</span>
<span class="calibre4">... </span>     <span class="calibre4">[</span> <span class="calibre4">0.</span><span class="calibre4">,</span>  <span class="calibre4">1.</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1.</span><span class="calibre4">]]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">binarizer</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">Binarizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>  <span class="calibre4"># fit does nothing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">binarizer</span>
<span class="calibre4">Binarizer(copy=True, threshold=0.0)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">binarizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[ 1.,  0.,  1.],</span>
<span class="calibre4">       [ 1.,  0.,  0.],</span>
<span class="calibre4">       [ 0.,  1.,  0.]])</span>
</pre>
</div>
</div>
<p class="calibre10">也可以为二值化器赋一个阈值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">binarizer</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">Binarizer</span><span class="calibre4">(</span><span class="calibre4">threshold</span><span class="calibre4">=</span><span class="calibre4">1.1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">binarizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[ 0.,  0.,  1.],</span>
<span class="calibre4">       [ 1.,  0.,  0.],</span>
<span class="calibre4">       [ 0.,  0.,  0.]])</span>
</pre>
</div>
</div>
<p class="calibre10">相比于 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">StandardScaler</span></code></a> 和 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer" title="sklearn.preprocessing.Normalizer"><code class="docutils"><span class="calibre4">Normalizer</span></code></a> 类的情况，预处理模块提供了一个相似的函数 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.binarize.html#sklearn.preprocessing.binarize" title="sklearn.preprocessing.binarize"><code class="docutils"><span class="calibre4">binarize</span></code></a> ，以便不需要转换接口时使用。</p>
<div class="toctree-wrapper">
<p class="calibre10">稀疏输入</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.binarize.html#sklearn.preprocessing.binarize" title="sklearn.preprocessing.binarize"><code class="docutils"><span class="calibre4">binarize</span></code></a> 以及 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Binarizer.html#sklearn.preprocessing.Binarizer" title="sklearn.preprocessing.Binarizer"><code class="docutils"><span class="calibre4">Binarizer</span></code></a> 接收 <strong class="calibre14">来自scipy.sparse的密集类数组数据以及稀疏矩阵作为输入</strong> 。</p>
<p class="calibre10">对于稀疏输入，数据被 <strong class="calibre14">转化为压缩的稀疏行形式</strong> (参见 <code class="docutils"><span class="calibre4">scipy.sparse.csr_matrix</span></code> )。为了避免不必要的内存复制，推荐在上游选择CSR表示。</p>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-863">
<span id="calibre_link-864" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.5. 分类特征编码</h2>
<p class="calibre2">在机器学习中，特征经常不是数值型的而是分类型的。举个例子，一个人可能有 <code class="docutils"><span class="calibre4">["male",</span> <span class="calibre4">"female"]</span></code> ， <code class="docutils"><span class="calibre4">["from</span> <span class="calibre4">Europe",</span> <span class="calibre4">"from</span> <span class="calibre4">US",</span> <span class="calibre4">"from</span> <span class="calibre4">Asia"]</span></code> ，  <code class="docutils"><span class="calibre4">["uses</span> <span class="calibre4">Firefox",</span> <span class="calibre4">"uses</span> <span class="calibre4">Chrome",</span> <span class="calibre4">"uses</span> <span class="calibre4">Safari",</span> <span class="calibre4">"uses</span> <span class="calibre4">Internet</span> <span class="calibre4">Explorer"]</span></code> 等分类的特征。这些特征能够被有效地编码成整数，比如 <code class="docutils"><span class="calibre4">["male",</span> <span class="calibre4">"from</span> <span class="calibre4">US",</span> <span class="calibre4">"uses</span> <span class="calibre4">Internet</span> <span class="calibre4">Explorer"]</span></code> 可以被表示为 <code class="docutils"><span class="calibre4">[0,</span> <span class="calibre4">1,</span> <span class="calibre4">3]</span></code> ， <code class="docutils"><span class="calibre4">["female",</span> <span class="calibre4">"from</span> <span class="calibre4">Asia",</span> <span class="calibre4">"uses</span> <span class="calibre4">Chrome"]</span></code> 表示为 <code class="docutils"><span class="calibre4">[1,</span> <span class="calibre4">2,</span> <span class="calibre4">1]</span></code> 。</p>
<p class="calibre10">这个的整数特征表示并不能在scikit-learn的估计器中直接使用，因为这样的连续输入，估计器会认为类别之间是有序的，但实际却是无序的。(例如：浏览器的类别数据则是任意排序的)</p>
<p class="calibre10">一种将分类特征转换为能够被scikit-learn中模型使用的编码是one-of-K或one-hot编码，在 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder"><code class="docutils"><span class="calibre4">OneHotEncoder</span></code></a> 中实现。这个类使用 <code class="docutils"><span class="calibre4">m</span></code> 个可能值转换为 <code class="docutils"><span class="calibre4">m</span></code> 值化特征，将分类特征的每个元素转化为一个值。</p>
<p class="calibre10">考虑如下例子:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">OneHotEncoder</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>  
<span class="calibre4">OneHotEncoder(categorical_features='all', dtype=&lt;... 'numpy.float64'&gt;,</span>
<span class="calibre4">       handle_unknown='error', n_values='auto', sparse=True)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]])</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])</span>
</pre>
</div>
</div>
<p class="calibre10">默认情况下，每个特征使用几维的数值由数据集自动推断。当然，你也可以通过使用参数``n_values``来精确指定。 在我们的例子数据集中，有两个可能得性别类别，三个洲，四个网络浏览器。接着，我们训练编码算法，并用来对一个样本数据进行转换。
在结果中，前两个数值是性别编码，紧接着的三个数值是洲编码，最后的四个数值是浏览器编码</p>
<p class="calibre10">注意，如果训练集中有丢失的分类特征值，必须显式地设置 <code class="docutils"><span class="calibre4">n_values</span></code> ，举个例子，</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">OneHotEncoder</span><span class="calibre4">(</span><span class="calibre4">n_values</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 注意到第二、三个特征是不全的</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># features</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]])</span>  
<span class="calibre4">OneHotEncoder(categorical_features='all', dtype=&lt;... 'numpy.float64'&gt;,</span>
<span class="calibre4">       handle_unknown='error', n_values=[2, 3, 4], sparse=True)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">enc</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">]])</span><span class="calibre4">.</span><span class="calibre4">toarray</span><span class="calibre4">()</span>
<span class="calibre4">array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])</span>
</pre>
</div>
</div>
<p class="calibre10">参见 <a class="calibre3 pcalibre" href="feature_extraction.html#dict-feature-extraction"><span class="calibre4">从字典类型加载特征</span></a> ，它对于分类特征代表一个dict，而不是整数。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-865">
<span id="calibre_link-866" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.6. 缺失值插补</h2>
<p class="calibre2">因为各种各样的原因，真实世界中的许多数据集都包含缺失数据，这类数据经常被编码成空格、NaNs，或者是其他的占位符。但是这样的数据集并不能scikit-learn学习算法兼容，因为大多的学习算法都默认假设数组中的元素都是数值，因而所有的元素都有自己的意义。
使用不完整的数据集的一个基本策略就是舍弃掉整行或整列包含缺失值的数据。但是这样就付出了舍弃可能有价值数据（即使是不完整的 ）的代价。 处理缺失数值的一个更好的策略就是从已有的数据推断出缺失的数值。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer" title="sklearn.preprocessing.Imputer"><code class="docutils"><span class="calibre4">Imputer</span></code></a> 类提供了估算缺失值的基本策略，使用缺失值所在的行/列中的平均值、中位数或者众数来填充。这个类也支持不同的缺失值编码。</p>
<p class="calibre10">以下代码段演示了如何使用包含缺失值的列(轴0)的平均值来替换编码为 <code class="docutils"><span class="calibre4">np.nan</span></code> 的缺失值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">Imputer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">imp</span> <span class="calibre4">=</span> <span class="calibre4">Imputer</span><span class="calibre4">(</span><span class="calibre4">missing_values</span><span class="calibre4">=</span><span class="calibre4">'NaN'</span><span class="calibre4">,</span> <span class="calibre4">strategy</span><span class="calibre4">=</span><span class="calibre4">'mean'</span><span class="calibre4">,</span> <span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">imp</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">nan</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">]])</span>
<span class="calibre4">Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">nan</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">6</span><span class="calibre4">,</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">nan</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">imp</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">))</span>                           
<span class="calibre4">[[ 4.          2.        ]</span>
<span class="calibre4"> [ 6.          3.666...]</span>
<span class="calibre4"> [ 7.          6.        ]]</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer" title="sklearn.preprocessing.Imputer"><code class="docutils"><span class="calibre4">Imputer</span></code></a> 类也支持稀疏矩阵:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">scipy.sparse</span> <span class="calibre4">as</span> <span class="calibre4">sp</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">sp</span><span class="calibre4">.</span><span class="calibre4">csc_matrix</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">imp</span> <span class="calibre4">=</span> <span class="calibre4">Imputer</span><span class="calibre4">(</span><span class="calibre4">missing_values</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">strategy</span><span class="calibre4">=</span><span class="calibre4">'mean'</span><span class="calibre4">,</span> <span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">imp</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">sp</span><span class="calibre4">.</span><span class="calibre4">csc_matrix</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">6</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">imp</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">))</span>                      
<span class="calibre4">[[ 4.          2.        ]</span>
<span class="calibre4"> [ 6.          3.666...]</span>
<span class="calibre4"> [ 7.          6.        ]]</span>
</pre>
</div>
</div>
<p class="calibre10">注意，缺失值被编码为0，因此隐式地存储在矩阵中。当缺失值比可观察到的值多的时候，这种格式是合适的。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer" title="sklearn.preprocessing.Imputer"><code class="docutils"><span class="calibre4">Imputer</span></code></a> 可以在 Pipeline 中用作构建支持插补的合成模型。参见 <a class="calibre3 pcalibre" href="../auto_examples/plot_missing_values.html#sphx-glr-auto-examples-plot-missing-values-py"><span class="calibre4">Imputing missing values before building an estimator</span></a> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-867">
<span id="calibre_link-868" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.7. 生成多项式特征</h2>
<p class="calibre2">在机器学习中，通过增加一些输入数据的非线性特征来增加模型的复杂度通常是有效的。一个简单通用的办法是使用多项式特征，这可以获得特征的更高维度和互相间关系的项。这在 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="docutils"><span class="calibre4">PolynomialFeatures</span></code></a> 中实现:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">PolynomialFeatures</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">6</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span>                                                 
<span class="calibre4">array([[0, 1],</span>
<span class="calibre4">       [2, 3],</span>
<span class="calibre4">       [4, 5]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span> <span class="calibre4">=</span> <span class="calibre4">PolynomialFeatures</span><span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>                             
<span class="calibre4">array([[  1.,   0.,   1.,   0.,   0.,   1.],</span>
<span class="calibre4">       [  1.,   2.,   3.,   4.,   6.,   9.],</span>
<span class="calibre4">       [  1.,   4.,   5.,  16.,  20.,  25.]])</span>
</pre>
</div>
</div>
<p class="calibre10">X 的特征已经从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000578.jpg" alt="(X_1, X_2)" /> 转换为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000102.jpg" alt="(1, X_1, X_2, X_1^2, X_1X_2, X_2^2)" /> 。</p>
<p class="calibre10">在一些情况下，只需要特征间的交互项，这可以通过设置 <code class="docutils"><span class="calibre4">interaction_only=True</span></code> 来得到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">arange</span><span class="calibre4">(</span><span class="calibre4">9</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span>                                                 
<span class="calibre4">array([[0, 1, 2],</span>
<span class="calibre4">       [3, 4, 5],</span>
<span class="calibre4">       [6, 7, 8]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span> <span class="calibre4">=</span> <span class="calibre4">PolynomialFeatures</span><span class="calibre4">(</span><span class="calibre4">degree</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">interaction_only</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">poly</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>                             
<span class="calibre4">array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],</span>
<span class="calibre4">       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],</span>
<span class="calibre4">       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])</span>
</pre>
</div>
</div>
<p class="calibre10">X的特征已经从 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000288.jpg" alt="(X_1, X_2, X_3)" /> 转换为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000000.jpg" alt="(1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)" /> 。</p>
<p class="calibre10">注意，当使用多项的 <a href="#calibre_link-83" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-869">:ref:`svm_kernels`时 ，多项式特征被隐式地使用在 `核函数(kernel methods) &lt;https://en.wikipedia.org/wiki/Kernel_method&gt;`_</span></a> 中(比如， <a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">sklearn.svm.SVC</span></code></a> ， <a class="calibre3 pcalibre" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="docutils"><span class="calibre4">sklearn.decomposition.KernelPCA</span></code></a> )。</p>
<p class="calibre10">创建并使用多项式特征的岭回归实例请见 <a class="calibre3 pcalibre" href="../auto_examples/linear_model/plot_polynomial_interpolation.html#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py"><span class="calibre4">Polynomial interpolation</span></a> 。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-870">
<span id="calibre_link-871" class="calibre4"></span><h2 class="sigil_not_in_toc">4.3.8. 自定义转换器</h2>
<p class="calibre2">在机器学习中，想要将一个已有的 Python 函数转化为一个转换器来协助数据清理或处理。可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer"><code class="docutils"><span class="calibre4">FunctionTransformer</span></code></a> 从任意函数中实现一个转换器。例如，在一个管道中构建一个实现日志转换的转化器，这样做:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">FunctionTransformer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">FunctionTransformer</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">log1p</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[ 0.        ,  0.69314718],</span>
<span class="calibre4">       [ 1.09861229,  1.38629436]])</span>
</pre>
</div>
</div>
<p class="calibre10">使用一个 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer" title="sklearn.preprocessing.FunctionTransformer"><code class="docutils"><span class="calibre4">FunctionTransformer</span></code></a> 类来做定制化特征选择的例子，请见 <a class="calibre3 pcalibre" href="../auto_examples/preprocessing/plot_function_transformer.html#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py"><span class="calibre4">Using FunctionTransformer to select columns</span></a> 。</p>
</div>
</div>


<div class="calibre" id="calibre_link-64">
<span id="calibre_link-872" class="calibre4"></span><h1 class="calibre5">4.4. 无监督降维</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@十四号</a><br class="calibre9" />
    </div>
<p class="calibre10">如果你的特征数量很多, 在监督步骤之前, 可以通过无监督的步骤来减少特征.
很多的 <a class="calibre3 pcalibre" href="../unsupervised_learning.html#unsupervised-learning"><span class="calibre4">无监督学习</span></a> 方法实现了一个名为 <code class="docutils"><span class="calibre4">transform</span></code> 的方法, 它可以用来降低维度.
下面我们将讨论大量使用这种模式的两个具体示例.</p>
<div class="toctree-wrapper" id="calibre_link-873">
<h2 class="sigil_not_in_toc">4.4.1. PCA: 主成份分析</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="docutils"><span class="calibre4">decomposition.PCA</span></code></a> 寻找能够捕捉原始特征的差异的特征的组合.
请参阅 <a class="calibre3 pcalibre" href="decomposition.html#decompositions"><span class="calibre4">分解成分中的信号（矩阵分解问题）</span></a>.</p>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">示例</strong></p>
<ul class="calibre6">
<li class="toctree-l"><table class="first4" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">ref:</th>
<td class="label1">‘sphx_glr_auto_examples_applications_plot_face_recognition.py’</td>
</tr>
</tbody>
</table>
</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-874">
<h2 class="sigil_not_in_toc">4.4.2. 随机投影</h2>
<p class="calibre2">模块: <code class="docutils"><span class="calibre4">random_projection</span></code> 提供了几种用于通过随机投影减少数据的工具.
请参阅文档的相关部分: <a class="calibre3 pcalibre" href="random_projection.html#random-projection"><span class="calibre4">随机投影</span></a>.</p>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">示例</strong></p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py"><span class="calibre4">The Johnson-Lindenstrauss bound for embedding with random projections</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-875">
<h2 class="sigil_not_in_toc">4.4.3. 特征聚集</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="docutils"><span class="calibre4">cluster.FeatureAgglomeration</span></code></a> 应用
<a class="calibre3 pcalibre" href="clustering.html#hierarchical-clustering"><span class="calibre4">层次聚类</span></a> 将行为类似的特征分组在一起.</p>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">示例</strong></p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="calibre4">Feature agglomeration vs. univariate selection</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py"><span class="calibre4">Feature agglomeration</span></a></li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">特征缩放</strong></p>
<p class="calibre10">请注意，如果功能具有明显不同的缩放或统计属性，则 <a class="calibre3 pcalibre" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="docutils"><span class="calibre4">cluster.FeatureAgglomeration</span></code></a>
可能无法捕获相关特征之间的关系.使用一个  <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">preprocessing.StandardScaler</span></code></a> 可以在这些
设置中使用.</p>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-85">
<span id="calibre_link-876" class="calibre4"></span><h1 class="calibre5">4.5. 随机投影</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/FontTian" class="calibre3 pcalibre">@FontTian</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />              
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Sehriff</a><br class="calibre9" />
    </div>
<p class="calibre10"><a class="calibre3 pcalibre" href="classes.html#module-sklearn.random_projection" title="sklearn.random_projection"><code class="docutils"><span class="calibre4">sklearn.random_projection</span></code></a> 模块实现了一个简单且高效率的计算方式来减少数据维度，通过牺牲一定的精度（作为附加变量）来加速处理时间及更小的模型尺寸。
这个模型实现了两类无结构化的随机矩阵:
<a class="calibre3 pcalibre" href="#calibre_link-86"><span class="calibre4">Gaussian random matrix</span></a> 和
<a class="calibre3 pcalibre" href="#calibre_link-87"><span class="calibre4">sparse random matrix</span></a>.</p>
<p class="calibre10">随机投影矩阵的维度和分布是受控制的，所以可以保存任意两个数据集的距离。因此随机投影适用于基于距离的方法。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Sanjoy Dasgupta. 2000.
<a class="calibre3 pcalibre" href="http://cseweb.ucsd.edu/~dasgupta/papers/randomf.pdf">Experiments with random projection.</a>
In Proceedings of the Sixteenth conference on Uncertainty in artificial
intelligence (UAI‘00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.</li>
<li class="toctree-l">Ella Bingham and Heikki Mannila. 2001.
<a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.5135&amp;rep=rep1&amp;type=pdf">Random projection in dimensionality reduction: applications to image and text data.</a>
In Proceedings of the seventh ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA,
245-250.</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-877">
<span id="calibre_link-878" class="calibre4"></span><h2 class="sigil_not_in_toc">4.5.1. Johnson-Lindenstrauss 辅助定理</h2>
<p class="calibre2">支撑随机投影效率的主要理论成果是`Johnson-Lindenstrauss lemma (quoting Wikipedia)
&lt;<a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma</a>&gt;`_:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">在数学中，johnson - lindenstrauss 引理是一种将高维的点从高维到低维欧几里得空间的低失真嵌入的方案。
引理阐释了高维空间下的一小部分的点集可以内嵌到非常低维的空间，这种方式下点之间的距离几乎全部被保留。
内嵌所用到的映射至少符合 Lipschitz 条件,甚至可以被当做正交投影。</div>
</blockquote>
<p class="calibre10">有了样本数量，
<a class="calibre3 pcalibre" href="generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim" title="sklearn.random_projection.johnson_lindenstrauss_min_dim"><code class="docutils"><span class="calibre4">sklearn.random_projection.johnson_lindenstrauss_min_dim</span></code></a> 会保守估计随机子空间的最小大小来保证随机投影导致的变形在一定范围内：</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.random_projection</span> <span class="calibre4">import</span> <span class="calibre4">johnson_lindenstrauss_min_dim</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">johnson_lindenstrauss_min_dim</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">1e6</span><span class="calibre4">,</span> <span class="calibre4">eps</span><span class="calibre4">=</span><span class="calibre4">0.5</span><span class="calibre4">)</span>
<span class="calibre4">663</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">johnson_lindenstrauss_min_dim</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">1e6</span><span class="calibre4">,</span> <span class="calibre4">eps</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">,</span> <span class="calibre4">0.01</span><span class="calibre4">])</span>
<span class="calibre4">array([    663,   11841, 1112658])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">johnson_lindenstrauss_min_dim</span><span class="calibre4">(</span><span class="calibre4">n_samples</span><span class="calibre4">=</span><span class="calibre4">[</span><span class="calibre4">1e4</span><span class="calibre4">,</span> <span class="calibre4">1e5</span><span class="calibre4">,</span> <span class="calibre4">1e6</span><span class="calibre4">],</span> <span class="calibre4">eps</span><span class="calibre4">=</span><span class="calibre4">0.1</span><span class="calibre4">)</span>
<span class="calibre4">array([ 7894,  9868, 11841])`</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_johnson_lindenstrauss_bound.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_johnson_lindenstrauss_bound_0011.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000027.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<a class="calibre3 pcalibre" href="../auto_examples/plot_johnson_lindenstrauss_bound.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_johnson_lindenstrauss_bound_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000280.jpg" class="calibre27" /></a>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l">查看 <a class="calibre3 pcalibre" href="../auto_examples/plot_johnson_lindenstrauss_bound.html#sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py"><span class="calibre4">The Johnson-Lindenstrauss bound for embedding with random projections</span></a>
里面有Johnson-Lindenstrauss引理的理论说明和使用稀疏随机矩阵的经验验证。</li>
</ul>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Sanjoy Dasgupta and Anupam Gupta, 1999.
<a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.39.3334&amp;rep=rep1&amp;type=pdf">An elementary proof of the Johnson-Lindenstrauss Lemma.</a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-86">
<span id="calibre_link-879" class="calibre4"></span><h2 class="sigil_not_in_toc">4.5.2. 高斯随机投影</h2>
<p class="calibre2">The <a class="calibre3 pcalibre" href="generated/sklearn.random_projection.GaussianRandomProjection.html#sklearn.random_projection.GaussianRandomProjection" title="sklearn.random_projection.GaussianRandomProjection"><code class="docutils"><span class="calibre4">sklearn.random_projection.GaussianRandomProjection</span></code></a> 通过将原始输入空间投影到随机生成的矩阵（该矩阵的组件由以下分布中抽取）
:math:<a href="#calibre_link-88" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-880">`</span></a>N(0, frac{1}{n_{components}})`降低维度。</p>
<p class="calibre10">以下小片段演示了任何使用高斯随机投影转换器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">random_projection</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">rand</span><span class="calibre4">(</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">10000</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">random_projection</span><span class="calibre4">.</span><span class="calibre4">GaussianRandomProjection</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(100, 3947)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-87">
<span id="calibre_link-881" class="calibre4"></span><h2 class="sigil_not_in_toc">4.5.3. 稀疏随机矩阵</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection" title="sklearn.random_projection.SparseRandomProjection"><code class="docutils"><span class="calibre4">sklearn.random_projection.SparseRandomProjection</span></code></a>  使用稀疏随机矩阵，通过投影原始输入空间来降低维度。</div>
</blockquote>
<p class="calibre10">稀疏矩阵可以替换高斯随机投影矩阵来保证相似的嵌入质量，且内存利用率更高、投影数据的计算更快。</p>
<p class="calibre10">如果我们定义 <code class="docutils"><span class="calibre4">s</span> <span class="calibre4">=</span> <span class="calibre4">1</span> <span class="calibre4">/</span> <span class="calibre4">density</span></code>,  随机矩阵的元素由</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000469.jpg" alt="\left\{ \begin{array}{c c l} -\sqrt{\frac{s}{n_{\text{components}}}} &amp; &amp; 1 / 2s\\ 0 &amp;\text{with probability}  &amp; 1 - 1 / s \\ +\sqrt{\frac{s}{n_{\text{components}}}} &amp; &amp; 1 / 2s\\ \end{array} \right." class="math" /></p>
</div>
<p class="calibre10">抽取。</p>
<p class="calibre10">其中 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000374.jpg" alt="n_{\text{components}}" /> 是投影后的子空间大小。
默认非零元素的浓密度设置为最小浓密度，该值由Ping Li et al.:推荐，根据公式:math:<a href="#calibre_link-89" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-882">`</span></a>1 / sqrt{n_{text{features}}}`计算。</p>
<p class="calibre10">以下小片段演示了如何使用稀疏随机投影转换器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">random_projection</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">rand</span><span class="calibre4">(</span><span class="calibre4">100</span><span class="calibre4">,</span><span class="calibre4">10000</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">random_projection</span><span class="calibre4">.</span><span class="calibre4">SparseRandomProjection</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(100, 3947)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">D. Achlioptas. 2003.
<a class="calibre3 pcalibre" href="www.cs.ucsc.edu/~optas/papers/jl.pdf">Database-friendly random projections: Johnson-Lindenstrauss  with binary
coins</a>.
Journal of Computer and System Sciences 66 (2003) 671&ndash;687</li>
<li class="toctree-l">Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
<a class="calibre3 pcalibre" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.585&amp;rep=rep1&amp;type=pdf">Very sparse random projections.</a>
In Proceedings of the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA,
287-296.</li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-67">
<span id="calibre_link-883" class="calibre4"></span><h1 class="calibre5">4.6. 内核近似</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/FontTian" class="calibre3 pcalibre">@FontTian</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@numpy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
    </div>
<p class="calibre10">这个子模块包含与某些 kernel 对应的特征映射的函数，这个会用于例如支持向量机的算法当中(see <a class="calibre3 pcalibre" href="svm.html#svm"><span class="calibre4">支持向量机</span></a>)。
下面这些特征函数对输入执行非线性转换，可以用于线性分类或者其他算法。</p>
<p class="calibre10">与 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Kernel_trick">kernel trick</a> 相比，近似的进行特征映射更适合在线学习，并能够有效
减少学习大量数据的内存开销。使用标准核技巧的 svm 不能有效的适用到海量数据，但是使用近似内核映射的方法，对于线性 SVM 来说效果可能更好。
而且，使用 <a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">SGDClassifier</span></code></a> 进行近似的内核映射，使得对海量数据进行非线性学习也成为了可能。</p>
<p class="calibre10">由于近似嵌入的方法没有太多经验性的验证，所以建议将结果和使用精确的内核方法的结果进行比较。</p>
<div class="toctree-wrapper">
<p class="calibre10">See also</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="linear_model.html#polynomial-regression"><span class="calibre4">多项式回归：用基函数展开线性模型</span></a> 用于精确的多项式变换。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-884">
<span id="calibre_link-885" class="calibre4"></span><h2 class="sigil_not_in_toc">4.6.1. 内核近似的 Nystroem 方法</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code class="docutils"><span class="calibre4">Nystroem</span></code></a> 中实现了 Nystroem 方法用于低等级的近似核。它是通过采样 kernel 已经评估好的数据。默认情况下，
<a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code class="docutils"><span class="calibre4">Nystroem</span></code></a> 使用 <code class="docutils"><span class="calibre4">rbf</span></code> kernel，但它可以使用任何内核函数和预计算内核矩阵.
使用的样本数量 - 计算的特征维数 - 由参数 <code class="docutils"><span class="calibre4">n_components</span></code> 给出.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-886">
<span id="calibre_link-887" class="calibre4"></span><h2 class="sigil_not_in_toc">4.6.2. 径向基函数内核</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 为径向基函数核构造一个近似映射，又称为 Random Kitchen Sinks [RR2007].
在应用线性算法（例如线性 SVM ）之前，可以使用此转换来明确建模内核映射:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.kernel_approximation</span> <span class="calibre4">import</span> <span class="calibre4">RBFSampler</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">SGDClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rbf_feature</span> <span class="calibre4">=</span> <span class="calibre4">RBFSampler</span><span class="calibre4">(</span><span class="calibre4">gamma</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_features</span> <span class="calibre4">=</span> <span class="calibre4">rbf_feature</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SGDClassifier</span><span class="calibre4">()</span>   
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_features</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span>
<span class="calibre4">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span>
<span class="calibre4">       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,</span>
<span class="calibre4">       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,</span>
<span class="calibre4">       shuffle=True, tol=None, verbose=0, warm_start=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_features</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">1.0</span>
</pre>
</div>
</div>
<p class="calibre10">这个映射依赖于内核值的 Monte Carlo 近似.  <code class="docutils"><span class="calibre4">fit</span></code> 方法执行 Monte Carlo 采样，而该 <code class="docutils"><span class="calibre4">transform</span></code> 方法执行
数据的映射.由于过程的固有随机性，结果可能会在不同的 <code class="docutils"><span class="calibre4">fit</span></code> 函数调用之间变化。</p>
<p class="calibre10">该 <code class="docutils"><span class="calibre4">fit</span></code> 函数有两个参数:
<code class="docutils"><span class="calibre4">n_components</span></code> 是特征变换的目标维数. <code class="docutils"><span class="calibre4">gamma</span></code> 是 RBF-kernel 的参数. <code class="docutils"><span class="calibre4">n_components</span></code> 越高，会导致更好的内核近似，
并且将产生与内核 SVM 产生的结果更相似的结果。请注意，”拟合” 特征函数实际上不取决于 <code class="docutils"><span class="calibre4">fit</span></code> 函数传递的数据。只有数据的维数被使用。
详情可以参考 <a class="calibre3 pcalibre" href="#calibre_link-68" id="calibre_link-72">[RR2007]</a>.</p>
<p class="calibre10">对于给定的值 <code class="docutils"><span class="calibre4">n_components</span></code> <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 在 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code class="docutils"><span class="calibre4">Nystroem</span></code></a> 中使用通常不太准确，
但是 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 使用更大的特征空间，更容易计算。</p>
<div class="toctree-wrapper" id="calibre_link-888">
<a class="calibre3 pcalibre" href="../auto_examples/plot_kernel_approximation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_kernel_approximation_0021.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000574.jpg" class="calibre59" /></a>
<p class="calibre10"><span class="calibre4">将精确的 RBF kernel (左) 与 approximation (右) 进行比较。</span></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/plot_kernel_approximation.html#sphx-glr-auto-examples-plot-kernel-approximation-py"><span class="calibre4">Explicit feature map approximation for RBF kernels</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-889">
<span id="calibre_link-890" class="calibre4"></span><h2 class="sigil_not_in_toc">4.6.3. 加性卡方核</h2>
<p class="calibre2">Additive Chi Squared Kernel (加性卡方核)是直方图的核心，通常用于计算机视觉。</p>
<p class="calibre10">这里使用的 Additive Chi Squared Kernel 给出</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000392.jpg" alt="k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}" class="math" /></p>
</div>
<p class="calibre10">这个和 <code class="docutils"><span class="calibre4">sklearn.metrics.additive_chi2_kernel</span></code> 不完全一样.[VZ2010]_ 的作者喜欢上面的版本，因为它总是积极的。
由于这个 kernel 是可添加的，因此可以分别处理嵌入的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" />. 这使得在规则的间隔类对傅里叶变换进行性才赢，代替近似的 Monte Carlo 采样。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html#sklearn.kernel_approximation.AdditiveChi2Sampler" title="sklearn.kernel_approximation.AdditiveChi2Sampler"><code class="docutils"><span class="calibre4">AdditiveChi2Sampler</span></code></a> 类实现了这个组件采样方法. 每个组件都被采样 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 次，每一个输入维数都会产生 <cite class="calibre13">2n+1</cite> 维（来自傅立叶变换的实部和复数部分的两个数据段的倍数）.
在文献中，<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 经常取为 1 或者 2，将数据集转换为 <code class="docutils"><span class="calibre4">n_samples</span> <span class="calibre4">*</span> <span class="calibre4">5</span> <span class="calibre4">*</span> <span class="calibre4">n_features</span></code> 大小（在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000399.jpg" alt="n=2" /> 的情况下 ）.</div>
</blockquote>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html#sklearn.kernel_approximation.AdditiveChi2Sampler" title="sklearn.kernel_approximation.AdditiveChi2Sampler"><code class="docutils"><span class="calibre4">AdditiveChi2Sampler</span></code></a> 提供的近似特征映射可以和 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 提供的近似特征映射合并，得到一个取幂的 chi squared kerne。可以查看 <a class="calibre3 pcalibre" href="#calibre_link-69" id="calibre_link-74">[VZ2010]</a> 和 <a class="calibre3 pcalibre" href="#calibre_link-70" id="calibre_link-75">[VVZ2010]</a> <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 的合并.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-891">
<span id="calibre_link-892" class="calibre4"></span><h2 class="sigil_not_in_toc">4.6.4. Skewed Chi Squared Kernel (偏斜卡方核?暂译)</h2>
<p class="calibre2">skewed chi squared kernel 给出下面公式</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000703.jpg" alt="k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}" class="math" /></p>
</div>
<p class="calibre10">它有和 指数卡方核 相似的属性，用于计算机视觉.但是允许进行简单的 蒙特卡洛 近似 的特征映射。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.SkewedChi2Sampler.html#sklearn.kernel_approximation.SkewedChi2Sampler" title="sklearn.kernel_approximation.SkewedChi2Sampler"><code class="docutils"><span class="calibre4">SkewedChi2Sampler</span></code></a> 的使用和之前描述的 <a class="calibre3 pcalibre" href="generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code class="docutils"><span class="calibre4">RBFSampler</span></code></a> 一样.唯一的区别是自由参数，称之为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000577.jpg" alt="c" />.
这种映射和数学细节可以参考 <a class="calibre3 pcalibre" href="#calibre_link-71" id="calibre_link-73">[LS2010]</a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-893">
<h2 class="sigil_not_in_toc">4.6.5. 数学方面的细节</h2>
<p class="calibre2">核技巧 像支持向量机，或者 核化 PCA 依赖于 再生核希尔伯特空间（RKHS）
对于任何 核函数 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> （叫做 Mercer kernel），保证了 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" /> 进入 希尔伯特空间 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000489.jpg" alt="\mathcal{H}" /> 的映射，例如：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000232.jpg" alt="k(x,y) = \langle \phi(x), \phi(y) \rangle" class="math" /></p>
</div>
<p class="calibre10"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000261.jpg" alt="\langle \cdot, \cdot \rangle" />  是在 Hilbert space 中做内积.</p>
<p class="calibre10">如果一个算法，例如线性支持向量机或者 PCA，依赖于数据集的数量级 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> ，可能会使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000456.jpg" alt="k(x_i, x_j)" /> ，
符合孙发的映射 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000814.jpg" alt="\phi(x_i)" /> . 使用 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 的优点在于 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" /> 永远不会直接计算，允许大量的特征计算（甚至是无限的）.</p>
<p class="calibre10">kernel 方法的一个缺点是，在优化过程中有可能存储大量的 kernel 值 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000456.jpg" alt="k(x_i, x_j)" />.
如果使用核函数的分类器应用于新的数据 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000432.jpg" alt="y_j" /> ， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000245.jpg" alt="k(x_i, y_j)" /> 需要计算用来做预测，训练集中的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000880.jpg" alt="x_i" /> 有可能有很多不同的。</p>
<p class="calibre10">这个子模块的这些类中允许嵌入 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000779.jpg" alt="\phi" />，从而明确的与 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000814.jpg" alt="\phi(x_i)" /> 一起工作，
这消除了使用 kernel 的需要和存储训练样本.</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<table class="docutils1" frame="void" id="calibre_link-68" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-72">[RR2007]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf">“Random features for large-scale kernel machines”</a>
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-71" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-73">[LS2010]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://www.maths.lth.se/matematiklth/personal/sminchis/papers/lis_dagm10.pdf">“Random Fourier approximations for skewed multiplicative histogram kernels”</a>
Random Fourier approximations for skewed multiplicative histogram kernels
- Lecture Notes for Computer Sciencd (DAGM)</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-69" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-74">[VZ2010]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf">“Efficient additive kernels via explicit feature maps”</a>
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" id="calibre_link-70" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-75">[VVZ2010]</a></td>
<td class="label1"><a class="calibre3 pcalibre" href="https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf">“Generalized RBF feature maps for Efficient Detection”</a>
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-231">
<span id="calibre_link-894" class="calibre4"></span><h1 class="calibre5">4.7. 成对的矩阵, 类别和核函数</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/FontTian" class="calibre3 pcalibre">@FontTian</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@numpy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
    </div>
<p class="calibre10">The <a class="calibre3 pcalibre" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="docutils"><span class="calibre4">sklearn.metrics.pairwise</span></code></a> 子模块实现了用于评估成对距离或样本集合之间的联系的实用程序。</p>
<p class="calibre10">本模块同时包含距离度量和核函数，对于这两者这里提供一个简短的总结。</p>
<p class="calibre10">距离度量是形如 <code class="docutils"><span class="calibre4">d(a,</span> <span class="calibre4">b)</span></code> 例如 <code class="docutils"><span class="calibre4">d(a,</span> <span class="calibre4">b)</span> <span class="calibre4">&lt;</span> <span class="calibre4">d(a,</span> <span class="calibre4">c)</span></code>
如果对象 <code class="docutils"><span class="calibre4">a</span></code> 和 <code class="docutils"><span class="calibre4">b</span></code> 被认为 “更加相似” 相比于 <code class="docutils"><span class="calibre4">a</span></code>
和 <code class="docutils"><span class="calibre4">c</span></code>. 两个完全相同的目标的距离是零。最广泛使用的例子就是欧几里得距离。
为了保证是 ‘真实的’ 度量, 其必须满足以下条件:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">对于所有的 a 和 b，d(a, b) &gt;= 0</li>
<li class="toctree-l">正定性：当且仅当 a = b时，d(a, b) == 0</li>
<li class="toctree-l">对称性：d(a, b) == d(b, a)</li>
<li class="toctree-l">三角不等式：d(a, c) &lt;= d(a, b) + d(b, c)</li>
</ol>
</div>
</blockquote>
<p class="calibre10">核函数是相似度的标准.
如果对象 <code class="docutils"><span class="calibre4">a</span></code> 和 <code class="docutils"><span class="calibre4">b</span></code> 被认为 “更加相似” 相比对象
<code class="docutils"><span class="calibre4">a</span></code> 和 <code class="docutils"><span class="calibre4">c</span></code>，那么 <code class="docutils"><span class="calibre4">s(a,</span> <span class="calibre4">b)</span> <span class="calibre4">&gt;</span> <span class="calibre4">s(a,</span> <span class="calibre4">c)</span></code>. 核函数必须是半正定性的.</p>
<p class="calibre10">存在许多种方法将距离度量转换为相似度标准，例如核函数。 假定 <code class="docutils"><span class="calibre4">D</span></code> 是距离, and <code class="docutils"><span class="calibre4">S</span></code> 是核函数:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l"><code class="docutils"><span class="calibre4">S</span> <span class="calibre4">=</span> <span class="calibre4">np.exp(-D</span> <span class="calibre4">*</span> <span class="calibre4">gamma)</span></code>, 其中
<code class="docutils"><span class="calibre4">gamma</span></code> 的一种选择是  <code class="docutils"><span class="calibre4">1</span> <span class="calibre4">/</span> <span class="calibre4">num_features</span></code></li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">S</span> <span class="calibre4">=</span> <span class="calibre4">1.</span> <span class="calibre4">/</span> <span class="calibre4">(D</span> <span class="calibre4">/</span> <span class="calibre4">np.max(D))</span></code></li>
</ol>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-895">
<span id="calibre_link-896" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.1. 余弦相似度</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="docutils"><span class="calibre4">cosine_similarity</span></code></a>  计算L2正则化的向量的点积.
也就是说, if <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000201.jpg" alt="x" /> 和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" /> 都是行向量,,
它们的余弦相似度 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" /> 定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000310.jpg" alt="k(x, y) = \frac{x y^\top}{\|x\| \|y\|}" class="math" /></p>
</div>
<p class="calibre10">这被称为余弦相似度, 因为欧几里得(L2) 正则化将向量投影到单元球面内，那么它们的点积就是被向量表示的点之间的角度。</p>
<p class="calibre10">这种核函数对于计算以tf-idf向量表示的文档之间的相似度是一个通常的选择.
<a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="docutils"><span class="calibre4">cosine_similarity</span></code></a> 接受 <code class="docutils"><span class="calibre4">scipy.sparse</span></code> 矩阵.
(注意到 <code class="docutils"><span class="calibre4">sklearn.feature_extraction.text</span></code>
中的tf-idf函数能计算归一化的向量，在这种情况下 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.cosine_similarity.html#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="docutils"><span class="calibre4">cosine_similarity</span></code></a>
等同于 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code class="docutils"><span class="calibre4">linear_kernel</span></code></a>, 只是慢一点而已.)</p>
<div class="toctree-wrapper">
<p class="calibre10">References:</p>
<ul class="calibre6">
<li class="toctree-l">C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press.
<a class="calibre3 pcalibre" href="http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html">http://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html</a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-897">
<span id="calibre_link-898" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.2. 线性核函数</h2>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel" title="sklearn.metrics.pairwise.linear_kernel"><code class="docutils"><span class="calibre4">linear_kernel</span></code></a> 是计算线性核函数, 也就是一种在 <code class="docutils"><span class="calibre4">degree=1</span></code> 和 <code class="docutils"><span class="calibre4">coef0=0</span></code> (同质化) 情况下的 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code class="docutils"><span class="calibre4">polynomial_kernel</span></code></a> 的特殊形式.
如果 <code class="docutils"><span class="calibre4">x</span></code> 和 <code class="docutils"><span class="calibre4">y</span></code> 是列向量, 它们的线性核函数是:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000720.jpg" alt="k(x, y) = x^\top y" class="math" /></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-899">
<span id="calibre_link-900" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.3. 多项式核函数</h2>
<dl class="calibre10">
<dt class="calibre18">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel" title="sklearn.metrics.pairwise.polynomial_kernel"><code class="docutils"><span class="calibre4">polynomial_kernel</span></code></a> 计算两个向量的d次方的多项式核函数. 多项式核函数代表着两个向量之间的相似度.</dt>
<dd class="calibre19">概念上来说，多项式核函数不仅考虑相同维度还考虑跨维度的向量的相似度。当被用在机器学习中的时候，这可以原来代表着特征之间的 相互作用。</dd>
</dl>
<p class="calibre10">多项式函数定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000186.jpg" alt="k(x, y) = (\gamma x^\top y +c_0)^d" class="math" /></p>
</div>
<p class="calibre10">其中:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">x</span></code>, <code class="docutils"><span class="calibre4">y</span></code> 是输入向量</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">d</span></code>  核函数维度</li>
</ul>
</div>
</blockquote>
<p class="calibre10">如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000805.jpg" alt="c_0 = 0" /> 那么核函数就被定义为同质化的.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-901">
<span id="calibre_link-902" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.4. Sigmoid 核函数</h2>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.sigmoid_kernel.html#sklearn.metrics.pairwise.sigmoid_kernel" title="sklearn.metrics.pairwise.sigmoid_kernel"><code class="docutils"><span class="calibre4">sigmoid_kernel</span></code></a> 计算两个向量之间的S型核函数.
S型核函数也被称为双曲切线或者 多层感知机(因为在神经网络领域，它经常被当做激活函数). S型核函数定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000220.jpg" alt="k(x, y) = \tanh( \gamma x^\top y + c_0)" class="math" /></p>
</div>
<p class="calibre10">where:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">x</span></code>, <code class="docutils"><span class="calibre4">y</span></code> 是输入向量</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000566.jpg" alt="\gamma" /> 是斜度</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000056.jpg" alt="c_0" /> 是截距</li>
</ul>
</div>
</blockquote>
</div>
<div class="toctree-wrapper" id="calibre_link-903">
<span id="calibre_link-904" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.5. RBF 核函数</h2>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.rbf_kernel.html#sklearn.metrics.pairwise.rbf_kernel" title="sklearn.metrics.pairwise.rbf_kernel"><code class="docutils"><span class="calibre4">rbf_kernel</span></code></a> 计算计算两个向量之间的径向基函数核 (RBF) 。 其定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000269.jpg" alt="k(x, y) = \exp( -\gamma \| x-y \|^2)" class="math" /></p>
</div>
<p class="calibre10">其中 <code class="docutils"><span class="calibre4">x</span></code> 和 <code class="docutils"><span class="calibre4">y</span></code> 是输入向量. 如果 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000332.jpg" alt="\gamma = \sigma^{-2}" />
核函数就变成方差为 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000477.jpg" alt="\sigma^2" /> 的高斯核函数.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-905">
<span id="calibre_link-906" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.6. 拉普拉斯核函数</h2>
<p class="calibre2">函数 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.laplacian_kernel.html#sklearn.metrics.pairwise.laplacian_kernel" title="sklearn.metrics.pairwise.laplacian_kernel"><code class="docutils"><span class="calibre4">laplacian_kernel</span></code></a> 是一种径向基函数核的变体，定义为:</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000499.jpg" alt="k(x, y) = \exp( -\gamma \| x-y \|_1)" class="math" /></p>
</div>
<p class="calibre10">其中 <code class="docutils"><span class="calibre4">x</span></code> 和 <code class="docutils"><span class="calibre4">y</span></code> 是输入向量 并且 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000140.jpg" alt="\|x-y\|_1" /> 是输入向量之间的曼哈顿距离.</p>
<p class="calibre10">已被证明在机器学习中运用到无噪声数据中是有用的.
可见例如 <a class="calibre3 pcalibre" href="http://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/">Machine learning for quantum mechanics in a nutshell</a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-907">
<span id="calibre_link-908" class="calibre4"></span><h2 class="sigil_not_in_toc">4.7.7. 卡方核函数</h2>
<dl class="calibre10">
<dt class="calibre18">在计算机视觉应用中训练非线性支持向量机时，卡方核函数是一种非常流行的选择.</dt>
<dd class="calibre19">它能以 <a class="calibre3 pcalibre" href="generated/sklearn.metrics.pairwise.chi2_kernel.html#sklearn.metrics.pairwise.chi2_kernel" title="sklearn.metrics.pairwise.chi2_kernel"><code class="docutils"><span class="calibre4">chi2_kernel</span></code></a> 计算然后将参数 <a href="#calibre_link-232" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-909">``</span></a>kernel=”precomputed”<a href="#calibre_link-233" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-910">``</span></a>传递到</dd>
</dl>
<p class="calibre10"><a class="calibre3 pcalibre" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">sklearn.svm.SVC</span></code></a> :</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.metrics.pairwise</span> <span class="calibre4">import</span> <span class="calibre4">chi2_kernel</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">8</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">3</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">K</span> <span class="calibre4">=</span> <span class="calibre4">chi2_kernel</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">gamma</span><span class="calibre4">=.</span><span class="calibre4">5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">K</span>                        
<span class="calibre4">array([[ 1.        ,  0.36...,  0.89...,  0.58...],</span>
<span class="calibre4">       [ 0.36...,  1.        ,  0.51...,  0.83...],</span>
<span class="calibre4">       [ 0.89...,  0.51...,  1.        ,  0.77... ],</span>
<span class="calibre4">       [ 0.58...,  0.83...,  0.77... ,  1.        ]])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'precomputed'</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">K</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">K</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 1, 0, 1])</span>
</pre>
</div>
</div>
<p class="calibre10">也可以直接使用 <code class="docutils"><span class="calibre4">kernel</span></code> 变量:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">chi2_kernel</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 1, 0, 1])</span>
</pre>
</div>
</div>
<p class="calibre10">卡方核函数定义为</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000837.jpg" alt="k(x, y) = \exp \left (-\gamma \sum_i \frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \right )" class="math" /></p>
</div>
<p class="calibre10">数据假定为非负的，并且已经以L1正则化。 归一化随着与卡方平方距离的连接而被合理化，其是离散概率分布之间的距离。</p>
<p class="calibre10">卡方核函数最常用于可视化词汇的矩形图。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考:</p>
<ul class="calibre6">
<li class="toctree-l">Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.
Local features and kernels for classification of texture and object
categories: A comprehensive study
International Journal of Computer Vision 2007
<a class="calibre3 pcalibre" href="http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf">http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf</a></li>
</ul>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-259">
<span id="calibre_link-911" class="calibre4"></span><h1 class="calibre5">4.8. 预测目标 (<code class="docutils2"><span class="pre">y</span></code>) 的转换</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/FontTian" class="calibre3 pcalibre">@FontTian</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@numpy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@程威</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper" id="calibre_link-912">
<h2 class="sigil_not_in_toc">4.8.1. 标签二值化</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer" title="sklearn.preprocessing.LabelBinarizer"><code class="docutils"><span class="calibre4">LabelBinarizer</span></code></a> 是一个用来从多类别列表创建标签矩阵的工具类:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">preprocessing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">LabelBinarizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span><span class="calibre4">.</span><span class="calibre4">classes_</span>
<span class="calibre4">array([1, 2, 4, 6])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">])</span>
<span class="calibre4">array([[1, 0, 0, 0],</span>
<span class="calibre4">       [0, 0, 0, 1]])</span>
</pre>
</div>
</div>
<p class="calibre10">对于多类别是实例，可以使用 <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer" title="sklearn.preprocessing.MultiLabelBinarizer"><code class="docutils"><span class="calibre4">MultiLabelBinarizer</span></code></a>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">MultiLabelBinarizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">([(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">,)])</span>
<span class="calibre4">array([[1, 1, 0],</span>
<span class="calibre4">       [0, 0, 1]])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lb</span><span class="calibre4">.</span><span class="calibre4">classes_</span>
<span class="calibre4">array([1, 2, 3])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-913">
<h2 class="sigil_not_in_toc">4.8.2. 标签编码</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" title="sklearn.preprocessing.LabelEncoder"><code class="docutils"><span class="calibre4">LabelEncoder</span></code></a> 是一个可以用来将标签规范化的工具类，它可以将标签的编码值范围限定在[0,n_classes-1].
这在编写高效的Cython程序时是非常有用的. <a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder" title="sklearn.preprocessing.LabelEncoder"><code class="docutils"><span class="calibre4">LabelEncoder</span></code></a> 可以如下使用:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">preprocessing</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">LabelEncoder</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">])</span>
<span class="calibre4">LabelEncoder()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">classes_</span>
<span class="calibre4">array([1, 2, 6])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">])</span>
<span class="calibre4">array([0, 0, 1, 2])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">inverse_transform</span><span class="calibre4">([</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">])</span>
<span class="calibre4">array([1, 1, 2, 6])</span>
</pre>
</div>
</div>
<p class="calibre10">当然，它也可以用于非数值型标签的编码转换成数值标签（只要它们是可哈希并且可比较的）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span> <span class="calibre4">=</span> <span class="calibre4">preprocessing</span><span class="calibre4">.</span><span class="calibre4">LabelEncoder</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">([</span><span class="calibre4">"paris"</span><span class="calibre4">,</span> <span class="calibre4">"paris"</span><span class="calibre4">,</span> <span class="calibre4">"tokyo"</span><span class="calibre4">,</span> <span class="calibre4">"amsterdam"</span><span class="calibre4">])</span>
<span class="calibre4">LabelEncoder()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">classes_</span><span class="calibre4">)</span>
<span class="calibre4">['amsterdam', 'paris', 'tokyo']</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">([</span><span class="calibre4">"tokyo"</span><span class="calibre4">,</span> <span class="calibre4">"tokyo"</span><span class="calibre4">,</span> <span class="calibre4">"paris"</span><span class="calibre4">])</span>
<span class="calibre4">array([2, 2, 1])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">le</span><span class="calibre4">.</span><span class="calibre4">inverse_transform</span><span class="calibre4">([</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]))</span>
<span class="calibre4">['tokyo', 'tokyo', 'paris']</span>
</pre>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-221">
<span id="calibre_link-914" class="calibre4"></span><h1 class="calibre5">5. 数据集加载工具</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@不吃曲奇的趣多多</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@A</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@火星</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Trembleguy</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@cowboy</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@peels</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@t9UhoI</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Sun</a><br class="calibre9" />     
    </div>
<p class="calibre10">该 <code class="docutils"><span class="calibre4">sklearn.datasets</span></code> 包装在 <a class="calibre3 pcalibre" href="../tutorial/basic/tutorial.html#loading-example-dataset"><span class="calibre4">Getting Started</span></a> 部分中嵌入了介绍一些小型玩具的数据集。</p>
<p class="calibre10">为了在控制数据的统计特性（通常是特征的 correlation （相关性）和 informativeness （信息性））的同时评估数据集 (<code class="docutils"><span class="calibre4">n_samples</span></code> 和 <code class="docutils"><span class="calibre4">n_features</span></code>) 的规模的影响，也可以生成综合数据。</p>
<p class="calibre10">这个软件包还具有帮助用户获取更大的数据集的功能，这些数据集通常由机器学习社区使用，用于对来自 ‘real world’ 的数据进行检测算法。</p>
<div class="toctree-wrapper" id="calibre_link-915">
<h2 class="sigil_not_in_toc">5.1. 通用数据集 API</h2>
<p class="calibre2">对于不同类型的数据集，有三种不同类型的数据集接口。最简单的是样品图像的界面，下面在 <a class="calibre3 pcalibre" href="#calibre_link-222"><span class="calibre4">样本图片</span></a> 部分中进行了描述。</p>
<p class="calibre10">数据集生成函数和 svmlight 加载器分享了一个较为简化的接口，返回一个由 <code class="docutils"><span class="calibre4">n_samples</span></code> * <code class="docutils"><span class="calibre4">n_features</span></code> 组成的 tuple <code class="docutils"><span class="calibre4">(X,</span> <span class="calibre4">y)</span></code> 其中的 <code class="docutils"><span class="calibre4">X</span></code> 是 numpy 数组 <code class="docutils"><span class="calibre4">y</span></code> 是包含目标值的长度为 <code class="docutils"><span class="calibre4">n_samples</span></code> 的数组</p>
<p class="calibre10">玩具数据集以及 ‘real world’ 数据集和从 mldata.org 获取的数据集具有更复杂的结构。这些函数返回一个类似于字典的对象包含至少两项：一个具有 <code class="docutils"><span class="calibre4">data</span></code> 键（key）的 <code class="docutils"><span class="calibre4">n_samples</span></code> * <code class="docutils"><span class="calibre4">n_features</span></code> 形状的数组（除了20个新组之外except for 20newsgroups）和一个具有 <code class="docutils"><span class="calibre4">target</span></code> 键（key）的包含 target values （目标值）的 <code class="docutils"><span class="calibre4">n_samples</span></code> 长度的 numpy 数组。</p>
<p class="calibre10">数据集还包含一些对``DESCR`` 描述，同时一部分也包含 <code class="docutils"><span class="calibre4">feature_names</span></code> 和 <a href="#calibre_link-223" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-916">``</span></a>target_names``的特征。有关详细信息，请参阅下面的数据集说明</p>
</div>
<div class="toctree-wrapper" id="calibre_link-917">
<h2 class="sigil_not_in_toc">5.2. 玩具数据集</h2>
<p class="calibre2">scikit-learn 内置有一些小型标准数据集，不需要从某个外部网站下载任何文件。</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston" title="sklearn.datasets.load_boston"><code class="docutils"><span class="calibre4">load_boston</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the boston house-prices dataset (regression).</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris" title="sklearn.datasets.load_iris"><code class="docutils"><span class="calibre4">load_iris</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the iris dataset (classification).</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes" title="sklearn.datasets.load_diabetes"><code class="docutils"><span class="calibre4">load_diabetes</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the diabetes dataset (regression).</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits" title="sklearn.datasets.load_digits"><code class="docutils"><span class="calibre4">load_digits</span></code></a>([n_class,&nbsp;return_X_y])</td>
<td class="label1">Load and return the digits dataset (classification).</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_linnerud.html#sklearn.datasets.load_linnerud" title="sklearn.datasets.load_linnerud"><code class="docutils"><span class="calibre4">load_linnerud</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the linnerud dataset (multivariate regression).</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine" title="sklearn.datasets.load_wine"><code class="docutils"><span class="calibre4">load_wine</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the wine dataset (classification).</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer" title="sklearn.datasets.load_breast_cancer"><code class="docutils"><span class="calibre4">load_breast_cancer</span></code></a>([return_X_y])</td>
<td class="label1">Load and return the breast cancer wisconsin dataset (classification).</td>
</tr>
</tbody>
</table>
<p class="calibre10">这些数据集有助于快速说明在 scikit 中实现的各种算法的行为。然而，它们数据规模往往太小，无法代表真实世界的机器学习任务。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-222">
<span id="calibre_link-918" class="calibre4"></span><h2 class="sigil_not_in_toc">5.3. 样本图片</h2>
<p class="calibre2">scikit 在通过图片的作者共同授权下嵌入了几个样本 JPEG 图片。这些图像为了方便用户对 test algorithms （测试算法）和 pipeline on 2D data （二维数据管道）进行测试。</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_sample_images.html#sklearn.datasets.load_sample_images" title="sklearn.datasets.load_sample_images"><code class="docutils"><span class="calibre4">load_sample_images</span></code></a>()</td>
<td class="label1">Load sample images for image manipulation.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_sample_image.html#sklearn.datasets.load_sample_image" title="sklearn.datasets.load_sample_image"><code class="docutils"><span class="calibre4">load_sample_image</span></code></a>(image_name)</td>
<td class="label1">Load the numpy array of a single sample image</td>
</tr>
</tbody>
</table>
<a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_color_quantization.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_color_quantization_0011.png" class="align-right1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000747.jpg" /></a>
<p class="calibre10">默认编码的图像是基于 <code class="docutils"><span class="calibre4">uint8</span></code> dtype 到空闲内存。通常，如果把输入转换为浮点数表示，机器学习算法的效果最好。另外，如果你计划使用 <code class="docutils"><span class="calibre4">matplotlib.pyplpt.imshow</span></code> 别忘了尺度范围 0 - 1，如下面的示例所做的。</p>
<div class="toctree-wrapper">
<p class="calibre10">示例:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/cluster/plot_color_quantization.html#sphx-glr-auto-examples-cluster-plot-color-quantization-py"><span class="calibre4">Color Quantization using K-Means</span></a></li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-919">
<span id="calibre_link-920" class="calibre4"></span><h2 class="sigil_not_in_toc">5.4. 样本生成器</h2>
<p class="calibre2">此外，scikit-learn 包括各种随机样本的生成器，可以用来建立可控制的大小和复杂性人工数据集。</p>
<div class="toctree-wrapper" id="calibre_link-921">
<h3 class="sigil_not_in_toc1">5.4.1. 分类和聚类生成器</h3>
<p class="calibre2">这些生成器将产生一个相应特征的离散矩阵。</p>
<div class="toctree-wrapper" id="calibre_link-922">
<h4 class="sigil_not_in_toc1">5.4.1.1. 单标签</h4>
<p class="calibre2"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs" title="sklearn.datasets.make_blobs"><code class="docutils"><span class="calibre4">make_blobs</span></code></a> 和  <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification" title="sklearn.datasets.make_classification"><code class="docutils"><span class="calibre4">make_classification</span></code></a> 通过分配每个类的一个或多个正态分布的点的群集创建的多类数据集。 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs" title="sklearn.datasets.make_blobs"><code class="docutils"><span class="calibre4">make_blobs</span></code></a> 对于中心和各簇的标准偏差提供了更好的控制，可用于演示聚类。 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification" title="sklearn.datasets.make_classification"><code class="docutils"><span class="calibre4">make_classification</span></code></a> 专门通过引入相关的，冗余的和未知的噪音特征；将高斯集群的每类复杂化；在特征空间上进行线性变换。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_gaussian_quantiles.html#sklearn.datasets.make_gaussian_quantiles" title="sklearn.datasets.make_gaussian_quantiles"><code class="docutils"><span class="calibre4">make_gaussian_quantiles</span></code></a>  将single Gaussian cluster （单高斯簇）分成近乎相等大小的同心超球面分离。 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_hastie_10_2.html#sklearn.datasets.make_hastie_10_2" title="sklearn.datasets.make_hastie_10_2"><code class="docutils"><span class="calibre4">make_hastie_10_2</span></code></a> 产生类似的二进制、10维问题。</p>
<a class="calibre3 pcalibre" href="../auto_examples/datasets/plot_random_dataset.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_random_dataset_0011.png" class="align-center1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000828.jpg" /></a>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_circles.html#sklearn.datasets.make_circles" title="sklearn.datasets.make_circles"><code class="docutils"><span class="calibre4">make_circles</span></code></a> and :func:<a href="#calibre_link-224" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-923">`</span></a>make_moons`生成二维分类数据集时可以帮助确定算法（如质心聚类或线性分类），包括可以选择性加入高斯噪声。它们有利于可视化。用球面决策边界对高斯数据生成二值分类。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-924">
<h4 class="sigil_not_in_toc1">5.4.1.2. 多标签</h4>
<p class="calibre2"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_multilabel_classification.html#sklearn.datasets.make_multilabel_classification" title="sklearn.datasets.make_multilabel_classification"><code class="docutils"><span class="calibre4">make_multilabel_classification</span></code></a> 生成多个标签的随机样本，反映从a mixture of topics（一个混合的主题）中引用a bag of words （一个词袋）。每个文档的主题数是基于泊松分布随机提取的，同时主题本身也是从固定的随机分布中提取的。同样地，单词的数目是基于泊松分布提取的，单词通过多项式被抽取，其中每个主题定义了单词的概率分布。在以下方面真正简化了 bag-of-words mixtures （单词混合包）：</p>
<ul class="calibre6">
<li class="toctree-l">独立绘制的每个主题词分布，在现实中，所有这些都会受到稀疏基分布的影响，并将相互关联。</li>
<li class="toctree-l">对于从文档中生成多个主题，所有主题在生成单词包时都是同等权重的。</li>
<li class="toctree-l">随机产生没有标签的文件，而不是基于分布（base distribution）来产生文档</li>
</ul>
<a class="calibre3 pcalibre" href="../auto_examples/datasets/plot_random_multilabel_dataset.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_random_multilabel_dataset_0011.png" class="align-center2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000071.jpg" /></a>
</div>
<div class="toctree-wrapper" id="calibre_link-925">
<h4 class="sigil_not_in_toc1">5.4.1.3. 二分聚类</h4>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_biclusters.html#sklearn.datasets.make_biclusters" title="sklearn.datasets.make_biclusters"><code class="docutils"><span class="calibre4">make_biclusters</span></code></a>(shape,&nbsp;n_clusters[,&nbsp;noise,&nbsp;…])</td>
<td class="label1">Generate an array with constant block diagonal structure for biclustering.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_checkerboard.html#sklearn.datasets.make_checkerboard" title="sklearn.datasets.make_checkerboard"><code class="docutils"><span class="calibre4">make_checkerboard</span></code></a>(shape,&nbsp;n_clusters[,&nbsp;…])</td>
<td class="label1">Generate an array with block checkerboard structure for biclustering.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-926">
<h3 class="sigil_not_in_toc1">5.4.2. 回归生成器</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression" title="sklearn.datasets.make_regression"><code class="docutils"><span class="calibre4">make_regression</span></code></a> 产生的回归目标作为一个可选择的稀疏线性组合的具有噪声的随机的特征。它的信息特征可能是不相关的或低秩（少数特征占大多数的方差）。</p>
<p class="calibre10">其他回归生成器产生确定性的随机特征函数。 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_sparse_uncorrelated.html#sklearn.datasets.make_sparse_uncorrelated" title="sklearn.datasets.make_sparse_uncorrelated"><code class="docutils"><span class="calibre4">make_sparse_uncorrelated</span></code></a> 产生目标为一个有四个固定系数的线性组合。其他编码明确的非线性关系：<a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1" title="sklearn.datasets.make_friedman1"><code class="docutils"><span class="calibre4">make_friedman1</span></code></a> 与多项式和正弦相关变换相联系； <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_friedman2.html#sklearn.datasets.make_friedman2" title="sklearn.datasets.make_friedman2"><code class="docutils"><span class="calibre4">make_friedman2</span></code></a> 包括特征相乘与交互； <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_friedman3.html#sklearn.datasets.make_friedman3" title="sklearn.datasets.make_friedman3"><code class="docutils"><span class="calibre4">make_friedman3</span></code></a> 类似与对目标的反正切变换。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-927">
<h3 class="sigil_not_in_toc1">5.4.3. 流形学习生成器</h3>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_s_curve.html#sklearn.datasets.make_s_curve" title="sklearn.datasets.make_s_curve"><code class="docutils"><span class="calibre4">make_s_curve</span></code></a>([n_samples,&nbsp;noise,&nbsp;random_state])</td>
<td class="label1">Generate an S curve dataset.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_swiss_roll.html#sklearn.datasets.make_swiss_roll" title="sklearn.datasets.make_swiss_roll"><code class="docutils"><span class="calibre4">make_swiss_roll</span></code></a>([n_samples,&nbsp;noise,&nbsp;random_state])</td>
<td class="label1">Generate a swiss roll dataset.</td>
</tr>
</tbody>
</table>
</div>
<div class="toctree-wrapper" id="calibre_link-928">
<h3 class="sigil_not_in_toc1">5.4.4. 生成器分解</h3>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="10%" class="label"></col>
<col width="90%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_low_rank_matrix.html#sklearn.datasets.make_low_rank_matrix" title="sklearn.datasets.make_low_rank_matrix"><code class="docutils"><span class="calibre4">make_low_rank_matrix</span></code></a>([n_samples,&nbsp;…])</td>
<td class="label1">Generate a mostly low rank matrix with bell-shaped singular values</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_sparse_coded_signal.html#sklearn.datasets.make_sparse_coded_signal" title="sklearn.datasets.make_sparse_coded_signal"><code class="docutils"><span class="calibre4">make_sparse_coded_signal</span></code></a>(n_samples,&nbsp;…[,&nbsp;…])</td>
<td class="label1">Generate a signal as a sparse combination of dictionary elements.</td>
</tr>
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_spd_matrix.html#sklearn.datasets.make_spd_matrix" title="sklearn.datasets.make_spd_matrix"><code class="docutils"><span class="calibre4">make_spd_matrix</span></code></a>(n_dim[,&nbsp;random_state])</td>
<td class="label1">Generate a random symmetric, positive-definite matrix.</td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.make_sparse_spd_matrix.html#sklearn.datasets.make_sparse_spd_matrix" title="sklearn.datasets.make_sparse_spd_matrix"><code class="docutils"><span class="calibre4">make_sparse_spd_matrix</span></code></a>([dim,&nbsp;alpha,&nbsp;…])</td>
<td class="label1">Generate a sparse symmetric definite positive matrix.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-929">
<span id="calibre_link-930" class="calibre4"></span><h2 class="sigil_not_in_toc">5.5. Datasets in svmlight / libsvm format</h2>
<p class="calibre2">scikit-learn 中有加载svmlight / libsvm格式的数据集的功能函数。此种格式中，每行
采用如 <code class="docutils"><span class="calibre4">&lt;label&gt;</span> <span class="calibre4">&lt;feature-id&gt;:&lt;feature-value&gt;&lt;feature-id&gt;:&lt;feature-value&gt;</span> <span class="calibre4">...</span></code>
的形式。这种格式尤其适合稀疏数据集，在该模块中，数据集 <code class="docutils"><span class="calibre4">X</span></code> 使用的是scipy稀疏CSR矩阵，
特征集 <code class="docutils"><span class="calibre4">y</span></code> 使用的是numpy数组。</p>
<p class="calibre10">你可以通过如下步骤加载数据集:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">load_svmlight_file</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span> <span class="calibre4">=</span> <span class="calibre4">load_svmlight_file</span><span class="calibre4">(</span><span class="calibre4">"/path/to/train_dataset.txt"</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                                        
</pre>
</div>
</div>
<p class="calibre10">你也可以一次加载两个或多个的数据集:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">load_svmlight_files</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">(</span><span class="calibre4">"/path/to/train_dataset.txt"</span><span class="calibre4">,</span> <span class="calibre4">"/path/to/test_dataset.txt"</span><span class="calibre4">))</span>
<span class="calibre4">... </span>                                                        
</pre>
</div>
</div>
<p class="calibre10">这种情况下，保证了 <code class="docutils"><span class="calibre4">X_train</span></code> 和 <code class="docutils"><span class="calibre4">X_test</span></code> 具有相同的特征数量。
固定特征的数量也可以得到同样的结果:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">load_svmlight_file</span><span class="calibre4">(</span>
<span class="calibre4">... </span>    <span class="calibre4">"/path/to/test_dataset.txt"</span><span class="calibre4">,</span> <span class="calibre4">n_features</span><span class="calibre4">=</span><span class="calibre4">X_train</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">... </span>                                                        
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">相关链接:</p>
<p class="calibre10"><span class="calibre4" id="calibre_link-931">svmlight / libsvm 格式的公共数据集</span>: <a class="calibre3 pcalibre" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets</a></p>
<p class="calibre10"><span class="calibre4" id="calibre_link-932">更快的API兼容的实现</span>: <a class="calibre3 pcalibre" href="https://github.com/mblondel/svmlight-loader">https://github.com/mblondel/svmlight-loader</a></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-933">
<span id="calibre_link-934" class="calibre4"></span><h2 class="sigil_not_in_toc">5.6. 从外部数据集加载</h2>
<p class="calibre2">scikit-learn使用任何存储为numpy数组或者scipy稀疏数组的数值数据。
其他可以转化成数值数组的类型也可以接受，如pandas中的DataFrame。</p>
<p class="calibre10">以下推荐一些将标准纵列形式的数据转换为scikit-learn可以使用的格式的方法:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://pandas.pydata.org/pandas-docs/stable/io.html">pandas.io</a>
提供了从常见格式(包括CSV,Excel,JSON,SQL等)中读取数据的工具.DateFrame 也可以从由
元组或者字典组成的列表构建而成.Pandas能顺利的处理异构的数据，并且提供了处理和转换
成方便scikit-learn使用的数值数据的工具。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://docs.scipy.org/doc/scipy/reference/io.html">scipy.io</a>
专门处理科学计算领域经常使用的二进制格式，例如.mat和.arff格式的内容。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://docs.scipy.org/doc/numpy/reference/routines.io.html">numpy/routines.io</a>
将纵列形式的数据标准的加载为numpy数组</li>
<li class="toctree-l">scikit-learn的 :func:<a href="#calibre_link-225" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-935">`</span></a>datasets.load_svmlight_file`处理svmlight或者libSVM稀疏矩阵</li>
<li class="toctree-l">scikit-learn的 <code class="docutils"><span class="calibre4">datasets.load_files</span></code> 处理文本文件组成的目录，每个目录名是每个
类别的名称，每个目录内的每个文件对应该类别的一个样本</li>
</ul>
<p class="calibre10">对于一些杂项数据，例如图像，视屏，音频。您可以参考:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://scikit-image.org/docs/dev/api/skimage.io.html">skimage.io</a> 或
<a class="calibre3 pcalibre" href="https://imageio.readthedocs.io/en/latest/userapi.html">Imageio</a>
将图像或者视屏加载为numpy数组</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imread.html#scipy.misc.imread">scipy.misc.imread</a> (requires the <a class="calibre3 pcalibre" href="https://pypi.python.org/pypi/Pillow">Pillow</a> package)将各种图像文件格式加载为
像素灰度数据</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html">scipy.io.wavfile.read</a>
将WAV文件读入一个numpy数组</li>
</ul>
<p class="calibre10">存储为字符串的无序(或者名字)特征(在pandas的DataFrame中很常见)需要转换为整数，当整数类别变量
被编码成独热变量(<a class="calibre3 pcalibre" href="../modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder"><code class="docutils"><span class="calibre4">sklearn.preprocessing.OneHotEncoder</span></code></a>)或类似数据时，它或许可以被最好的利用。
参见 <a class="calibre3 pcalibre" href="../modules/preprocessing.html#preprocessing"><span class="calibre4">预处理数据</span></a>.</p>
<p class="calibre10">注意：如果你要管理你的数值数据，建议使用优化后的文件格式来减少数据加载时间,例如HDF5。像
H5Py, PyTables和pandas等的各种库提供了一个Python接口，来读写该格式的数据。</p>
<div class="toctree-wrapper">
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-936">
<span id="calibre_link-937" class="calibre4"></span><h2 class="sigil_not_in_toc">5.7. Olivetti 脸部数据集</h2>
<p class="calibre2">该数据集包含 1992年4月至1994年4月在AT＆T实验室剑桥采集的一组面部图像。
该 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_olivetti_faces.html#sklearn.datasets.fetch_olivetti_faces" title="sklearn.datasets.fetch_olivetti_faces"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_olivetti_faces</span></code></a> 函数是从AT＆T下载
数据存档的数据获取/缓存函数。</p>
<dl class="calibre10">
<dt class="calibre18">如原网站所述：</dt>
<dd class="calibre19">有四十个不同的个体，每个个体有十张不同的图片。对于某些个体，图像在不同时间拍摄并且改变
照明和面部表情(睁开/闭上眼睛， 微小/不微笑)和面部细节(戴眼镜/不带眼镜)。所有的图像采用
黑色均匀的背景，个体处于直立的正面位置。(容许一定的侧移)</dd>
</dl>
<p class="calibre10">图像被量化为256个的灰度级并以8位无符号整数的形式存储；加载器将这些无符号整数转换为[0,1]之间
的浮点值，这样能方面很多算法的使用。</p>
<p class="calibre10">该数据库的”目标”一个是从0到39的整数，代表着图中人物的身份。然而，由于每一类只有十个样例，从
无监督学习或半监督学习的角度来看，这个相对较小的数据集更加有趣。</p>
<p class="calibre10">原始的数据集由92 x 112大小的图像组成，然而这里提供的版本由64 x 64大小的图像组成。</p>
<p class="calibre10">当使用这些图像时， 请致谢AT&amp;T剑桥实验室。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-938">
<span id="calibre_link-939" class="calibre4"></span><h2 class="sigil_not_in_toc">5.8. 20个新闻组文本数据集</h2>
<p class="calibre2">20个新闻组文本数据集包含有关20个主题的大约18000个新闻组，被分为两个子集：一个用于
训练(或者开发)，另一个用于测试(或者用于性能评估)。训练和测试集的划分是基于某个特定日期
前后发布的消息。</p>
<p class="calibre10">这个模块包含两个加载器。第一个是 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_20newsgroups</span></code></a>，
返回一个能够被文本特征提取器接受的原始文本列表，例如 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">sklearn.feature_extraction.text.CountVectorizer</span></code></a>
使用自定义的参数来提取特征向量。第二个是 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized" title="sklearn.datasets.fetch_20newsgroups_vectorized"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_20newsgroups_vectorized</span></code></a>，
返回即用特征，换句话说就是，这样就没必要使用特征提取器了。</p>
<div class="toctree-wrapper" id="calibre_link-940">
<h3 class="sigil_not_in_toc1">5.8.1. 用法</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_20newsgroups</span></code></a>  是一个用于从原始的20个新闻组网址( <a class="calibre3 pcalibre" href="http://people.csail.mit.edu/jrennie/20Newsgroups/">20 newsgroups website</a>)</div>
</blockquote>
<p class="calibre10">下载数据归档的数据获取/缓存函数，提取 <code class="docutils"><span class="calibre4">~/scikit_learn_data/20news_home</span></code> 文件夹中的
归档内容。并且在训练集或测试集文件夹，或者两者上调用函数 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files" title="sklearn.datasets.load_files"><code class="docutils"><span class="calibre4">sklearn.datasets.load_files</span></code></a>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_20newsgroups</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'train'</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">pprint</span> <span class="calibre4">import</span> <span class="calibre4">pprint</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pprint</span><span class="calibre4">(</span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">))</span>
<span class="calibre4">['alt.atheism',</span>
<span class="calibre4"> 'comp.graphics',</span>
<span class="calibre4"> 'comp.os.ms-windows.misc',</span>
<span class="calibre4"> 'comp.sys.ibm.pc.hardware',</span>
<span class="calibre4"> 'comp.sys.mac.hardware',</span>
<span class="calibre4"> 'comp.windows.x',</span>
<span class="calibre4"> 'misc.forsale',</span>
<span class="calibre4"> 'rec.autos',</span>
<span class="calibre4"> 'rec.motorcycles',</span>
<span class="calibre4"> 'rec.sport.baseball',</span>
<span class="calibre4"> 'rec.sport.hockey',</span>
<span class="calibre4"> 'sci.crypt',</span>
<span class="calibre4"> 'sci.electronics',</span>
<span class="calibre4"> 'sci.med',</span>
<span class="calibre4"> 'sci.space',</span>
<span class="calibre4"> 'soc.religion.christian',</span>
<span class="calibre4"> 'talk.politics.guns',</span>
<span class="calibre4"> 'talk.politics.mideast',</span>
<span class="calibre4"> 'talk.politics.misc',</span>
<span class="calibre4"> 'talk.religion.misc']</span>
</pre>
</div>
</div>
<p class="calibre10">真实数据在属性 <code class="docutils"><span class="calibre4">filenames</span></code> 和 <code class="docutils"><span class="calibre4">target</span></code> 中，target属性就是类别的整数索引:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">filenames</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(11314,)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(11314,)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">10</span><span class="calibre4">]</span>
<span class="calibre4">array([12,  6,  9,  8,  6,  7,  9,  2, 13, 19])</span>
</pre>
</div>
</div>
<p class="calibre10">可以通过将类别列表传给 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" title="sklearn.datasets.fetch_20newsgroups"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_20newsgroups</span></code></a> 函数来实现只加载一部分的类别:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cats</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'alt.atheism'</span><span class="calibre4">,</span> <span class="calibre4">'sci.space'</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'train'</span><span class="calibre4">,</span> <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">cats</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">)</span>
<span class="calibre4">['alt.atheism', 'sci.space']</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">filenames</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1073,)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1073,)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">10</span><span class="calibre4">]</span>
<span class="calibre4">array([1, 1, 1, 0, 1, 0, 0, 1, 1, 1])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-941">
<h3 class="sigil_not_in_toc1">5.8.2. 将文本转换成向量</h3>
<p class="calibre2">为了用文本数据训练预测或者聚类模型，首先需要做的是将文本转换成适合统计分析的数值
向量。这能使用 <code class="docutils"><span class="calibre4">sklearn.feature_extraction.text</span></code> 的功能来实现，正如下面展示的
从一个20个新闻的子集中提取单个词的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Tf-idf">TF-IDF</a> 向量的例子</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">TfidfVectorizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">categories</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'alt.atheism'</span><span class="calibre4">,</span> <span class="calibre4">'talk.religion.misc'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>              <span class="calibre4">'comp.graphics'</span><span class="calibre4">,</span> <span class="calibre4">'sci.space'</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'train'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                      <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectorizer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(2034, 34118)</span>
</pre>
</div>
</div>
<p class="calibre10">提取的TF-IDF向量非常稀疏，在一个超过30000维的空间中采样，
平均只有159个非零成分(少于.5%的非零成分):</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors</span><span class="calibre4">.</span><span class="calibre4">nnz</span> <span class="calibre4">/</span> <span class="calibre4">float</span><span class="calibre4">(</span><span class="calibre4">vectors</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">])</span>
<span class="calibre4">159.01327433628319</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized" title="sklearn.datasets.fetch_20newsgroups_vectorized"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_20newsgroups_vectorized</span></code></a> 是一个返回即用的tfidf特征的函数
，而不是返回文件名。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-942">
<h3 class="sigil_not_in_toc1">5.8.3. 过滤文本进行更加逼真的训练</h3>
<p class="calibre2">分类器很容易过拟合一个出现在20个新闻组数据中的特定事物，例如新闻组标头。许多分类器有
很好的F分数，但是他们的结果不能泛化到不在这个时间窗的其他文档。</p>
<p class="calibre10">例如，我们来看一下多项式贝叶斯分类器，它训练速度快并且能获得很好的F分数。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.naive_bayes</span> <span class="calibre4">import</span> <span class="calibre4">MultinomialNB</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_test</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'test'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                     <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors_test</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MultinomialNB</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=.</span><span class="calibre4">01</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">vectors</span><span class="calibre4">,</span> <span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">vectors_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">f1_score</span><span class="calibre4">(</span><span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)</span>
<span class="calibre4">0.88213592402729568</span>
</pre>
</div>
</div>
<p class="calibre10">(<a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a> 的例子将训练和测试数据混合，
而不是按时间划分，这种情况下，多项式贝叶斯能得到更高的0.88的F分数.你是否还不信任这个分类器的内部实现？)</p>
<p class="calibre10">让我们看看信息量最大一些特征是:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">def</span> <span class="calibre4">show_top10</span><span class="calibre4">(</span><span class="calibre4">classifier</span><span class="calibre4">,</span> <span class="calibre4">vectorizer</span><span class="calibre4">,</span> <span class="calibre4">categories</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">feature_names</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">asarray</span><span class="calibre4">(</span><span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">get_feature_names</span><span class="calibre4">())</span>
<span class="calibre4">... </span>    <span class="calibre4">for</span> <span class="calibre4">i</span><span class="calibre4">,</span> <span class="calibre4">category</span> <span class="calibre4">in</span> <span class="calibre4">enumerate</span><span class="calibre4">(</span><span class="calibre4">categories</span><span class="calibre4">):</span>
<span class="calibre4">... </span>        <span class="calibre4">top10</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">argsort</span><span class="calibre4">(</span><span class="calibre4">classifier</span><span class="calibre4">.</span><span class="calibre4">coef_</span><span class="calibre4">[</span><span class="calibre4">i</span><span class="calibre4">])[</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">:]</span>
<span class="calibre4">... </span>        <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4">: </span><span class="calibre4">%s</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">category</span><span class="calibre4">,</span> <span class="calibre4">" "</span><span class="calibre4">.</span><span class="calibre4">join</span><span class="calibre4">(</span><span class="calibre4">feature_names</span><span class="calibre4">[</span><span class="calibre4">top10</span><span class="calibre4">])))</span>
<span class="calibre4">...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">show_top10</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">vectorizer</span><span class="calibre4">,</span> <span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">)</span>
<span class="calibre4">alt.atheism: sgi livesey atheists writes people caltech com god keith edu</span>
<span class="calibre4">comp.graphics: organization thanks files subject com image lines university edu graphics</span>
<span class="calibre4">sci.space: toronto moon gov com alaska access henry nasa edu space</span>
<span class="calibre4">talk.religion.misc: article writes kent people christian jesus sandvik edu com god</span>
</pre>
</div>
</div>
<p class="calibre10">你现在可以看到这些特征过拟合了许多东西:</p>
<ul class="calibre6">
<li class="toctree-l">几乎所有的组都通过标题是出现更多还是更少来区分，例如 <code class="docutils"><span class="calibre4">NNTP-Posting-Host:</span></code> 和 <code class="docutils"><span class="calibre4">Distribution:</span></code> 标题</li>
<li class="toctree-l">正如他的标头或者签名所表示，另外重要的特征有关发送者是否隶属于一个大学。</li>
<li class="toctree-l">“article”这个单词是一个重要的特征，它基于人们像 “In article [article ID], [name] &lt;[e-mail address]&gt;
wrote:” 的方式引用原先的帖子频率。</li>
<li class="toctree-l">其他特征和当时发布的特定的人的名字和e-mail相匹配。</li>
</ul>
<p class="calibre10">有如此大量的线索来区分新闻组，分类器根本不需要从文本中识别主题，而且他们的性能都一样好。</p>
<p class="calibre10">由于这个原因，加载20个新闻组数据的函数提供了一个叫做 <strong class="calibre14">remove</strong> 的参数，来告诉函数需要从文件
中去除什么类别的信息。 <strong class="calibre14">remove</strong> 应该是一个来自集合 <code class="docutils"><span class="calibre4">('headers',</span> <span class="calibre4">'footers',</span> <span class="calibre4">'quotes')</span></code> 的子集
的元组，来告诉函数分别移除标头标题，签名块还有引用块。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_test</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'test'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                     <span class="calibre4">remove</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">'headers'</span><span class="calibre4">,</span> <span class="calibre4">'footers'</span><span class="calibre4">,</span> <span class="calibre4">'quotes'</span><span class="calibre4">),</span>
<span class="calibre4">... </span>                                     <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors_test</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">vectors_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">f1_score</span><span class="calibre4">(</span><span class="calibre4">pred</span><span class="calibre4">,</span> <span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)</span>
<span class="calibre4">0.77310350681274775</span>
</pre>
</div>
</div>
<p class="calibre10">由于我们移除了跟主题分类几乎没有关系的元数据，分类器的F分数降低了很多。
如果我们从训练数据中也移除这个元数据，F分数将会更低:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">newsgroups_train</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'train'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                      <span class="calibre4">remove</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">'headers'</span><span class="calibre4">,</span> <span class="calibre4">'footers'</span><span class="calibre4">,</span> <span class="calibre4">'quotes'</span><span class="calibre4">),</span>
<span class="calibre4">... </span>                                      <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MultinomialNB</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=.</span><span class="calibre4">01</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">vectors</span><span class="calibre4">,</span> <span class="calibre4">newsgroups_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">vectors_test</span> <span class="calibre4">=</span> <span class="calibre4">vectorizer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pred</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">vectors_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">f1_score</span><span class="calibre4">(</span><span class="calibre4">newsgroups_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">pred</span><span class="calibre4">,</span> <span class="calibre4">average</span><span class="calibre4">=</span><span class="calibre4">'macro'</span><span class="calibre4">)</span>
<span class="calibre4">0.76995175184521725</span>
</pre>
</div>
</div>
<dl class="calibre10">
<dt class="calibre18">其他的一些分类器能够更好的处理这个更难版本的任务。试着带 <code class="docutils"><span class="calibre4">--filter</span></code> 选项和不带 <code class="docutils"><span class="calibre4">--filter</span></code> 选项运行</dt>
<dd class="calibre19"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="calibre4">Sample pipeline for text feature extraction and evaluation</span></a> 来比较结果间的差异。</dd>
</dl>
<div class="toctree-wrapper">
<p class="calibre10">推荐</p>
<dl class="calibre10">
<dt class="calibre18">当使用20个新闻组数据中评估文本分类器时，你应该移除与新闻组相关的元数据。你可以通过设置</dt>
<dd class="calibre19"><code class="docutils"><span class="calibre4">remove=('headers',</span> <span class="calibre4">'footers',</span> <span class="calibre4">'quotes')</span></code> 来实现。F分数将更加低因为这更符合实际</dd>
</dl>
</div>
<div class="toctree-wrapper">
<p class="calibre10">例子</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="calibre4">Sample pipeline for text feature extraction and evaluation</span></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="../auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="calibre4">Classification of text documents using sparse features</span></a></li>
</ul>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-943">
<span id="calibre_link-944" class="calibre4"></span><h2 class="sigil_not_in_toc">5.9. 从 mldata.org 上下载数据集</h2>
<p class="calibre2"><a class="calibre3 pcalibre" href="http://mldata.org">mldata.org</a> 是一个公开的机器学习数据 repository ,由 <a class="calibre3 pcalibre" href="http://www.pascal-network.org">PASCAL network</a> 负责支持。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">sklearn.datasets</span></code> 包可以使用函数 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_mldata.html#sklearn.datasets.fetch_mldata" title="sklearn.datasets.fetch_mldata"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_mldata</span></code></a> 直接从 repository 下载数据集。</p>
<p class="calibre10">举个例子，下载 MNIST 手写数字字符识别数据集:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_mldata</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mnist</span> <span class="calibre4">=</span> <span class="calibre4">fetch_mldata</span><span class="calibre4">(</span><span class="calibre4">'MNIST original'</span><span class="calibre4">,</span> <span class="calibre4">data_home</span><span class="calibre4">=</span><span class="calibre4">custom_data_home</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">MNIST 手写数字字符数据集包含有 70000 个样本，每个样本带有从 0 到 9 的标签，并且样本像素尺寸大小为 28x28:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mnist</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(70000, 784)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">mnist</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(70000,)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">unique</span><span class="calibre4">(</span><span class="calibre4">mnist</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
<span class="calibre4">array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])</span>
</pre>
</div>
</div>
<p class="calibre10">首次下载之后，数据集被缓存在本地的由 <code class="docutils"><span class="calibre4">data_home</span></code> 关键字指定的路径中，路径默认是 <code class="docutils"><span class="calibre4">~/scikit_learn_data/</span></code></p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">os</span><span class="calibre4">.</span><span class="calibre4">listdir</span><span class="calibre4">(</span><span class="calibre4">os</span><span class="calibre4">.</span><span class="calibre4">path</span><span class="calibre4">.</span><span class="calibre4">join</span><span class="calibre4">(</span><span class="calibre4">custom_data_home</span><span class="calibre4">,</span> <span class="calibre4">'mldata'</span><span class="calibre4">))</span>
<span class="calibre4">['mnist-original.mat']</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="http://mldata.org">mldata.org</a> 里的数据集在命名规则和数据格式上不遵循严格的约定。
<a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_mldata.html#sklearn.datasets.fetch_mldata" title="sklearn.datasets.fetch_mldata"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_mldata</span></code></a> 可以应对大多数的常见情况，并且允许个人对数据集的默认设置进行调整:</p>
<ul class="calibre6">
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="http://mldata.org">mldata.org</a> 中的数据大多都是以 <code class="docutils"><span class="calibre4">(n_features,</span> <span class="calibre4">n_samples)</span></code> 这样的组织形式存在。
这与 <code class="docutils"><span class="calibre4">scikit-learn</span></code> 中的习惯约定是不一致的，所以 <a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_mldata.html#sklearn.datasets.fetch_mldata" title="sklearn.datasets.fetch_mldata"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_mldata</span></code></a> 默认情况下通过 <code class="docutils"><span class="calibre4">transpose_data</span></code> 关键字控制对这个矩阵进行转置运算。:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">fetch_mldata</span><span class="calibre4">(</span><span class="calibre4">'iris'</span><span class="calibre4">,</span> <span class="calibre4">data_home</span><span class="calibre4">=</span><span class="calibre4">custom_data_home</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 4)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">fetch_mldata</span><span class="calibre4">(</span><span class="calibre4">'iris'</span><span class="calibre4">,</span> <span class="calibre4">transpose_data</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                    <span class="calibre4">data_home</span><span class="calibre4">=</span><span class="calibre4">custom_data_home</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(4, 150)</span>
</pre>
</div>
</div>
</li>
<li class="toctree-l"><p class="first">数据集有多列的时候，<a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_mldata.html#sklearn.datasets.fetch_mldata" title="sklearn.datasets.fetch_mldata"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_mldata</span></code></a> 这个函数会识别目标列和数据列，
并将它们重命名为 <code class="docutils"><span class="calibre4">target（目标）</span></code> 和 <code class="docutils"><span class="calibre4">data（数据）</span></code> 。
这是通过在数据集中寻找名为 <code class="docutils"><span class="calibre4">label（标签）</span></code> 和 <code class="docutils"><span class="calibre4">data（数据）</span></code> 的数组来完成的，
如果选择第一个数组是 <code class="docutils"><span class="calibre4">target（目标）</span></code>，而第二个数组是 <code class="docutils"><span class="calibre4">data（数据）</span></code> ，则前边的设置会失效。
这个行为可以通过对关键字 <code class="docutils"><span class="calibre4">target_name</span></code> 和 <code class="docutils"><span class="calibre4">data_name</span></code> 进行设置来改变，设置的值可以是具体的名字也可以是索引数字，
数据集中列的名字和索引序号都可以在 <a class="calibre3 pcalibre" href="http://mldata.org">mldata.org</a> 中的 “Data” 选项卡下找到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris2</span> <span class="calibre4">=</span> <span class="calibre4">fetch_mldata</span><span class="calibre4">(</span><span class="calibre4">'datasets-UCI iris'</span><span class="calibre4">,</span> <span class="calibre4">target_name</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">data_name</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                     <span class="calibre4">data_home</span><span class="calibre4">=</span><span class="calibre4">custom_data_home</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris3</span> <span class="calibre4">=</span> <span class="calibre4">fetch_mldata</span><span class="calibre4">(</span><span class="calibre4">'datasets-UCI iris'</span><span class="calibre4">,</span> <span class="calibre4">target_name</span><span class="calibre4">=</span><span class="calibre4">'class'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                     <span class="calibre4">data_name</span><span class="calibre4">=</span><span class="calibre4">'double0'</span><span class="calibre4">,</span> <span class="calibre4">data_home</span><span class="calibre4">=</span><span class="calibre4">custom_data_home</span><span class="calibre4">)</span>
</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-945">
<span id="calibre_link-946" class="calibre4"></span><h2 class="sigil_not_in_toc">5.10. 带标签的人脸识别数据集</h2>
<p class="calibre2">这个数据集是一个在互联网上收集的名人 JPEG 图片集，所有详细信息都可以在官方网站上获得:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="http://vis-www.cs.umass.edu/lfw/">http://vis-www.cs.umass.edu/lfw/</a></div>
</blockquote>
<p class="calibre10">每张图片的居中部分都是一张脸。典型人物人脸验证是给定两幅图片，二元分类器必须能够预测这两幅图片是否是同一个人。</p>
<p class="calibre10">另一项任务人脸识别或面部识别说的是给定一个未知的面孔，通过参考一系列已经学习经过鉴定的人的照片来识别此人的名字。</p>
<p class="calibre10">人脸验证和人脸识别都是基于经过训练用于人脸检测的模型的输出所进行的任务。 最流行的人脸检测模型叫作 Viola Jones是在 OpenCV 库中实现的。 LFW 人脸数据库中的人脸是使用该人脸检测器从各种在线网站上提取的。</p>
<div class="toctree-wrapper" id="calibre_link-947">
<h3 class="sigil_not_in_toc1">5.10.1. 用法</h3>
<p class="calibre2"><code class="docutils"><span class="calibre4">scikit-learn</span></code> 提供两个可以自动下载、缓存、解析元数据文件的 loader (加载器)，解码 JPEG
并且将 slices 转换成内存映射过的 NumPy 数组(numpy.memmap)。
这个数据集大小超过 200 MB。第一个加载器通常需要超过几分钟才能完全解码 JPEG 文件的相关部分为 NumPy 数组。
如果数据集已经被加载过，通过在磁盘上采用内存映射版( memmaped version )的 memoized，
即 <code class="docutils"><span class="calibre4">~/scikit_learn_data/lfw_home/</span></code> 文件夹使用 <code class="docutils"><span class="calibre4">joblib</span></code>，再次加载时间会小于 200ms。</p>
<p class="calibre10">第一个 loader (加载器)用于人脸识别任务:一个多类分类任务(属于监督学习):</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_lfw_people</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lfw_people</span> <span class="calibre4">=</span> <span class="calibre4">fetch_lfw_people</span><span class="calibre4">(</span><span class="calibre4">min_faces_per_person</span><span class="calibre4">=</span><span class="calibre4">70</span><span class="calibre4">,</span> <span class="calibre4">resize</span><span class="calibre4">=</span><span class="calibre4">0.4</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">name</span> <span class="calibre4">in</span> <span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">:</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">name</span><span class="calibre4">)</span>
<span class="calibre4">...</span>
<span class="calibre4">Ariel Sharon</span>
<span class="calibre4">Colin Powell</span>
<span class="calibre4">Donald Rumsfeld</span>
<span class="calibre4">George W Bush</span>
<span class="calibre4">Gerhard Schroeder</span>
<span class="calibre4">Hugo Chavez</span>
<span class="calibre4">Tony Blair</span>
</pre>
</div>
</div>
<p class="calibre10">默认的 slice 是一个删除掉大部分背景，只剩下围绕着脸周围的长方形的形状:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">dtype</span>
<span class="calibre4">dtype('float32')</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1288, 1850)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1288, 50, 37)</span>
</pre>
</div>
</div>
<p class="calibre10">在 <code class="docutils"><span class="calibre4">target(目标)</span></code> 数组中，<a href="#calibre_link-226" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-948">``</span></a>1140``个人脸图片中的每一个图都分配一个属于某人的 id:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1288,)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">10</span><span class="calibre4">])</span>
<span class="calibre4">[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]</span>
</pre>
</div>
</div>
<p class="calibre10">第二个 loader (加载器)通常用于人脸验证任务: 每个样本是属于或不属于同一个人的两张图片:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span> &gt;&gt;&gt; from sklearn.datasets import fetch_lfw_pairs
 &gt;&gt;&gt; lfw_pairs_train = fetch_lfw_pairs(subset='train')

 &gt;&gt;&gt; list(lfw_pairs_train.target_names)
 ['Different persons', 'Same person']

 &gt;&gt;&gt; lfw_pairs_train.pairs.shape
 (2200, 2, 62, 47)

 &gt;&gt;&gt; lfw_pairs_train.data.shape
 (2200, 5828)

 &gt;&gt;&gt; lfw_pairs_train.target.shape
 (2200,)

:func:`sklearn.datasets.fetch_lfw_people` 和 :func:`sklearn.datasets.fetch_lfw_pairs` 函数，都可以通过 ``color=True`` 来获得 RGB 颜色通道的维度，在这种情况下尺寸将为 ``(2200, 2, 62, 47, 3)`` 。
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_lfw_pairs.html#sklearn.datasets.fetch_lfw_pairs" title="sklearn.datasets.fetch_lfw_pairs"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_lfw_pairs</span></code></a> 数据集细分为 3 类:
<code class="docutils"><span class="calibre4">train</span></code> set(训练集)、<code class="docutils"><span class="calibre4">test</span></code> set(测试集)和一个 <code class="docutils"><span class="calibre4">10_folds</span></code> 评估集, <code class="docutils"><span class="calibre4">10_folds</span></code> 评估集意味着性能的计算指标使用 10 折交叉验证( 10-folds cross validation )方案。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://vis-www.cs.umass.edu/lfw/lfw.pdf">Labeled Faces in the Wild: A Database for Studying Face Recognition
in Unconstrained Environments.</a>
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.
University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.</li>
</ul>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-949">
<h3 class="sigil_not_in_toc1">5.10.2. 示例</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="calibre4">Faces recognition example using eigenfaces and SVMs</span></a></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-950">
<span id="calibre_link-951" class="calibre4"></span><h2 class="sigil_not_in_toc">5.11. 森林覆盖类型</h2>
<p class="calibre2">这个数据集中的样本对应美国的 30×30m 的 patches of forest(森林区域)，
收集这些数据用于预测每个 patch 的植被 cover type (覆盖类型)，即优势树种。
总共有七个植被类型，使得这是一个多分类问题。
每个样本有 54 个特征，在 <a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Covertype">dataset’s 的主页</a> 中有具体的描述。
有些特征是布尔指标，其他的是离散或者连续的量。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype" title="sklearn.datasets.fetch_covtype"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_covtype</span></code></a> 将加载 covertype 数据集；
它返回一个类似字典的对象，并在数据成员中使用特征矩阵以及 <code class="docutils"><span class="calibre4">target</span></code> 中的目标值。
如果需要，数据集可以从网上下载。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-952">
<span id="calibre_link-953" class="calibre4"></span><h2 class="sigil_not_in_toc">5.12. RCV1 数据集</h2>
<p class="calibre2">路透社语料库第一卷( RCV1 )是路透社为了研究目的提供的一个拥有超过 800,000 份手动分类的新闻报导的文档库。该数据集在 <a class="calibre3 pcalibre" href="#calibre_link-227" id="calibre_link-228">[1]</a> 中有详细描述。</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1" title="sklearn.datasets.fetch_rcv1"><code class="docutils"><span class="calibre4">sklearn.datasets.fetch_rcv1</span></code></a> 将加载以下版本: RCV1-v2, vectors, full sets, topics multilabels:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_rcv1</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rcv1</span> <span class="calibre4">=</span> <span class="calibre4">fetch_rcv1</span><span class="calibre4">()</span>
</pre>
</div>
</div>
<p class="calibre10">它返回一个类似字典的对象，具有以下属性:</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">data</span></code>:
特征矩阵是一个 scipy CSR 稀疏矩阵，有 804414 个样品和 47236 个特征。
非零值包含 cosine-normalized(余弦归一化)，log TF-IDF vectors。
按照年代顺序近似划分，在 <a class="calibre3 pcalibre" href="#calibre_link-227" id="calibre_link-229">[1]</a> 提出: 前 23149 个样本是训练集。后 781265 个样本是测试集。
这是官方的 LYRL2004 时间划分。数组有 0.16% 个非零值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rcv1</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(804414, 47236)</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">target</span></code>:
目标值是存储在 scipy CSR 的稀疏矩阵，有 804414 个样本和 103 个类别。
每个样本在其所属的类别中的值为 1，在其他类别中值为 0。数组有 3.15% 个非零值:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rcv1</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(804414, 103)</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">sample_id</span></code>:
每个样本都可以通过从 2286 到 810596 不等的 ID 来标识:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rcv1</span><span class="calibre4">.</span><span class="calibre4">sample_id</span><span class="calibre4">[:</span><span class="calibre4">3</span><span class="calibre4">]</span>
<span class="calibre4">array([2286, 2287, 2288], dtype=uint32)</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">target_names</span></code>:
目标值是每个样本的 topic (主题)。每个样本至少属于一个 topic (主题)最多 17 个 topic 。
总共有 103 个 topics ，每个 topic 用一个字符串表示。
从 <cite class="calibre13">GMIL</cite> 出现 5 次到 <cite class="calibre13">CCAT</cite> 出现 381327 次，该语料库频率跨越五个数量级:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rcv1</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[:</span><span class="calibre4">3</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">tolist</span><span class="calibre4">()</span>  
<span class="calibre4">['E11', 'ECAT', 'M11']</span>
</pre>
</div>
</div>
<p class="calibre10">如果有需要的话，可以从 <a class="calibre3 pcalibre" href="http://jmlr.csail.mit.edu/papers/volume5/lewis04a/">rcv1 homepage</a> 上下载该数据集。
数据集压缩后的大小大约是 656 MB。</p>
<div class="toctree-wrapper">
<p class="calibre10">参考文献</p>
<table class="docutils1" frame="void" id="calibre_link-227" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">[1]</td>
<td class="label1"><em class="calibre13">(<a class="calibre3 pcalibre" href="#calibre_link-228">1</a>, <a class="calibre3 pcalibre" href="#calibre_link-229">2</a>)</em> Lewis, D. D., Yang, Y., Rose, T. G., &amp; Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-954">
<span id="calibre_link-955" class="calibre4"></span><h2 class="sigil_not_in_toc">5.13. 波士顿房价数据集</h2>
<div class="toctree-wrapper" id="calibre_link-956">
<h3 class="sigil_not_in_toc1">5.13.1. 注释</h3>
<p class="calibre2">数据集特征:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1">506</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1">13 数值型或类别型，帮助预测的属性</td>
</tr>
</tbody>
</table>
<p class="calibre10">:中位数（第14个属性）经常是学习目标</p>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">属性信息 (按顺序):</th>
<td class="label1"><ul class="calibre6">
<li class="toctree-l">CRIM     城镇人均犯罪率</li>
<li class="toctree-l">ZN       占地面积超过2.5万平方英尺的住宅用地比例</li>
<li class="toctree-l">INDUS    城镇非零售业务地区的比例</li>
<li class="toctree-l">CHAS     查尔斯河虚拟变量 (= 1 如果土地在河边；否则是0)</li>
<li class="toctree-l">NOX      一氧化氮浓度（每1000万份）</li>
<li class="toctree-l">RM       平均每居民房数</li>
<li class="toctree-l">AGE      在1940年之前建成的所有者占用单位的比例</li>
<li class="toctree-l">DIS      与五个波士顿就业中心的加权距离</li>
<li class="toctree-l">RAD      辐射状公路的可达性指数</li>
<li class="toctree-l">TAX      每10,000美元的全额物业税率</li>
<li class="toctree-l">PTRATIO  城镇师生比例</li>
<li class="toctree-l">B        1000(Bk - 0.63)^2 其中 Bk 是城镇的黑人比例</li>
<li class="toctree-l">LSTAT    人口中地位较低人群的百分数</li>
<li class="toctree-l">MEDV     以1000美元计算的自有住房的中位数</li>
</ul>
</td>
</tr>
<tr class="row-odd"><th class="head">缺失属性值:</th>
<td class="label1"><p class="calibre10">无</p>
</td>
</tr>
<tr class="calibre23"><th class="head">创建者:</th>
<td class="label1"><p class="calibre10">Harrison, D. and Rubinfeld, D.L.</p>
</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="calibre10">这是UCI ML（欧文加利福尼亚大学 机器学习库）房价数据集的副本。
<a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Housing">http://archive.ics.uci.edu/ml/datasets/Housing</a></p>
<p class="calibre10">该数据集是从位于卡内基梅隆大学维护的StatLib图书馆取得的。</p>
<p class="calibre10">Harrison, D. 和 Rubinfeld, D.L. 的波士顿房价数据：’Hedonic
prices and the demand for clean air’, J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978，也被使用在 Belsley, Kuh &amp; Welsch 的 ‘Regression diagnostics
…’, Wiley, 1980。
注释：许多变化已经被应用在后者第244-261页的表中。</p>
<p class="calibre10">波士顿房价数据已被用于许多涉及回归问题的机器学习论文中。</p>
<p class="calibre10"><strong class="calibre14">参考资料</strong></p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">Belsley, Kuh &amp; Welsch, ‘Regression diagnostics: Identifying Influential Data and Sources of Collinearity’, Wiley, 1980. 244-261.</li>
<li class="toctree-l">Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.</li>
<li class="toctree-l">many more! (see <a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Housing">http://archive.ics.uci.edu/ml/datasets/Housing</a>)</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-957">
<span id="calibre_link-958" class="calibre4"></span><h2 class="sigil_not_in_toc">5.14. 威斯康辛州乳腺癌（诊断）数据库</h2>
<div class="toctree-wrapper" id="calibre_link-959">
<h3 class="sigil_not_in_toc1">5.14.1. 注释</h3>
<dl class="calibre10">
<dt class="calibre18">数据集特征：</dt>
<dd class="calibre19"><table class="first5" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1"><p class="calibre10">569</p>
</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1"><p class="calibre10">30 (数值型，帮助预测的属性和类)</p>
</td>
</tr>
<tr class="calibre23"><th class="head" colspan="2">Attribute Information:</th>
</tr>
<tr class="row-odd"><td class="label1">&nbsp;</td>
<td class="label1"><ul class="calibre6">
<li class="toctree-l">radius 半径（从中心到边缘上点的距离的平均值）</li>
<li class="toctree-l">texture 纹理（灰度值的标准偏差）</li>
<li class="toctree-l">perimeter 周长</li>
<li class="toctree-l">area 区域</li>
<li class="toctree-l">smoothness 平滑度（半径长度的局部变化）</li>
<li class="toctree-l">compactness 紧凑度（周长 ^ 2 /面积 - 1.0）</li>
<li class="toctree-l">concavity 凹面（轮廓的凹部的严重性）</li>
<li class="toctree-l">concave points 凹点（轮廓的凹部的数量）</li>
<li class="toctree-l">symmetry 对称性</li>
<li class="toctree-l">fractal dimension 分形维数（海岸线近似 - 1）</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">类:</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l">WDBC-Malignant 恶性</li>
<li class="toctree-l">WDBC-Benign 良性</li>
</ul>
</dd>
</dl>
</li>
</ul>
<p class="calibre10">对每个图像计算这些特征的平均值，标准误差，以及“最差”（因为是肿瘤）或最大值（最大的前三个值的平均值）</p>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="calibre10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;得到30个特征。例如，字段 3 是平均半径，字段 13 是半径的标准误差，字段 23 是最差半径。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">统计摘要:</th>
<td class="label1"><table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="76%" class="label"></col>
<col width="12%" class="label"></col>
<col width="12%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">radius (mean):</td>
<td class="label1">6.981</td>
<td class="label1">28.11</td>
</tr>
<tr class="row-odd"><td class="label1">texture (mean):</td>
<td class="label1">9.71</td>
<td class="label1">39.28</td>
</tr>
<tr class="calibre23"><td class="label1">perimeter (mean):</td>
<td class="label1">43.79</td>
<td class="label1">188.5</td>
</tr>
<tr class="row-odd"><td class="label1">area (mean):</td>
<td class="label1">143.5</td>
<td class="label1">2501.0</td>
</tr>
<tr class="calibre23"><td class="label1">smoothness (mean):</td>
<td class="label1">0.053</td>
<td class="label1">0.163</td>
</tr>
<tr class="row-odd"><td class="label1">compactness (mean):</td>
<td class="label1">0.019</td>
<td class="label1">0.345</td>
</tr>
<tr class="calibre23"><td class="label1">concavity (mean):</td>
<td class="label1">0.0</td>
<td class="label1">0.427</td>
</tr>
<tr class="row-odd"><td class="label1">concave points (mean):</td>
<td class="label1">0.0</td>
<td class="label1">0.201</td>
</tr>
<tr class="calibre23"><td class="label1">symmetry (mean):</td>
<td class="label1">0.106</td>
<td class="label1">0.304</td>
</tr>
<tr class="row-odd"><td class="label1">fractal dimension (mean):</td>
<td class="label1">0.05</td>
<td class="label1">0.097</td>
</tr>
<tr class="calibre23"><td class="label1">radius (standard error):</td>
<td class="label1">0.112</td>
<td class="label1">2.873</td>
</tr>
<tr class="row-odd"><td class="label1">texture (standard error):</td>
<td class="label1">0.36</td>
<td class="label1">4.885</td>
</tr>
<tr class="calibre23"><td class="label1">perimeter (standard error):</td>
<td class="label1">0.757</td>
<td class="label1">21.98</td>
</tr>
<tr class="row-odd"><td class="label1">area (standard error):</td>
<td class="label1">6.802</td>
<td class="label1">542.2</td>
</tr>
<tr class="calibre23"><td class="label1">smoothness (standard error):</td>
<td class="label1">0.002</td>
<td class="label1">0.031</td>
</tr>
<tr class="row-odd"><td class="label1">compactness (standard error):</td>
<td class="label1">0.002</td>
<td class="label1">0.135</td>
</tr>
<tr class="calibre23"><td class="label1">concavity (standard error):</td>
<td class="label1">0.0</td>
<td class="label1">0.396</td>
</tr>
<tr class="row-odd"><td class="label1">concave points (standard error):</td>
<td class="label1">0.0</td>
<td class="label1">0.053</td>
</tr>
<tr class="calibre23"><td class="label1">symmetry (standard error):</td>
<td class="label1">0.008</td>
<td class="label1">0.079</td>
</tr>
<tr class="row-odd"><td class="label1">fractal dimension (standard error):</td>
<td class="label1">0.001</td>
<td class="label1">0.03</td>
</tr>
<tr class="calibre23"><td class="label1">radius (worst):</td>
<td class="label1">7.93</td>
<td class="label1">36.04</td>
</tr>
<tr class="row-odd"><td class="label1">texture (worst):</td>
<td class="label1">12.02</td>
<td class="label1">49.54</td>
</tr>
<tr class="calibre23"><td class="label1">perimeter (worst):</td>
<td class="label1">50.41</td>
<td class="label1">251.2</td>
</tr>
<tr class="row-odd"><td class="label1">area (worst):</td>
<td class="label1">185.2</td>
<td class="label1">4254.0</td>
</tr>
<tr class="calibre23"><td class="label1">smoothness (worst):</td>
<td class="label1">0.071</td>
<td class="label1">0.223</td>
</tr>
<tr class="row-odd"><td class="label1">compactness (worst):</td>
<td class="label1">0.027</td>
<td class="label1">1.058</td>
</tr>
<tr class="calibre23"><td class="label1">concavity (worst):</td>
<td class="label1">0.0</td>
<td class="label1">1.252</td>
</tr>
<tr class="row-odd"><td class="label1">concave points (worst):</td>
<td class="label1">0.0</td>
<td class="label1">0.291</td>
</tr>
<tr class="calibre23"><td class="label1">symmetry (worst):</td>
<td class="label1">0.156</td>
<td class="label1">0.664</td>
</tr>
<tr class="row-odd"><td class="label1">fractal dimension (worst):</td>
<td class="label1">0.055</td>
<td class="label1">0.208</td>
</tr>
</tbody>
</table>
<table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">缺失属性值:</th>
<td class="label1">无</td>
</tr>
<tr class="row-odd"><th class="head">类别分布:</th>
<td class="label1">212 - 恶性, 357 - 良性</td>
</tr>
<tr class="calibre23"><th class="head">创建者:</th>
<td class="label1">Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian</td>
</tr>
<tr class="row-odd"><th class="head">捐助者:</th>
<td class="label1">Nick Street</td>
</tr>
<tr class="calibre23"><th class="head">日期:</th>
<td class="label1">1995年11月</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="calibre10">这是UCI ML（欧文加利福尼亚大学 机器学习库）威斯康星州乳腺癌（诊断）数据集的副本。
<a class="calibre3 pcalibre" href="https://goo.gl/U2Uwz2">https://goo.gl/U2Uwz2</a></p>
<p class="calibre10">这些特征是从乳房肿块的细针抽吸术（FNA）的数字图像中计算得到，描述了图像中存在的细胞核的特征。</p>
<p class="calibre10">上述的分离平面是由多表面方法树（MSM-T）[K.P.Bennett, “Decision Tree Construction Via
Linear Programming.” Proceedings of the 4th Midwest Artificial Intelligence and
Cognitive Science Society, pp.97-101, 1992], a classification method which uses
linear programming to construct a decision tree.
相关特征是在1-4的特征和1-3的分离平面中使用穷举法搜索选取出的。</p>
<p class="calibre10">用于分离平面的线性规划在三维空间中描述如下：
[K. P. Bennett and O. L. Mangasarian: “Robust Linear Programming Discrimination
of Two Linearly Inseparable Sets”, Optimization Methods and Software 1, 1992, 23-34].</p>
<p class="calibre10">该数据库也可通过UW CS ftp服务器获得：</p>
<p class="calibre10">ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WDBC/</p>
</div>
<div class="toctree-wrapper" id="calibre_link-960">
<h3 class="sigil_not_in_toc1">5.14.2. 参考资料</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction
for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on
Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
San Jose, CA, 1993.</li>
<li class="toctree-l">O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and
prognosis via linear programming. Operations Research, 43(4), pages 570-577,
July-August 1995.</li>
<li class="toctree-l">W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)
163-171.</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-961">
<span id="calibre_link-962" class="calibre4"></span><h2 class="sigil_not_in_toc">5.15. 糖尿病数据集</h2>
<div class="toctree-wrapper" id="calibre_link-963">
<h3 class="sigil_not_in_toc1">5.15.1. 注释</h3>
<p class="calibre2">从442例糖尿病患者中获得了十个基线变量，年龄，性别，体重指数，平均血压和六个血清测量值，以及一个我们感兴趣的，在基线后一年疾病发展的定量测量值。</p>
<p class="calibre10">数据集特征:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1"><p class="calibre10">442</p>
</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1"><p class="calibre10">前10列是数值型的帮助预测的值</p>
</td>
</tr>
<tr class="calibre23"><th class="head">目标:</th>
<td class="label1"><p class="calibre10">第11列是基线后一年疾病进展的定量测量址</p>
</td>
</tr>
<tr class="row-odd"><th class="head">属性:</th>
<td class="label1"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">Age年龄:</th>
<td class="label1"></td>
</tr>
<tr class="row-odd"><th class="head">Sex性别:</th>
<td class="label1"></td>
</tr>
<tr class="calibre23"><th class="head" colspan="2">Body mass index体重指数:</th>
</tr>
<tr class="row-odd"><td class="label1">&nbsp;</td>
<td class="label1"></td>
</tr>
<tr class="calibre23"><th class="head" colspan="2">Average blood pressure平均血压:</th>
</tr>
<tr class="row-odd"><td class="label1">&nbsp;</td>
<td class="label1"></td>
</tr>
<tr class="calibre23"><th class="head">S1血清测量值1:</th>
<td class="label1"></td>
</tr>
<tr class="row-odd"><th class="head">S2血清测量值2:</th>
<td class="label1"></td>
</tr>
<tr class="calibre23"><th class="head">S3血清测量值3:</th>
<td class="label1"></td>
</tr>
<tr class="row-odd"><th class="head">S4血清测量值4:</th>
<td class="label1"></td>
</tr>
<tr class="calibre23"><th class="head">S5血清测量值5:</th>
<td class="label1"></td>
</tr>
<tr class="row-odd"><th class="head">S6血清测量值6:</th>
<td class="label1"></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="calibre10">注意: 这10个特征变量都已经分别以均值为中心，并按照标准偏差乘以样本数（n_samples）进行缩放（即每列的平方和为1）。</p>
<p class="calibre10">源 URL:
<a class="calibre3 pcalibre" href="http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html">http://www4.stat.ncsu.edu/~boos/var.select/diabetes.html</a></p>
<p class="calibre10">更多信息，请参阅:
Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics (with discussion), 407-499.
(<a class="calibre3 pcalibre" href="http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">http://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf</a>)</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-964">
<span id="calibre_link-965" class="calibre4"></span><h2 class="sigil_not_in_toc">5.16. 光学识别手写数字数据集</h2>
<div class="toctree-wrapper" id="calibre_link-966">
<h3 class="sigil_not_in_toc1">5.16.1. 注释</h3>
<dl class="calibre10">
<dt class="calibre18">数据集特征：</dt>
<dd class="calibre19"><table class="first5" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1"><p class="calibre10">5620</p>
</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1"><p class="calibre10">64</p>
</td>
</tr>
<tr class="calibre23"><th class="head">属性信息:</th>
<td class="label1"><p class="calibre10">8x8 范围在（0-16）的整型像素值图片</p>
</td>
</tr>
<tr class="row-odd"><th class="head">缺失属性值:</th>
<td class="label1"><p class="calibre10">无</p>
</td>
</tr>
<tr class="calibre23"><th class="head">创建者:</th>
<td class="label1"><ol class="arabic" start="5">
<li class="toctree-l">Alpaydin (<a class="calibre3 pcalibre" href="mailto:alpaydin%40boun.edu.tr">alpaydin<span class="calibre4">@</span>boun<span class="calibre4">.</span>edu<span class="calibre4">.</span>tr</a>)</li>
</ol>
</td>
</tr>
<tr class="row-odd"><th class="head">日期:</th>
<td class="label1"><p class="calibre10">1998年7月</p>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="calibre10">这是UCI ML（欧文加利福尼亚大学 机器学习库）手写数字数据集的测试集的副本。
<a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits</a></p>
<p class="calibre10">数据集包含手写数字的图像：10个类别中每个类都是一个数字。</p>
<p class="calibre10">从预印表格中提取手写数字的标准化的位图这一过程，应用了NIST提供的预处理程序。
这些数据是从43人中得到，其中30人为训练集，13人为测试集。32x32位图被划分为4x4的非重叠块，
并且在每个块中计数像素数。这产生8×8的输入矩阵，其中每个元素是0-16范围内的整数。
这个过程降低了维度，并且在小的变形中提供了不变性。</p>
<p class="calibre10">有关NIST处理程序的信息，请参见 M. D. Garris, J. L. Blue, G.T. Candela,
D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
1994.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-967">
<h3 class="sigil_not_in_toc1">5.16.2. 参考资料</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
Graduate Studies in Science and Engineering, Bogazici University.</li>
<li class="toctree-l"><ol class="first3" start="5">
<li class="toctree-l">Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.</li>
</ol>
</li>
<li class="toctree-l">Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.
Linear dimensionalityreduction using relevance weighted LDA. School of
Electrical and Electronic Engineering Nanyang Technological University.
2005.</li>
<li class="toctree-l">Claudio Gentile. A New Approximate Maximal Margin Classification
Algorithm. NIPS. 2000.</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-968">
<span id="calibre_link-969" class="calibre4"></span><h2 class="sigil_not_in_toc">5.17. 鸢尾花数据集</h2>
<div class="toctree-wrapper" id="calibre_link-970">
<h3 class="sigil_not_in_toc1">5.17.1. 注释</h3>
<dl class="calibre10">
<dt class="calibre18">数据集特征:</dt>
<dd class="calibre19"><table class="first4" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1"><p class="calibre10">150 (三个类各有50个)</p>
</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1"><p class="calibre10">4 (数值型，数值型，帮助预测的属性和类)</p>
</td>
</tr>
<tr class="calibre23"><th class="head" colspan="2">Attribute Information:</th>
</tr>
<tr class="row-odd"><td class="label1">&nbsp;</td>
<td class="label1"><ul class="calibre6">
<li class="toctree-l">sepal length 萼片长度（厘米）</li>
<li class="toctree-l">sepal width 萼片宽度（厘米）</li>
<li class="toctree-l">petal length 花瓣长度（厘米）</li>
<li class="toctree-l">petal width 花瓣宽度（厘米）</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">class:</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l">Iris-Setosa 山鸢尾</li>
<li class="toctree-l">Iris-Versicolour 变色鸢尾</li>
<li class="toctree-l">Iris-Virginica 维吉尼亚鸢尾</li>
</ul>
</dd>
</dl>
</li>
</ul>
</td>
</tr>
<tr class="calibre23"><th class="head">统计摘要:</th>
<td class="label1"></td>
</tr>
</tbody>
</table>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="26%" class="label"></col>
<col width="7%" class="label"></col>
<col width="7%" class="label"></col>
<col width="13%" class="label"></col>
<col width="9%" class="label"></col>
<col width="37%" class="label"></col>
</colgroup>
<thead valign="bottom" class="calibre24">
<tr class="calibre23"><th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1">sepal length:</td>
<td class="label1">4.3</td>
<td class="label1">7.9</td>
<td class="label1">5.84</td>
<td class="label1">0.83</td>
<td class="label1">0.7826</td>
</tr>
<tr class="row-odd"><td class="label1">sepal width:</td>
<td class="label1">2.0</td>
<td class="label1">4.4</td>
<td class="label1">3.05</td>
<td class="label1">0.43</td>
<td class="label1">-0.4194</td>
</tr>
<tr class="calibre23"><td class="label1">petal length:</td>
<td class="label1">1.0</td>
<td class="label1">6.9</td>
<td class="label1">3.76</td>
<td class="label1">1.76</td>
<td class="label1">0.9490  (high!)</td>
</tr>
<tr class="row-odd"><td class="label1">petal width:</td>
<td class="label1">0.1</td>
<td class="label1">2.5</td>
<td class="label1">1.20</td>
<td class="label1">0.76</td>
<td class="label1">0.9565  (high!)</td>
</tr>
</tbody>
</table>
<table class="last1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">缺失属性值:</th>
<td class="label1">无</td>
</tr>
<tr class="row-odd"><th class="head">类别分布:</th>
<td class="label1">3个类别各占33.3%</td>
</tr>
<tr class="calibre23"><th class="head">创建者:</th>
<td class="label1">R.A. Fisher</td>
</tr>
<tr class="row-odd"><th class="head">捐助者:</th>
<td class="label1">Michael Marshall (<a class="calibre3 pcalibre" href="mailto:MARSHALL%PLU%40io.arc.nasa.gov">MARSHALL%PLU<span class="calibre4">@</span>io<span class="calibre4">.</span>arc<span class="calibre4">.</span>nasa<span class="calibre4">.</span>gov</a>)</td>
</tr>
<tr class="calibre23"><th class="head">日期:</th>
<td class="label1">1988年7月</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="calibre10">这是UCI ML（欧文加利福尼亚大学 机器学习库）鸢尾花数据集的副本。
<a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Iris">http://archive.ics.uci.edu/ml/datasets/Iris</a></p>
<p class="calibre10">著名的鸢尾花数据库，首先由R. Fisher先生使用。</p>
<p class="calibre10">这可能是在模式识别文献中最有名的数据库。Fisher的论文是这个领域的经典之作，到今天也经常被引用。（例如：Duda＆Hart）
数据集包含3个类，每类有50个实例，每个类指向一种类型的鸢尾花。一类与另外两类线性分离，而后者不能彼此线性分离。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-971">
<h3 class="sigil_not_in_toc1">5.17.2. 参考资料</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">Fisher,R.A. “The use of multiple measurements in taxonomic problems”
Annual Eugenics, 7, Part II, 179-188 (1936); also in “Contributions to
Mathematical Statistics” (John Wiley, NY, 1950).</li>
<li class="toctree-l">Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.
(Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.</li>
<li class="toctree-l">Dasarathy, B.V. (1980) “Nosing Around the Neighborhood: A New System
Structure and Classification Rule for Recognition in Partially Exposed
Environments”.  IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. PAMI-2, No. 1, 67-71.</li>
<li class="toctree-l">Gates, G.W. (1972) “The Reduced Nearest Neighbor Rule”.  IEEE Transactions
on Information Theory, May 1972, 431-433.</li>
<li class="toctree-l">See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al”s AUTOCLASS II
conceptual clustering system finds 3 classes in the data.</li>
<li class="toctree-l">Many, many more …</li>
</ul>
</div>
</blockquote>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-972">
<span id="calibre_link-973" class="calibre4"></span><h2 class="sigil_not_in_toc">5.18. Linnerrud 数据集</h2>
<div class="toctree-wrapper" id="calibre_link-974">
<h3 class="sigil_not_in_toc1">5.18.1. 注释</h3>
<dl class="calibre10">
<dt class="calibre18">数据集特征:</dt>
<dd class="calibre19"><table class="first5" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">实例数量:</th>
<td class="label1">20</td>
</tr>
<tr class="row-odd"><th class="head">属性数量:</th>
<td class="label1">3</td>
</tr>
<tr class="calibre23"><th class="head">缺失属性值:</th>
<td class="label1">无</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="calibre10">Linnerud 数据集包含两个小的数据集:</p>
<ul class="calibre6">
<li class="toctree-l"><em class="calibre13">运动</em> ： 一个包含以下内容的列表：运动数据，关于3个运动相关变量的20个观测值：体重，腰围和脉搏。</li>
<li class="toctree-l"><em class="calibre13">生理</em> ： 一个包含以下内容的数据表：生理数据，关于三个生理变量的20个观测值：下巴，仰卧起坐和跳跃。</li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-975">
<h3 class="sigil_not_in_toc1">5.18.2. 参考资料</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.</li>
</ul>
</div>
</blockquote>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-252">
<span id="calibre_link-976" class="calibre4"></span><h1 class="calibre5">6. 大规模计算的策略: 更大量的数据</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@文谊</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@ゞFingヤ</a><br class="calibre9" />
    </div>
<p class="calibre10">对于一些应用程序，需要被处理的样本数量,特征数量（或两者）和/或速度这些对传统的方法而言非常具有挑战性。在这些情况下，scikit-learn 有许多你值得考虑的选项可以使你的系统规模化。</p>
<div class="toctree-wrapper" id="calibre_link-977">
<h2 class="sigil_not_in_toc">6.1. 使用外核学习实例进行拓展</h2>
<p class="calibre2">外核（或者称作 “外部存储器”）学习是一种用于学习那些无法装进计算机主存储（RAM）的数据的技术。</p>
<p class="calibre10">这里描述了一种为了实现这一目的而设计的系统：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">一种用流来传输实例的方式</li>
<li class="toctree-l">一种从实例中提取特征的方法</li>
<li class="toctree-l">增量式算法</li>
</ol>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-978">
<h3 class="sigil_not_in_toc1">6.1.1. 流式实例</h3>
<p class="calibre2">基本上， 1. 可能是从硬盘、数据库、网络流等文件中产生实例的读取器。然而，关于如何实现的相关细节已经超出了本文档的讨论范围。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-979">
<h3 class="sigil_not_in_toc1">6.1.2. 提取特征</h3>
<p class="calibre2">2. 可以是 scikit-learn 支持的的不同 :ref: <cite class="calibre13">特征提取 &lt;feature_extraction&gt;</cite> 方法中的任何相关的方法。然而，当处理那些需要矢量化并且特征或值的集合你预先不知道的时候，就得明确注意了。一个好的例子是文本分类，其中在训练的期间你很可能会发现未知的项。从应用的角度上来看，如果在数据上进行多次通过是合理的，则可以使用有状态的向量化器。否则，可以通过使用无状态特征提取器来提高难度。目前，这样做的首选方法是使用所谓的 <a class="calibre3 pcalibre" href="feature_extraction.html#feature-hashing"><span class="calibre4">哈希技巧</span></a>，在 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="docutils"><span class="calibre4">sklearn.feature_extraction.FeatureHasher</span></code></a> 中，其中有分类变量的表示为 Python 列表或 <a class="calibre3 pcalibre" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="docutils"><span class="calibre4">sklearn.feature_extraction.text.HashingVectorizer</span></code></a> 文本文档。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-980">
<h3 class="sigil_not_in_toc1">6.1.3. 增量学习</h3>
<p class="calibre2">最后，对于3. 我们在 scikit-learn 之中有许多选择。虽软不是所有的算法都能够增量学习（即不能一次性看到所有的实例），所有实 <code class="docutils"><span class="calibre4">partial_fit</span></code> 的 API 估计器都作为了候选。实际上，从小批量的实例（有时称为“在线学习”）逐渐学习的能力是外核学习的关键，因为它保证在任何给定的时间内只有少量的实例在主存储中，选择适合小批量的尺寸来平衡相关性和内存占用可能涉及一些调整 <a class="calibre3 pcalibre" href="#calibre_link-253" id="calibre_link-254">[1]</a>。</p>
<p class="calibre10">以下是针对不同任务的增量估算器列表：</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><dl class="first">
<dt class="calibre18">Classification（分类）</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="docutils"><span class="calibre4">sklearn.naive_bayes.MultinomialNB</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB" title="sklearn.naive_bayes.BernoulliNB"><code class="docutils"><span class="calibre4">sklearn.naive_bayes.BernoulliNB</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="docutils"><span class="calibre4">sklearn.linear_model.Perceptron</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="docutils"><span class="calibre4">sklearn.linear_model.SGDClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code class="docutils"><span class="calibre4">sklearn.linear_model.PassiveAggressiveClassifier</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="docutils"><span class="calibre4">sklearn.neural_network.MLPClassifier</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">Regression（回归）</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="docutils"><span class="calibre4">sklearn.linear_model.SGDRegressor</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code class="docutils"><span class="calibre4">sklearn.linear_model.PassiveAggressiveRegressor</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="docutils"><span class="calibre4">sklearn.neural_network.MLPRegressor</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">Clustering（聚类）</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="docutils"><span class="calibre4">sklearn.cluster.MiniBatchKMeans</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" title="sklearn.cluster.Birch"><code class="docutils"><span class="calibre4">sklearn.cluster.Birch</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">Decomposition / feature Extraction（分解/特征提取）</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="docutils"><span class="calibre4">sklearn.decomposition.MiniBatchDictionaryLearning</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="docutils"><span class="calibre4">sklearn.decomposition.IncrementalPCA</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="docutils"><span class="calibre4">sklearn.decomposition.LatentDirichletAllocation</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li class="toctree-l"><dl class="first">
<dt class="calibre18">Preprocessing（预处理）</dt>
<dd class="calibre19"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="docutils"><span class="calibre4">sklearn.preprocessing.StandardScaler</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" title="sklearn.preprocessing.MinMaxScaler"><code class="docutils"><span class="calibre4">sklearn.preprocessing.MinMaxScaler</span></code></a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler" title="sklearn.preprocessing.MaxAbsScaler"><code class="docutils"><span class="calibre4">sklearn.preprocessing.MaxAbsScaler</span></code></a></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>
</blockquote>
<p class="calibre10">对于分类，有一点要注意的是，虽然无状态特征提取程序可能能够应对新的/未知的属性，但增量学习者本身可能无法应对新的/未知的目标类。在这种情况下，你必须使用 <code class="docutils"><span class="calibre4">classes=</span></code> 参数将所有可能的类传递给第一个 <code class="docutils"><span class="calibre4">partial_fit</span></code> 调用。</p>
<p class="calibre10">选择合适的算法时要考虑的另一个方面是，所有这些算法随着时间的推移不会给每个样例相同的重要性。比如说， <code class="docutils"><span class="calibre4">Perceptron</span></code> 仍然对错误标签的例子是敏感的，即使经过多次的样例训练，而 <code class="docutils"><span class="calibre4">SGD*</span></code> 和 <code class="docutils"><span class="calibre4">PassiveAggressive*</span></code> 族对这些鲁棒性更好。相反，对于后面传入的数据流,算法的学习速率随着时间不断降低,后面两个算法对于那些显著差异的样本和标注正确的样本倾向于给予很少的重视。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-981">
<h3 class="sigil_not_in_toc1">6.1.4. 示例</h3>
<p class="calibre2">最后，我们有一个完整的 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="calibre4">Out-of-core classification of text documents</span></a> 文本文档的核心分类的示例。旨在为想要构建核心学习系统的人们提供一个起点，并展示上述大多数概念。</p>
<p class="calibre10">此外，它还展现了不同算法性能随着处理例子的数量的演变。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html"></a></strong></p>
<p class="calibre10">现在我们来看不同部分的计算时间，我们看到矢量化的过程比学习本身耗时还多。对于不同的算法，MultinomialNB 是耗时最多的，但通过增加其 mini-batches 的大小可以减轻开销。（练习：minibatch_size 在程序中更改为100和10000，并进行比较）。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html"></a></strong></p>
</div>
<div class="toctree-wrapper" id="calibre_link-982">
<h3 class="sigil_not_in_toc1">6.1.5. 注释</h3>
<table class="docutils1" frame="void" id="calibre_link-253" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="#calibre_link-254">[1]</a></td>
<td class="label1">根据算法，mini-batch 大小可以影响结果。SGD*，PassiveAggressive* 和离散的 NaiveBayes 是真正在线的，不受 batch 大小的影响。相反，MiniBatchKMeans 收敛速度受 batch 大小影响。此外，其内存占用可能会随 batch 大小而显着变化。</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-258">
<span id="calibre_link-983" class="calibre4"></span><h1 class="calibre5">7. 计算性能</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@曲晓峰</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />    
    </div>
<p class="calibre10">对于某些 applications （应用），estimators（估计器）的性能（主要是 prediction time （预测时间）的 latency （延迟）和 throughput （吞吐量））至关重要。考虑 training throughput （训练吞吐量）也可能是有意义的，但是在 production setup （生产设置）（通常在脱机中运行）通常是不太重要的。</p>
<p class="calibre10">我们将在这里审查您可以从不同上下文中的一些 scikit-learn estimators（估计器）预期的数量级，并提供一些 overcoming performance bottlenecks （解决性能瓶颈）的技巧和诀窍。</p>
<p class="calibre10">将 Prediction latency （预测延迟）作为进行预测所需的 elapsed time （经过时间）（例如，以 micro-seconds（微秒）为单位）进行测量。Latency （延迟）通常被认为一种分布，运营工程师通常将注意力集中在该分布的给定 percentile （百分位数）（例如 90 百分位数）上的延迟。</p>
<p class="calibre10">Prediction throughput （预测吞吐量）被定义为软件可以在给定的时间量内（例如每秒的预测）中 the number of predictions （可预测的预测数）。</p>
<p class="calibre10">performance optimization （性能优化）的一个重要方面也是它可能会损害 prediction accuracy （预测精度）。 实际上，更简单的模型（例如 linear （线性的），而不是 non-linear （非线性的），或者具有较少的参数）通常运行得更快，但并不总是能够考虑与更复杂的数据相同的确切属性。</p>
<div class="toctree-wrapper" id="calibre_link-984">
<h2 class="sigil_not_in_toc">7.1. 预测延迟</h2>
<p class="calibre2">在使用/选择机器学习工具包时可能遇到的最直接的问题之一是生产环境中可以进行预测的 latency （延迟）。</p>
<dl class="calibre10">
<dt class="calibre18">影响 prediction latency （预测延迟）的主要因素是</dt>
<dd class="calibre19"><ol class="first3">
<li class="toctree-l">Number of features（特征的数量）</li>
<li class="toctree-l">Input data representation and sparsity（输入数据的表示和稀疏性）</li>
<li class="toctree-l">Model complexity（模型复杂性）</li>
<li class="toctree-l">Feature extraction（特征提取）</li>
</ol>
</dd>
</dl>
<p class="calibre10">最后一个主要参数也是在 bulk or one-at-a-time mode （批量或执行一次的时间模式）下进行预测的可能性。</p>
<div class="toctree-wrapper" id="calibre_link-985">
<h3 class="sigil_not_in_toc1">7.1.1. 批量与原子模式</h3>
<p class="calibre2">通常，通过大量原因（branching predictability（分支可预测性）, CPU cache（CPU缓存）, linear algebra libraries optimizations（线性代数库优化）等），predictions in bulk（批量进行预测）（同时许多情况）更有效。 在这里，我们看到一些具有很少功能的设置，独立于估计器选择，bulk mode（批量模式）总是更快，而对于其中的一些，它们的数量大约是 1 到 2 个数量级:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html"><img alt="atomic_prediction_latency" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000404.jpg" class="calibre60" /></a></strong></p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html"><img alt="bulk_prediction_latency" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000662.jpg" class="calibre60" /></a></strong></p>
<p class="calibre10">为了对您的案例的不同的 estimators 进行基准测试，您可以在此示例中简单地更改 <code class="docutils"><span class="calibre4">n_features</span></code> 参数:
<a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py"><span class="calibre4">Prediction Latency</span></a>. 这应该给你估计 prediction latency （预测延迟）的数量级。</p>
<div class="toctree-wrapper">
<p class="calibre10">配置 Scikit-learn 来减少验证开销</p>
<p class="calibre10">Scikit-learn 对数据进行了一些验证，从而增加了对 <code class="docutils"><span class="calibre4">predict（预测）</span></code> 和类似函数的调用开销。特别地，检查这些 features （特征）是有限的（不是 NaN 或无限）涉及对数据的完全传递。如果您确定你的数据是 acceptable （可接受的），您可以通过在导入 scikit-learn 之前将环境变量配置 <code class="docutils"><span class="calibre4">SKLEARN_ASSUME_FINITE</span></code> 设置为 non-empty string （非空字符串）来抑制检查有限性，或者使用以下方式在 Python 中配置 <a class="calibre3 pcalibre" href="generated/sklearn.set_config.html#sklearn.set_config" title="sklearn.set_config"><code class="docutils"><span class="calibre4">sklearn.set_config</span></code></a> 。为了比这些全局设置更多的控制 <code class="docutils"><span class="calibre4">config_context</span></code> 允许您在指定的上下文中设置此配置:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">sklearn</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">with</span> <span class="calibre4">sklearn</span><span class="calibre4">.</span><span class="calibre4">config_context</span><span class="calibre4">(</span><span class="calibre4">assume_finite</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">):</span>
<span class="calibre4">... </span>   <span class="calibre4">pass</span>  <span class="calibre4"># do learning/prediction here with reduced validation</span>
</pre>
</div>
</div>
<p class="calibre10">注意，这将影响上下文中的 <a class="calibre3 pcalibre" href="generated/sklearn.utils.assert_all_finite.html#sklearn.utils.assert_all_finite" title="sklearn.utils.assert_all_finite"><code class="docutils"><span class="calibre4">sklearn.utils.assert_all_finite</span></code></a> 的所有用途。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-986">
<h3 class="sigil_not_in_toc1">7.1.2. 特征数量的影响</h3>
<p class="calibre2">显然，当特征数量增加时，每个示例的内存消耗量也会增加。实际上，对于具有 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000277.jpg" alt="N" /> 个特征的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000184.jpg" alt="M" /> 个实例的矩阵，空间复杂度在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000581.jpg" alt="O(NM)" /> 。</p>
<p class="calibre10">从 computing （计算）角度来看，这也意味着 the number of basic operations （基本操作的数量）（例如，线性模型中向量矩阵乘积的乘法）也增加。以下是 prediction latency (预测延迟)与 number of features(特征数) 的变化图:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html"><img alt="influence_of_n_features_on_latency" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000565.jpg" class="calibre60" /></a></strong></p>
<p class="calibre10">总的来说，您可以预期 prediction time （预测时间）至少随 number of features （特征数量）线性增加（非线性情况可能会发生，取决于 global memory footprint （全局内存占用）和 estimator （估计））。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-987">
<h3 class="sigil_not_in_toc1">7.1.3. 输入数据表示的影响</h3>
<p class="calibre2">Scipy 提供对 storing sparse data（存储稀疏数据）进行优化的 sparse matrix （稀疏矩阵）数据结构。sparse formats（稀疏格式）的主要特点是您不会存储零，所以如果您的数据稀疏，那么您使用的内存会更少。sparse（稀疏） (<a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/scipy/reference/sparse.html">CSR or CSC</a>) 表示中的非零值将仅占用一个 32 位整数位置 + 64 位 floating point （浮点值） + 矩阵中每行或列的额外的 32 位。在 dense（密集） (or sparse（稀疏）) 线性模型上使用稀疏输入可以加速预测，只有非零值特征才会影响点积，从而影响模型预测。因此，如果在 1e6 维空间中有 100 个非零，则只需要 100 次乘法和加法运算而不是 1e6 。</p>
<p class="calibre10">然而，密度表示的计算可以利用 BLAS 中高度优化的向量操作和多线程，并且往往导致更少的 CPU 高速缓存 misses 。因此，sparse input （稀疏输入）表示的 sparsity （稀疏度）通常应相当高（10% 非零最大值，要根据硬件进行检查）比在具有多个 CPU 和优化 BLAS 实现的机器上的 dense input （密集输入）表示更快。</p>
<p class="calibre10">以下是测试输入 sparsity （稀疏度）的示例代码:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">def</span> <span class="calibre4">sparsity_ratio</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
    <span class="calibre4">return</span> <span class="calibre4">1.0</span> <span class="calibre4">-</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">count_nonzero</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span> <span class="calibre4">/</span> <span class="calibre4">float</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span> <span class="calibre4">*</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"input sparsity ratio:"</span><span class="calibre4">,</span> <span class="calibre4">sparsity_ratio</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">))</span>
</pre>
</div>
</div>
<p class="calibre10">根据经验，您可以考虑如果 sparsity ratio （稀疏比）大于 90% , 您可能会从 sparse formats （稀疏格式）中受益。有关如何构建（或将数据转换为） sparse matrix formats （稀疏矩阵格式）的更多信息，请参阅 Scipy 的稀疏矩阵格式文档 <a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/scipy/reference/sparse.html">documentation</a> 。大多数的时候, <code class="docutils"><span class="calibre4">CSR</span></code> 和 <code class="docutils"><span class="calibre4">CSC</span></code> 格式是最有效的。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-988">
<h3 class="sigil_not_in_toc1">7.1.4. 模型复杂度的影响</h3>
<p class="calibre2">一般来说，当 model complexity （模型复杂度）增加时，predictive power （预测能力）和 latency （延迟）应该会增加。增加 predictive power （预测能力）通常很有意思，但对于许多应用，我们最好不要太多地增加预测延迟。我们现在将对不同 families 的 supervised models （监督模式）进行审查。</p>
<p class="calibre10">对于 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.linear_model" title="sklearn.linear_model"><code class="docutils"><span class="calibre4">sklearn.linear_model</span></code></a> (例如 Lasso, ElasticNet, SGDClassifier/Regressor, Ridge &amp; RidgeClassifier, PassiveAgressiveClassifier/Regressor, LinearSVC, LogisticRegression…) 在预测时间应用的 decision function （决策函数）是一样的（dot product（ 点积）），所以 latency （延迟）应该是等效的。</p>
<p class="calibre10">这里有一个例子使用 <code class="docutils"><span class="calibre4">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code> 和 <code class="docutils"><span class="calibre4">elasticnet</span></code> penalty（惩罚）。 regularization strength（正则化强度）由 <code class="docutils"><span class="calibre4">alpha</span></code> 参数全局控制。有一个足够高的 <code class="docutils"><span class="calibre4">alpha</span></code> ，可以增加 <code class="docutils"><span class="calibre4">elasticnet</span></code> 的 <code class="docutils"><span class="calibre4">l1_ratio</span></code> 参数，以在模型参数中执行各种稀疏程度。这里的 Higher sparsity （较高稀疏度）被解释为 less model complexity （较少的模型复杂度），因为我们需要较少的系数充分描述它。当然， sparsity （稀疏性）会随着稀疏点积 产生时间大致与非零系数的数目成比例地影响 prediction time （预测时间）。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_model_complexity_influence.html"></a></strong></p>
<p class="calibre10">对于具有 non-linear kernel （非线性内核）的 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.svm" title="sklearn.svm"><code class="docutils"><span class="calibre4">sklearn.svm</span></code></a> 算法系列，latency （延迟）与 support vectors （支持向量）的数量有关（越少越快）。
随着 SVC 或 SVR 模型中的支持向量的数量， Latency （延迟）和 throughput （吞吐量）应该渐渐地增长。kernel （内核）也将影响 latency （延迟），因为它用于计算每个 support vector （支持向量）一次 input vector（输入向量）的 projection （投影）。在下面的图中， <code class="docutils"><span class="calibre4">sklearn.svm.classes.NuSVR</span></code> 的 <code class="docutils"><span class="calibre4">nu</span></code> 参数用于影响 number of support vectors（支持向量的数量）。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_model_complexity_influence.html"></a></strong></p>
<p class="calibre10">对于 <a class="calibre3 pcalibre" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="docutils"><span class="calibre4">sklearn.ensemble</span></code></a> 的 trees （例如 RandomForest, GBT, ExtraTrees 等） number of trees （树的数量）及其 depth（深度）发挥着最重要的作用。Latency and throughput（延迟和吞吐量）应与树的数量呈线性关系。在这种情况下，我们直接使用 <code class="docutils"><span class="calibre4">sklearn.ensemble.gradient_boosting.GradientBoostingRegressor</span></code> 的 <code class="docutils"><span class="calibre4">n_estimators</span></code> 参数。</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_model_complexity_influence.html"></a></strong></p>
<p class="calibre10">在任何情况下都应该警告，降低的 model complexity （模型复杂性）可能会损害如上所述的准确性。例如，可以用快速线性模型来处理 non-linearly separable problem （非线性可分离问题），但是在该过程中预测能力将很可能受到影响。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-989">
<h3 class="sigil_not_in_toc1">7.1.5. 特征提取延迟</h3>
<p class="calibre2">大多数 scikit-learn 模型通常非常快，因为它们可以通过编译的 Cython 扩展或优化的计算库来实现。
另一方面，在许多现实世界的应用中，feature extraction process（特征提取过程）（即，将 database rows or network packets （数据库行或网络分组）的原始数据转换为 numpy arrays ）来控制总体预测时间。例如在 Reuters text classification task（路透社文本分类任务）中，根据所选择的模型，整个准备（读取和解析 SGML 文件，将文本进行标记并将其散列为公共向量空间）的时间比实际预测代码的时间长 100 到 500 倍。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"></div>
</blockquote>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_out_of_core_classification.html"></a></strong></p>
<p class="calibre10">因此，在很多情况下，建议您仔细地对 carefully time and profile your feature extraction code ( 特征提取代码进行时间预估和简档)，因为当您的 overall latency （整体延迟）对您的应用程序来说太慢时，可能是开始优化的好地方。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-990">
<h2 class="sigil_not_in_toc">7.2. 预测吞吐量</h2>
<p class="calibre2">考虑到生产系统大小的另一个重要指标是 throughput （吞吐量），即在一定时间内可以做出的预测数量。以下是 <a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py"><span class="calibre4">Prediction Latency</span></a> 示例的基准测试，该示例针对合成数据的多个 estimators （估计器）测量此数量:</p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../auto_examples/applications/plot_prediction_latency.html"><img alt="throughput_benchmark" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000401.jpg" class="calibre60" /></a></strong></p>
<p class="calibre10">这些 throughputs（吞吐量）早单个进程上实现。提高应用程序吞吐量的一个明显的方法是产生其他实例（通常是 Python 中的进程，因为 <a class="calibre3 pcalibre" href="https://wiki.python.org/moin/GlobalInterpreterLock">GIL</a> ）共享相同模型。还可能添加机器来分布式负载。关于如何实现这一点的详细解释超出了本文档的范围。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-991">
<h2 class="sigil_not_in_toc">7.3. 技巧和窍门</h2>
<div class="toctree-wrapper" id="calibre_link-992">
<h3 class="sigil_not_in_toc1">7.3.1. 线性代数库</h3>
<p class="calibre2">由于 scikit-learn 在很大程度上依赖于 Numpy/Scipy 和 线性代数，所以需要理解这些库的版本。
基本上，你应该确保使用优化的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a> / <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/LAPACK">LAPACK</a> 构建 Numpy 库。</p>
<p class="calibre10">并非所有的模型都受益于优化的 BLAS 和 Lapack 实现。例如，基于（随机化）决策树的模型通常不依赖于内部循环中的 BLAS 调用，kernel SVMs (<code class="docutils"><span class="calibre4">SVC</span></code>, <code class="docutils"><span class="calibre4">SVR</span></code>, <code class="docutils"><span class="calibre4">NuSVC</span></code>, <code class="docutils"><span class="calibre4">NuSVR</span></code>) 。另一方面，使用 BLAS DGEMM 调用（通过 <code class="docutils"><span class="calibre4">numpy.dot</span></code>）实现的线性模型通常将受益于调整的 BLAS 实现，并且导致非优化 BLAS 的数量级加速。</p>
<p class="calibre10">你可以使用以下命令显示您的 NumPy / SciPy / scikit-learn 安装使用的 BLAS / LAPACK 实现:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">from</span> <span class="calibre4">numpy.distutils.system_info</span> <span class="calibre4">import</span> <span class="calibre4">get_info</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">get_info</span><span class="calibre4">(</span><span class="calibre4">'blas_opt'</span><span class="calibre4">))</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">get_info</span><span class="calibre4">(</span><span class="calibre4">'lapack_opt'</span><span class="calibre4">))</span>
</pre>
</div>
</div>
<dl class="calibre10">
<dt class="calibre18">Optimized(优化的) BLAS / LAPACK 实现包括:</dt>
<dd class="calibre19"><ul class="first2">
<li class="toctree-l">Atlas (需要通过在目标机器上 rebuilding 进行硬件特定调整)</li>
<li class="toctree-l">OpenBLAS</li>
<li class="toctree-l">MKL</li>
<li class="toctree-l">Apple Accelerate 和 vecLib frameworks (仅适用于 OSX)</li>
</ul>
</dd>
</dl>
<p class="calibre10">有关更多信息，请参见 <a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/numpy/user/install.html">Scipy install page</a> 并在来自 Daniel Nouri 的博客  <a class="calibre3 pcalibre" href="http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/">blog post</a> 它为 Debain / Ubuntu 提供了一些很好的一步一步的安装说明。</p>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">Multithreaded BLAS libraries（多线程 BLAS 库）有时与 Python 的 <code class="docutils"><span class="calibre4">multiprocessing</span></code> 模块相冲突，这个模块由例如 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 和大多数其他估计器，它们使用 <code class="docutils"><span class="calibre4">n_jobs</span></code> 参数（除了 <code class="docutils"><span class="calibre4">SGDClassifier</span></code>, <code class="docutils"><span class="calibre4">SGDRegressor</span></code>, <code class="docutils"><span class="calibre4">Perceptron</span></code>, <code class="docutils"><span class="calibre4">PassiveAggressiveClassifier</span></code> 和 基于树的方法如 random forests（随机森林））。在 OpenMP 支持的情况下， Apple 的 Accelerate 和 OpenBLAS 也是如此。</p>
<p class="calibre10">除了 scikit-learn, Numpy 和 Scipy 也在内部使用 BLAS, 如上所述。</p>
<p class="calibre10">如果您遇到带有 <code class="docutils"><span class="calibre4">n_jobs&gt;1</span></code> 或 <code class="docutils"><span class="calibre4">n_jobs=-1</span></code> 的 hanging subprocesses （挂起子进程），请确保你有一个单线程 BLAS 库，或者设置 <code class="docutils"><span class="calibre4">n_jobs=1</span></code> 或者升级到 Python 3.4 有一个新版本的 <code class="docutils"><span class="calibre4">multiprocessing</span></code> ，应该免于这个问题。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-993">
<h3 class="sigil_not_in_toc1">7.3.2. 模型压缩</h3>
<p class="calibre2">scikit-learn 中的 Model compression （模型压缩）只关注 linear models （线性模型）。
在这种情况下，这意味着我们要控制模型 sparsity （稀疏度）（即 模型向量中的非零坐标数）。将 model sparsity （模型稀疏度）与 sparse input data representation （稀疏输入数据表示）相结合是一个好主意。</p>
<p class="calibre10">以下是示例代码，说明了如何使用 <code class="docutils"><span class="calibre4">sparsify()</span></code> 方法:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SGDRegressor</span><span class="calibre4">(</span><span class="calibre4">penalty</span><span class="calibre4">=</span><span class="calibre4">'elasticnet'</span><span class="calibre4">,</span> <span class="calibre4">l1_ratio</span><span class="calibre4">=</span><span class="calibre4">0.25</span><span class="calibre4">)</span>
<span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">sparsify</span><span class="calibre4">()</span>
<span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">在这个例子中，我们更喜欢 <code class="docutils"><span class="calibre4">elasticnet</span></code> penalty（惩罚），因为它通常是 model compactness（模型紧凑性）和 prediction power （预测能力）之间的一个很好的妥协。还可以进一步调整 <code class="docutils"><span class="calibre4">l1_ratio</span></code> 参数（结合正则化强度 <code class="docutils"><span class="calibre4">alpha</span></code> ）来控制这个权衡。</p>
<p class="calibre10">对于 synthetic data （合成数据），典型的 <a class="calibre3 pcalibre" href="https://github.com/scikit-learn/scikit-learn/blob/master/benchmarks/bench_sparsify.py">benchmark</a> 在模型和输入时都会降低 30% 的延迟。稀疏（分别为 0.000024 和 0.027400 非零系数比）。您的里程可能会因您的数据和模型的稀疏性和大小而有所不同。
因此，为了减少部署在生产服务器上的预测模型的内存使用，扩展可能非常有用。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-994">
<h3 class="sigil_not_in_toc1">7.3.3. 模型重塑</h3>
<p class="calibre2">Model reshaping（模型重塑）在于仅选择一部分可用功能以适应模型。换句话说，如果模型在学习阶段 discards features （丢弃特征），我们可以从输入中删除这些特征。这有几个好处。首先，它减少了模型本身的内存（因此是减少了时间）的开销。一旦知道要从上一次运行中保留哪些功能，它也允许在 pipeline 中 discard explicit feature selection components （丢弃显式的特征选择组件）。最后，它可以通过不收集和构建模型丢弃的特征来帮助减少数据访问和 feature extraction layers （特征提取层）upstream （上游）的处理时间和 I/O 的使用。例如，如果原始数据来自数据库，则可以通过使查询返回较轻的记录，从而可以编写更简单和更快速的查询或减少 I/O 的使用。
目前，reshaping（重塑）需要在 scikit-learn 中手动执行。
在 sparse input（稀疏输入）（特别是 <code class="docutils"><span class="calibre4">CSR</span></code> 格式）的情况下，通常不能生成相关的特征，使其列为空。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-995">
<h3 class="sigil_not_in_toc1">7.3.4. 链接</h3>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../developers/performance.html">scikit-learn developer performance documentation</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://docs.scipy.org/doc/scipy/reference/sparse.html">Scipy sparse matrix formats documentation</a></li>
</ul>
</div>
</blockquote>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-230">
<h1 class="calibre1">教程</h1>
</div>


<div class="calibre" id="calibre_link-52">
<span id="calibre_link-996" class="calibre4"></span><h1 class="calibre5">使用 scikit-learn 介绍机器学习</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@李昊伟</a><br class="calibre9" />  
    </div>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/hlxstc" class="calibre3 pcalibre">@hlxstc</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@BWM-蜜蜂</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@小瑶</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@...</a><br class="calibre9" />   
    </div>
<div class="toctree-wrapper">
<p class="calibre10">内容提要</p>
<p class="calibre10">在本节中，我们介绍一些在使用 scikit-learn 过程中用到的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Machine_learning">机器学习</a> 词汇，并且给出一些例子阐释它们。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-997">
<h2 class="sigil_not_in_toc">机器学习：问题设置</h2>
<p class="calibre2">一般来说，一个学习问题通常会考虑一系列 n 个 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Sample_(statistics)">样本</a> 数据，然后尝试预测未知数据的属性。
如果每个样本是 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Multivariate_random_variable">多个属性的数据</a> （比如说是一个多维记录），就说它有许多“属性”，或称 <strong class="calibre14">features(特征)</strong> 。</p>
<p class="calibre10">我们可以将学习问题分为几大类:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Supervised_learning">监督学习</a> ,
其中数据带有一个附加属性，即我们想要预测的结果值（ <a class="calibre3 pcalibre" href="../../supervised_learning.html#supervised-learning"><span class="calibre4">点击此处</span></a> 转到 scikit-learn 监督学习页面）。这个问题可以是:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre7">
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Classification_in_machine_learning">分类</a> :
样本属于两个或更多个类，我们想从已经标记的数据中学习如何预测未标记数据的类别。
分类问题的一个例子是手写数字识别，其目的是将每个输入向量分配给有限数目的离散类别之一。
我们通常把分类视作监督学习的一个离散形式（区别于连续形式），从有限的类别中，给每个样本贴上正确的标签。</li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Regression_analysis">回归</a> :
如果期望的输出由一个或多个连续变量组成，则该任务称为 <em class="calibre13">回归</em> 。
回归问题的一个例子是预测鲑鱼的长度是其年龄和体重的函数。</li>
</ul>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Unsupervised_learning">无监督学习</a>,
其中训练数据由没有任何相应目标值的一组输入向量x组成。这种问题的目标可能是在数据中发现彼此类似的示例所聚成的组，这种问题称为 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Cluster_analysis">聚类</a> ,
或者，确定输入空间内的数据分布，称为 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Density_estimation">密度估计</a> ，又或从高维数据投影数据空间缩小到二维或三维以进行 <em class="calibre13">可视化</em> （<a class="calibre3 pcalibre" href="../../unsupervised_learning.html#unsupervised-learning"><span class="calibre4">点击此处</span></a> 转到 scikit-learn 无监督学习页面）。</p>
</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper">
<p class="calibre10">训练集和测试集</p>
<p class="calibre10">机器学习是从数据的属性中学习，并将它们应用到新数据的过程。
这就是为什么机器学习中评估算法的普遍实践是把数据分割成 <strong class="calibre14">训练集</strong> （我们从中学习数据的属性）和 <strong class="calibre14">测试集</strong> （我们测试这些性质）。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-998">
<span id="calibre_link-999" class="calibre4"></span><h2 class="sigil_not_in_toc">加载示例数据集</h2>
<p class="calibre2"><cite class="calibre13">scikit-learn</cite> 提供了一些标准数据集，例如 用于分类的 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a>
和 <a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits">digits</a> 数据集
和 <a class="calibre3 pcalibre" href="http://archive.ics.uci.edu/ml/datasets/Housing">波士顿房价回归数据集</a> .</p>
<p class="calibre10">在下文中，我们从我们的 shell 启动一个 Python 解释器，然后加载 <code class="docutils"><span class="calibre4">iris</span></code> 和 <code class="docutils"><span class="calibre4">digits</span></code> 数据集。我们的符号约定是 <code class="docutils"><span class="calibre4">$</span></code> 表示 shell 提示符，而 <code class="docutils"><span class="calibre4">&gt;&gt;&gt;</span></code> 表示 Python 解释器提示符:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>$ python
&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; digits = datasets.load_digits()
</pre>
</div>
</div>
<p class="calibre10">数据集是一个类似字典的对象，它保存有关数据的所有数据和一些元数据。 该数据存储在 <code class="docutils"><span class="calibre4">.data</span></code> 成员中，它是 <code class="docutils"><span class="calibre4">n_samples,</span> <span class="calibre4">n_features</span></code> 数组。
在监督问题的情况下，一个或多个响应变量存储在 <code class="docutils"><span class="calibre4">.target</span></code> 成员中。 有关不同数据集的更多详细信息，请参见 <a class="calibre3 pcalibre" href="../../datasets/index.html#datasets"><span class="calibre4">专用数据集部分</span></a> 。</p>
<p class="calibre10">例如，在数字数据集的情况下，<code class="docutils"><span class="calibre4">digits.data</span></code> 使我们能够得到一些用于分类的样本特征:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>  
<span class="calibre4">[[  0.   0.   5. ...,   0.   0.   0.]</span>
<span class="calibre4"> [  0.   0.   0. ...,  10.   0.   0.]</span>
<span class="calibre4"> [  0.   0.   0. ...,  16.   9.   0.]</span>
<span class="calibre4"> ...,</span>
<span class="calibre4"> [  0.   0.   1. ...,   6.   0.   0.]</span>
<span class="calibre4"> [  0.   0.   2. ...,  12.   0.   0.]</span>
<span class="calibre4"> [  0.   0.  10. ...,  12.   1.   0.]]</span>
</pre>
</div>
</div>
<p class="calibre10">并且 <code class="docutils"><span class="calibre4">digits.target</span></code> 表示了数据集内每个数字的真实类别，也就是我们期望从每个手写数字图像中学得的相应的数字标记:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">array([0, 1, 2, ..., 8, 9, 8])</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">数据数组的形状</p>
<p class="calibre10">数据总是二维数组，形状 <code class="docutils"><span class="calibre4">(n_samples,</span> <span class="calibre4">n_features)</span></code> ，尽管原始数据可能具有不同的形状。
在数字的情况下，每个原始样本是形状 <code class="docutils"><span class="calibre4">(8,</span> <span class="calibre4">8)</span></code> 的图像，可以使用以下方式访问:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">array([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],</span>
<span class="calibre4">       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],</span>
<span class="calibre4">       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],</span>
<span class="calibre4">       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],</span>
<span class="calibre4">       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],</span>
<span class="calibre4">       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],</span>
<span class="calibre4">       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],</span>
<span class="calibre4">       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])</span>
</pre>
</div>
</div>
<p class="calibre10">该 <a class="calibre3 pcalibre" href="../../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="calibre4">数据集上的简单示例</span></a> 说明了如何从原始数据开始调整，形成可以在 scikit-learn 中使用的数据。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">从外部数据集加载</p>
<p class="calibre10">要从外部数据集加载，请参阅 <a class="calibre3 pcalibre" href="../../datasets/index.html#external-datasets"><span class="calibre4">加载外部数据集</span></a> 。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1000">
<h2 class="sigil_not_in_toc">学习和预测</h2>
<p class="calibre2">在数字数据集的情况下，任务是给出图像来预测其表示的数字。
我们给出了 10 个可能类（数字 0 到 9）中的每一个的样本，我们在这些类上 <em class="calibre13">拟合</em> 一个 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Estimator">估计器</a> ，以便能够 <em class="calibre13">预测</em> 未知的样本所属的类。</p>
<p class="calibre10">在 scikit-learn 中，分类的估计器是一个 Python 对象，它实现了 <code class="docutils"><span class="calibre4">fit(X,</span> <span class="calibre4">y)</span></code> 和 <code class="docutils"><span class="calibre4">predict(T)</span></code> 等方法。</p>
<p class="calibre10">估计器的一个例子类 <code class="docutils"><span class="calibre4">sklearn.svm.SVC</span></code> ，实现了 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量分类</a> 。 估计器的构造函数以相应模型的参数为参数，但目前我们将把估计器视为黑箱即可:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">gamma</span><span class="calibre4">=</span><span class="calibre4">0.001</span><span class="calibre4">,</span> <span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">100.</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">选择模型的参数</p>
<p class="calibre10">在这个例子中，我们手动设置 <code class="docutils"><span class="calibre4">gamma</span></code> 值。不过，通过使用 <a class="calibre3 pcalibre" href="../../modules/grid_search.html#grid-search"><span class="calibre4">网格搜索</span></a> 及 <a class="calibre3 pcalibre" href="../../modules/cross_validation.html#cross-validation"><span class="calibre4">交叉验证</span></a> 等工具，可以自动找到参数的良好值。</p>
</div>
<p class="calibre10">我们把我们的估计器实例命名为 <code class="docutils"><span class="calibre4">clf</span></code> ，因为它是一个分类器（classifier）。我们需要它适应模型，也就是说，要它从模型中 <em class="calibre13">学习</em> 。
这是通过将我们的训练集传递给 <code class="docutils"><span class="calibre4">fit</span></code> 方法来完成的。作为一个训练集，让我们使用数据集中除最后一张以外的所有图像。
我们用 <code class="docutils"><span class="calibre4">[:-1]</span></code> Python 语法选择这个训练集，它产生一个包含 <code class="docutils"><span class="calibre4">digits.data</span></code> 中除最后一个条目（entry）之外的所有条目的新数组</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">])</span>  
<span class="calibre4">SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>
</pre>
</div>
</div>
<p class="calibre10">现在你可以预测新的值，特别是我们可以向分类器询问 <code class="docutils"><span class="calibre4">digits</span></code> 数据集中最后一个图像（没有用来训练的一条实例）的数字是什么:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">:])</span>
<span class="calibre4">array([8])</span>
</pre>
</div>
</div>
<p class="calibre10">相应的图像如下:</p>
<a class="calibre3 pcalibre" href="../../auto_examples/datasets/plot_digits_last_image.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_last_image_001.png" class="align-center3" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000088.jpg" /></a>
<p class="calibre10">正如你所看到的，这是一项具有挑战性的任务：图像分辨率差。你是否认同这个分类？</p>
<p class="calibre10">这个分类问题的一个完整例子可以作为一个例子来运行和学习： 识别手写数字。
<a class="calibre3 pcalibre" href="../../auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py"><span class="calibre4">Recognizing hand-written digits</span></a>.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1001">
<h2 class="sigil_not_in_toc">模型持久化</h2>
<p class="calibre2">可以通过使用 Python 的内置持久化模块（即 <a class="calibre3 pcalibre" href="https://docs.python.org/2/library/pickle.html">pickle</a> ）将模型保存:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">pickle</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">s</span> <span class="calibre4">=</span> <span class="calibre4">pickle</span><span class="calibre4">.</span><span class="calibre4">dumps</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span> <span class="calibre4">=</span> <span class="calibre4">pickle</span><span class="calibre4">.</span><span class="calibre4">loads</span><span class="calibre4">(</span><span class="calibre4">s</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf2</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">:</span><span class="calibre4">1</span><span class="calibre4">])</span>
<span class="calibre4">array([0])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>
<span class="calibre4">0</span>
</pre>
</div>
</div>
<p class="calibre10">在scikit的具体情况下，使用 joblib 替换 pickle（ <code class="docutils"><span class="calibre4">joblib.dump</span></code> &amp; <code class="docutils"><span class="calibre4">joblib.load</span></code> ）可能会更有趣，这对大数据更有效，但只能序列化 (pickle) 到磁盘而不是字符串变量:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.externals</span> <span class="calibre4">import</span> <span class="calibre4">joblib</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">joblib</span><span class="calibre4">.</span><span class="calibre4">dump</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">'filename.pkl'</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<p class="calibre10">之后，您可以加载已保存的模型（可能在另一个 Python 进程中）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">joblib</span><span class="calibre4">.</span><span class="calibre4">load</span><span class="calibre4">(</span><span class="calibre4">'filename.pkl'</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">joblib.dump</span></code> 以及 <code class="docutils"><span class="calibre4">joblib.load</span></code> 函数也接受 file-like（类文件） 对象而不是文件名。有关 Joblib 的数据持久化的更多信息，请 <a class="calibre3 pcalibre" href="https://pythonhosted.org/joblib/persistence.html">点击此处</a> 。</p>
</div>
<p class="calibre10">请注意，pickle 有一些安全性和维护性问题。有关使用 scikit-learn 的模型持久化的更多详细信息，请参阅 <a class="calibre3 pcalibre" href="../../modules/model_persistence.html#model-persistence"><span class="calibre4">模型持久化</span></a> 部分。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1002">
<h2 class="sigil_not_in_toc">规定</h2>
<p class="calibre2">scikit-learn 估计器遵循某些规则，使其行为更可预测。</p>
<div class="toctree-wrapper" id="calibre_link-1003">
<h3 class="sigil_not_in_toc1">类型转换</h3>
<p class="calibre2">除非特别指定，输入将被转换为 <code class="docutils"><span class="calibre4">float64</span></code></p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">random_projection</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rng</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">RandomState</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">rng</span><span class="calibre4">.</span><span class="calibre4">rand</span><span class="calibre4">(</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">2000</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">dtype</span><span class="calibre4">=</span><span class="calibre4">'float32'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">dtype</span>
<span class="calibre4">dtype('float32')</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">transformer</span> <span class="calibre4">=</span> <span class="calibre4">random_projection</span><span class="calibre4">.</span><span class="calibre4">GaussianRandomProjection</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span> <span class="calibre4">=</span> <span class="calibre4">transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new</span><span class="calibre4">.</span><span class="calibre4">dtype</span>
<span class="calibre4">dtype('float64')</span>
</pre>
</div>
</div>
<p class="calibre10">在这个例子中，<code class="docutils"><span class="calibre4">X</span></code> 原本是 <code class="docutils"><span class="calibre4">float32</span></code> ，被 <code class="docutils"><span class="calibre4">fit_transform(X)</span></code> 转换成 <code class="docutils"><span class="calibre4">float64</span></code> 。</p>
<p class="calibre10">回归目标被转换为 <code class="docutils"><span class="calibre4">float64</span></code> ，但分类目标维持不变:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">3</span><span class="calibre4">]))</span>
<span class="calibre4">[0, 0, 0]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">])</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">3</span><span class="calibre4">]))</span>  
<span class="calibre4">['setosa', 'setosa', 'setosa']</span>
</pre>
</div>
</div>
<p class="calibre10">这里，第一个 <code class="docutils"><span class="calibre4">predict()</span></code> 返回一个整数数组，因为在 <code class="docutils"><span class="calibre4">fit</span></code> 中使用了 <code class="docutils"><span class="calibre4">iris.target</span></code> （一个整数数组）。
第二个 <code class="docutils"><span class="calibre4">predict()</span></code> 返回一个字符串数组，因为 <code class="docutils"><span class="calibre4">iris.target_names</span></code> 是一个字符串数组。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1004">
<h3 class="sigil_not_in_toc1">再次训练和更新参数</h3>
<p class="calibre2">估计器的超参数可以通过 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.set_params" title="sklearn.pipeline.Pipeline.set_params"><code class="docutils"><span class="calibre4">sklearn.pipeline.Pipeline.set_params</span></code></a> 方法在实例化之后进行更新。
调用 <code class="docutils"><span class="calibre4">fit()</span></code> 多次将覆盖以前的 <code class="docutils"><span class="calibre4">fit()</span></code> 所学到的参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">rng</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">RandomState</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">rng</span><span class="calibre4">.</span><span class="calibre4">rand</span><span class="calibre4">(</span><span class="calibre4">100</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">rng</span><span class="calibre4">.</span><span class="calibre4">binomial</span><span class="calibre4">(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">100</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_test</span> <span class="calibre4">=</span> <span class="calibre4">rng</span><span class="calibre4">.</span><span class="calibre4">rand</span><span class="calibre4">(</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">SVC</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">array([1, 0, 1, 1, 0])</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>  
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',</span>
<span class="calibre4">  max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">  tol=0.001, verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 0, 0, 1, 0])</span>
</pre>
</div>
</div>
<p class="calibre10">在这里，估计器被 <code class="docutils"><span class="calibre4">SVC()</span></code> 构造之后，默认内核 <code class="docutils"><span class="calibre4">rbf</span></code> 首先被改变到 <code class="docutils"><span class="calibre4">linear</span></code> ，然后改回到 <code class="docutils"><span class="calibre4">rbf</span></code> 重新训练估计器并进行第二次预测。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1005">
<h3 class="sigil_not_in_toc1">多分类与多标签拟合</h3>
<p class="calibre2">当使用 <a class="calibre3 pcalibre" href="../../modules/classes.html#module-sklearn.multiclass" title="sklearn.multiclass"><code class="docutils"><span class="calibre4">多类分类器</span></code></a> 时，执行的学习和预测任务取决于参与训练的目标数据的格式:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.multiclass</span> <span class="calibre4">import</span> <span class="calibre4">OneVsRestClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">LabelBinarizer</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">5</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">classif</span> <span class="calibre4">=</span> <span class="calibre4">OneVsRestClassifier</span><span class="calibre4">(</span><span class="calibre4">estimator</span><span class="calibre4">=</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">classif</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 0, 1, 1, 2])</span>
</pre>
</div>
</div>
<p class="calibre10">在上述情况下，分类器使用含有多个标签的一维数组训练模型，因此 <code class="docutils"><span class="calibre4">predict()</span></code> 方法可提供相应的多标签预测。分类器也可以通过标签二值化后的二维数组来训练:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">LabelBinarizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">classif</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array([[1, 0, 0],</span>
<span class="calibre4">       [1, 0, 0],</span>
<span class="calibre4">       [0, 1, 0],</span>
<span class="calibre4">       [0, 0, 0],</span>
<span class="calibre4">       [0, 0, 0]])</span>
</pre>
</div>
</div>
<p class="calibre10">这里，使用 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.preprocessing.LabelBinarizer.html#sklearn.preprocessing.LabelBinarizer" title="sklearn.preprocessing.LabelBinarizer"><code class="docutils"><span class="calibre4">LabelBinarizer</span></code></a> 将目标向量 y 转化成二值化后的二维数组。在这种情况下， <code class="docutils"><span class="calibre4">predict()</span></code> 返回一个多标签预测相应的 二维 数组。</p>
<p class="calibre10">请注意，第四个和第五个实例返回全零向量，表明它们不能匹配用来训练中的目标标签中的任意一个。使用多标签输出，类似地可以为一个实例分配多个标签:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;</span> <span class="calibre4">from</span> <span class="calibre4">sklearn.preprocessing</span> <span class="calibre4">import</span> <span class="calibre4">MultiLabelBinarizer</span>
<span class="calibre4">&gt;&gt;</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;</span> <span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">MultiLabelBinarizer</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;</span> <span class="calibre4">classif</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
       <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
       <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
       <span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">],</span>
       <span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]])</span>
</pre>
</div>
</div>
<p class="calibre10">在这种情况下，用来训练分类器的多个向量被赋予多个标记， <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html#sklearn.preprocessing.MultiLabelBinarizer" title="sklearn.preprocessing.MultiLabelBinarizer"><code class="docutils"><span class="calibre4">MultiLabelBinarizer</span></code></a> 用来二值化多个标签产生二维数组并用来训练。
<code class="docutils"><span class="calibre4">predict()</span></code> 函数返回带有多个标签的二维数组作为每个实例的结果。</p>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-93">
<span id="calibre_link-1006" class="calibre4"></span><h1 class="calibre5">关于科学数据处理的统计学习教程</h1>
<div class="toctree-wrapper">
<p class="calibre10">Statistical learning</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a> is
a technique with a growing importance, as the
size of the datasets experimental sciences are facing is rapidly
growing. Problems it tackles range from building a prediction function
linking different observations, to classifying observations, or
learning the structure in an unlabeled dataset.</p>
<p class="calibre10">This tutorial will explore <em class="calibre13">statistical learning</em>, the use of
machine learning techniques with the goal of <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Statistical_inference">statistical inference</a>:
drawing conclusions on the data at hand.</p>
<p class="calibre10">Scikit-learn is a Python module integrating classic machine
learning algorithms in the tightly-knit world of scientific Python
packages (<a class="calibre3 pcalibre" href="http://www.scipy.org">NumPy</a>, <a class="calibre3 pcalibre" href="http://www.scipy.org">SciPy</a>, <a class="calibre3 pcalibre" href="http://matplotlib.org">matplotlib</a>).</p>
</div>
<div class="toctree-wrapper">
<ul class="calibre62">
<li class="toctree-l1"><a class="reference pcalibre" href="settings.html">机器学习: scikit-learn 中的设置以及预估对象</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="settings.html#id1">数据集</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="settings.html#id2">预估对象</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference pcalibre" href="supervised_learning.html">监督学习：从高维观察预测输出变量</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="supervised_learning.html#id2">最近邻和维度惩罚</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="supervised_learning.html#id6">线性模型：从回归到稀疏</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="supervised_learning.html#svms">支持向量积(SVMs)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference pcalibre" href="model_selection.html">模型选择：选择估计量及其参数</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="model_selection.html#id2">分数和交叉验证分数</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="model_selection.html#cv-generators-tut">交叉验证生成器</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="model_selection.html#id4">网格搜索和交叉验证估计量</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference pcalibre" href="unsupervised_learning.html">无监督学习: 寻求数据表示</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="unsupervised_learning.html#id2">聚类: 对样本数据进行分组</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="unsupervised_learning.html#id6">分解: 将一个信号转换成多个成份并且加载</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference pcalibre" href="putting_together.html">把它们放在一起</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="putting_together.html#id2">模型管道化</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="putting_together.html#id3">用特征面进行人脸识别</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="putting_together.html#id4">开放性问题: 股票市场结构</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference pcalibre" href="finding_help.html">寻求帮助</a><ul class="calibre63">
<li class="toctree-l2"><a class="calibre3 pcalibre" href="finding_help.html#id2">项目邮件列表</a></li>
<li class="toctree-l2"><a class="calibre3 pcalibre" href="finding_help.html#q-a">机器学习从业者的 Q&amp;A 社区</a></li>
</ul>
</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-92">
<h1 class="calibre1">机器学习: scikit-learn 中的设置以及预估对象</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Kyrie</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@冰块</a><br class="calibre9" />   
    </div>
<div class="toctree-wrapper" id="calibre_link-1007">
<h2 class="sigil_not_in_toc">数据集</h2>
<p class="calibre2">Scikit-learn可以从一个或者多个数据集中学习信息，这些数据集合可表示为2维阵列，也可认为是一个列表。列表的第一个维度代表 <strong class="calibre14">样本</strong> ，第二个维度代表 <strong class="calibre14">特征</strong> （每一行代表一个样本，每一列代表一种特征）。</p>
<div class="toctree-wrapper">
<p class="calibre10">样例: iris 数据集（鸢尾花卉数据集）</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">data</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">data</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(150, 4)</span>
</pre>
</div>
</div>
<p class="calibre10">这个数据集包含150个样本，每个样本包含4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，详细数据可以通过``iris.DESCR``查看。</p>
</div>
<p class="calibre10">如果原始数据不是``(n_samples, n_features)``的形状时，使用之前需要进行预处理以供scikit-learn使用。</p>
<div class="toctree-wrapper">
<p class="calibre10">数据预处理样例:digits数据集(手写数字数据集)</p>
<a class="calibre3 pcalibre" href="../../auto_examples/datasets/plot_digits_last_image.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_last_image_001.png" class="align-right2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000088.jpg" /></a>
<p class="calibre10">digits数据集包含1797个手写数字的图像，每个图像为8*8像素</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(1797, 8, 8)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">matplotlib.pyplot</span> <span class="calibre4">as</span> <span class="calibre4">plt</span> 
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">imshow</span><span class="calibre4">(</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">cmap</span><span class="calibre4">=</span><span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">cm</span><span class="calibre4">.</span><span class="calibre4">gray_r</span><span class="calibre4">)</span> 
<span class="calibre4">&lt;matplotlib.image.AxesImage object at ...&gt;</span>
</pre>
</div>
</div>
<p class="calibre10">为了在scikit中使用这一数据集，需要将每一张8×8的图像转换成长度为64的特征向量</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">data</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">((</span><span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">],</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">))</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1008">
<h2 class="sigil_not_in_toc">预估对象</h2>
<p class="calibre2"><strong class="calibre14">拟合数据</strong>: scikit-learn实现最重要的一个API是`estimator`。estimators是基于数据进行学习的任何对象，它可以是一个分类器，回归或者是一个聚类算法，或者是从原始数据中提取/过滤有用特征的变换器。</p>
<p class="calibre10">所有的拟合模型对象拥有一个名为``fit``的方法，参数是一个数据集（通常是一个2维列表）:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">data</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10"><strong class="calibre14">拟合模型对象构造参数</strong>: 在创建一个拟合模型时，可以设置相关参数，在创建之后也可以修改对应的参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimator</span> <span class="calibre4">=</span> <span class="calibre4">Estimator</span><span class="calibre4">(</span><span class="calibre4">param1</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">param2</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">param1</span>
<span class="calibre4">1</span>
</pre>
</div>
</div>
<p class="calibre10"><strong class="calibre14">拟合参数</strong>: 当拟合模型完成对数据的拟合之后，可以从拟合模型中获取拟合的参数结果，所有拟合完成的参数均以下划线(_)作为结尾:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">estimated_param_</span> 
</pre>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-94">
<span id="calibre_link-1009" class="calibre4"></span><h1 class="calibre5">监督学习：从高维观察预测输出变量</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Kyrie</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@森系</a><br class="calibre9" /> 
    </div>
<div class="toctree-wrapper">
<p class="calibre10">监督学习解决的问题</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../../supervised_learning.html#supervised-learning"><span class="calibre4">监督学习</span></a> 在于学习两个数据集的联系：观察数据 <code class="docutils"><span class="calibre4">X</span></code> 和我们正在尝试预测的额外变量 <code class="docutils"><span class="calibre4">y</span></code> (通常称“目标”或“标签”)， 而且通常是长度为 <code class="docutils"><span class="calibre4">n_samples</span></code> 的一维数组。</p>
<p class="calibre10">scikit-learn 中所有监督的 <cite class="calibre13">估计量 &lt;https://en.wikipedia.org/wiki/Estimator&gt;</cite> 都有一个用来拟合模型的 <code class="docutils"><span class="calibre4">fit(X,</span> <span class="calibre4">y)</span></code> 方法，和根据给定的没有标签观察值 <code class="docutils"><span class="calibre4">X</span></code> 返回预测的带标签的 <code class="docutils"><span class="calibre4">y</span></code> 的 <code class="docutils"><span class="calibre4">predict(X)</span></code> 方法。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">词汇：分类和回归</p>
<p class="calibre10">如果预测任务是为了将观察值分类到有限的标签集合中，换句话说，就是给观察对象命名，那任务就被称为 <strong class="calibre14">分类</strong> 任务。另外，如果任务是为了预测一个连续的目标变量，那就被称为 <strong class="calibre14">回归</strong> 任务。</p>
<p class="calibre10">当在 scikit-learn 中进行分类时，<code class="docutils"><span class="calibre4">y</span></code> 是一个整数或字符型的向量。</p>
<p class="calibre10">注：可以查看 :ref: <cite class="calibre13">用 scikit-learn 进行机器学习介绍 &lt;introduction&gt;</cite> 快速了解机器学习中的基础词汇。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1010">
<h2 class="sigil_not_in_toc">最近邻和维度惩罚</h2>
<div class="toctree-wrapper">
<p class="calibre10">鸢尾属植物分类：</p>
<a class="calibre3 pcalibre" href="../../auto_examples/datasets/plot_iris_dataset.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_dataset_001.png" class="align-right3" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000100.jpg" /></a>
<p class="calibre10">鸢尾属植物数据集是根据花瓣长度、花瓣度度、萼片长度和萼片宽度4个特征对3种不同类型的鸢尾属植物进行分类:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_X</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">unique</span><span class="calibre4">(</span><span class="calibre4">iris_y</span><span class="calibre4">)</span>
<span class="calibre4">array([0, 1, 2])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1011">
<h3 class="sigil_not_in_toc1">K近邻分类器</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">最近邻</a>: 也许是最简单的分类器：给定一个新的观察值 <code class="docutils"><span class="calibre4">X_test</span></code>，用最接近的特征向量在训练集(比如，用于训练估计器的数据)找到观察值。(请看 Scikit-learn 在线学习文档的 <a class="calibre3 pcalibre" href="../../modules/neighbors.html#neighbors"><span class="calibre4">最近邻章节</span></a> 获取更多关于这种分类器的信息)</p>
<div class="toctree-wrapper">
<p class="calibre10">训练集和测试集</p>
<p class="calibre10">当用任意的学习算法进行实验时，最重要的就是不要在用于拟合估计器的数据上测试一个估计器的预期值，因为这不会评估在 <strong class="calibre14">新数据</strong> 上估计器的执行情况。这也是数据集经常被分为 <em class="calibre13">训练</em> 和 <em class="calibre13">测试</em> 数据的原因。</p>
</div>
<p class="calibre10"><strong class="calibre14">KNN(k 最近邻)分类器例子</strong>:</p>
<a class="calibre3 pcalibre" href="../../auto_examples/neighbors/plot_classification.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_classification_001.png" class="calibre38" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000844.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 将鸢尾属植物数据集分解为训练集和测试集</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 随机排列，用于使分解的数据随机分布</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">seed</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">indices</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">permutation</span><span class="calibre4">(</span><span class="calibre4">len</span><span class="calibre4">(</span><span class="calibre4">iris_X</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_X_train</span> <span class="calibre4">=</span> <span class="calibre4">iris_X</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_y_train</span> <span class="calibre4">=</span> <span class="calibre4">iris_y</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_X_test</span>  <span class="calibre4">=</span> <span class="calibre4">iris_X</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">:]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_y_test</span>  <span class="calibre4">=</span> <span class="calibre4">iris_y</span><span class="calibre4">[</span><span class="calibre4">indices</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">:]]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 创建和拟合一个最近邻分类器</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.neighbors</span> <span class="calibre4">import</span> <span class="calibre4">KNeighborsClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">knn</span> <span class="calibre4">=</span> <span class="calibre4">KNeighborsClassifier</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">knn</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris_X_train</span><span class="calibre4">,</span> <span class="calibre4">iris_y_train</span><span class="calibre4">)</span> 
<span class="calibre4">KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',</span>
<span class="calibre4">           metric_params=None, n_jobs=1, n_neighbors=5, p=2,</span>
<span class="calibre4">           weights='uniform')</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">knn</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">iris_X_test</span><span class="calibre4">)</span>
<span class="calibre4">array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris_y_test</span>
<span class="calibre4">array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1012">
<span id="calibre_link-1013" class="calibre4"></span><h3 class="sigil_not_in_toc1">维度惩罚</h3>
<p class="calibre2">为了使一个估计器有效，你需要邻接点间的距离小于一些值：<img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000615.jpg" alt="d" />，这取决于具体问题。在一维中，这需要平均 <cite class="calibre13">n sim 1/d</cite> 点。在上文 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" />-NN 例子中，如果数据只是由一个0到1的特征值和 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000757.jpg" alt="n" /> 训练观察值所描述，那么新数据将不会超过 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000021.jpg" alt="1/n" />。因此，最近邻决策规则会很有效率，因为与类间特征变量范围相比， <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000021.jpg" alt="1/n" /> 很小。</p>
<p class="calibre10">如果特征数是 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" />，你现在就需要 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000039.jpg" alt="n \sim 1/d^p" /> 点。也就是说我们在一维 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000321.jpg" alt="[0, 1]" /> 空间里需要10个点，在 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" /> 维里就需要 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000640.jpg" alt="10^p" /> 个点。当 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000009.jpg" alt="p" /> 增大时，为了得到一个好的估计器，相应的训练点数量就需要成倍增大。</p>
<p class="calibre10">比如，如果每个点只是单个数字(8个字节)，那么一个 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000334.jpg" alt="k" />-NN 估计器在一个非常小的 <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000347.jpg" alt="p \sim 20" /> 维度下就需要比现在估计的整个互联网的大小(±1000 艾字节或更多)还要多的训练数据。</p>
<p class="calibre10">这叫 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">维度惩罚</a>，是机器学习领域的核心问题。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1014">
<h2 class="sigil_not_in_toc">线性模型：从回归到稀疏</h2>
<div class="toctree-wrapper">
<p class="calibre10">糖尿病数据集</p>
<p class="calibre10">糖尿病数据集包括442名患者的10个生理特征(年龄，性别，体重，血压)，和一年后的疾病级别指标:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_diabetes</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes_X_train</span> <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">20</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes_X_test</span>  <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">20</span><span class="calibre4">:]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes_y_train</span> <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">20</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes_y_test</span>  <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">20</span><span class="calibre4">:]</span>
</pre>
</div>
</div>
<p class="calibre10">手头上的任务是为了从生理特征预测疾病级别。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1015">
<h3 class="sigil_not_in_toc1">线性回归</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="docutils"><span class="calibre4">LinearRegression</span></code></a>，最简单的拟合线性模型形式，是通过调整数据集的一系列参数令残差平方和尽可能小。</p>
<a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ols_001.png" class="align-right4" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000312.jpg" /></a>
<p class="calibre10">Linear models: <img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000231.jpg" alt="y = X\beta + \epsilon" /></p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000078.jpg" alt="X" />: 数据</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000097.jpg" alt="y" />: 目标变量</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000092.jpg" alt="\beta" />: 回归系数</li>
<li class="toctree-l"><img class="math" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000516.jpg" alt="\epsilon" />: 观察噪声</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LinearRegression</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_train</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_train</span><span class="calibre4">)</span>
<span class="calibre4">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">coef_</span><span class="calibre4">)</span>
<span class="calibre4">[   0.30349955 -237.63931533  510.53060544  327.73698041 -814.13170937</span>
<span class="calibre4">  492.81458798  102.84845219  184.60648906  743.51961675   76.09517222]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 均方误差</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">((</span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_test</span><span class="calibre4">)</span><span class="calibre4">-</span><span class="calibre4">diabetes_y_test</span><span class="calibre4">)</span><span class="calibre4">**</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">2004.56760268...</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 方差分数：1 是完美的预测</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 0 意味着 X 和 y 之间没有线性关系。</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_test</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_test</span><span class="calibre4">)</span> 
<span class="calibre4">0.5850753022690...</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1016">
<span id="calibre_link-1017" class="calibre4"></span><h3 class="sigil_not_in_toc1">收缩</h3>
<p class="calibre2">如果每个维度的数据点很少，观察噪声就会导致很大的方差：</p>
<a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols_ridge_variance.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ols_ridge_variance_001.png" class="align-right5" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000153.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">c_</span><span class="calibre4">[</span> <span class="calibre4">.</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">.</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">test</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">c_</span><span class="calibre4">[</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LinearRegression</span><span class="calibre4">()</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">matplotlib.pyplot</span> <span class="calibre4">as</span> <span class="calibre4">plt</span> 
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">figure</span><span class="calibre4">()</span> 

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">seed</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">_</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">6</span><span class="calibre4">):</span> 
<span class="calibre4">... </span>   <span class="calibre4">this_X</span> <span class="calibre4">=</span> <span class="calibre4">.</span><span class="calibre4">1</span><span class="calibre4">*</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">normal</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">))</span> <span class="calibre4">+</span> <span class="calibre4">X</span>
<span class="calibre4">... </span>   <span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">this_X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">... </span>   <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">plot</span><span class="calibre4">(</span><span class="calibre4">test</span><span class="calibre4">,</span> <span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">test</span><span class="calibre4">))</span> 
<span class="calibre4">... </span>   <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">scatter</span><span class="calibre4">(</span><span class="calibre4">this_X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">s</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>  
</pre>
</div>
</div>
<p class="calibre10">高纬统计学习中的一个解决方法是 <em class="calibre13">收缩</em> 回归系数到0：任何两个随机选择的观察值数据集都很可能是不相关的。这称为 <cite class="calibre13">岭回归</cite> ：</p>
<a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols_ridge_variance.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ols_ridge_variance_002.png" class="align-right5" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000103.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">Ridge</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=.</span><span class="calibre4">1</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">figure</span><span class="calibre4">()</span> 

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">seed</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">_</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">6</span><span class="calibre4">):</span> 
<span class="calibre4">... </span>   <span class="calibre4">this_X</span> <span class="calibre4">=</span> <span class="calibre4">.</span><span class="calibre4">1</span><span class="calibre4">*</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">normal</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">))</span> <span class="calibre4">+</span> <span class="calibre4">X</span>
<span class="calibre4">... </span>   <span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">this_X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">)</span>
<span class="calibre4">... </span>   <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">plot</span><span class="calibre4">(</span><span class="calibre4">test</span><span class="calibre4">,</span> <span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">test</span><span class="calibre4">))</span> 
<span class="calibre4">... </span>   <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">scatter</span><span class="calibre4">(</span><span class="calibre4">this_X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">s</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span> 
</pre>
</div>
</div>
<p class="calibre10">这是 <strong class="calibre14">bias/variance tradeoff</strong> 中的一个例子：岭参数 <code class="docutils"><span class="calibre4">alpha</span></code> 越大，偏差越大，方差越小。</p>
<p class="calibre10">我们可以选择 <code class="docutils"><span class="calibre4">alpha</span></code> 来最小化排除错误，这里使用糖尿病数据集而不是人为数据:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">alphas</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">logspace</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">6</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">__future__</span> <span class="calibre4">import</span> <span class="calibre4">print_function</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">([</span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">alpha</span>
<span class="calibre4">... </span>            <span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_train</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_train</span><span class="calibre4">,</span>
<span class="calibre4">... </span>            <span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_test</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_test</span><span class="calibre4">)</span> <span class="calibre4">for</span> <span class="calibre4">alpha</span> <span class="calibre4">in</span> <span class="calibre4">alphas</span><span class="calibre4">])</span> 
<span class="calibre4">[0.5851110683883..., 0.5852073015444..., 0.5854677540698..., 0.5855512036503..., 0.5830717085554..., 0.57058999437...]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">捕获拟合参数噪声使得模型不能归纳新的数据称为 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Overfitting">过拟合</a>。岭回归产生的偏差被称为 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Regularization_%28machine_learning%29">正则化</a>。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1018">
<span id="calibre_link-1019" class="calibre4"></span><h3 class="sigil_not_in_toc1">稀疏</h3>
<p class="calibre2"><strong class="calibre14">只拟合特征1和2</strong></p>
<p class="calibre10">
<strong class="calibre14"><a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_1" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000054.jpg" class="calibre64" /></a> <a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_3" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000156.jpg" class="calibre64" /></a> <a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_2" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000051.jpg" class="calibre64" /></a></strong></p>
<div class="toctree-wrapper">
<p class="calibre10">Note</p>
<p class="calibre10">整个糖尿病数据集包括11个维度(10个特征维度和1个目标变量)。很难直观地表示出来，但是记住那是一个比较 <em class="calibre13">空</em> 的空间可能比较有用。</p>
</div>
<p class="calibre10">我们可以看到，尽管特征2在整个模型占有一个很大的系数，但是当考虑特征1时，其对 <code class="docutils"><span class="calibre4">y</span></code> 的影响就较小了。</p>
<p class="calibre10">为了提高问题的条件(比如，缓解`维度惩罚`)，只选择信息特征和设置无信息时就会变得有趣，比如特征2到0。岭回归会减小他们的值，但不会减到0.另一种抑制方法，称为 <a class="calibre3 pcalibre" href="../../modules/linear_model.html#lasso"><span class="calibre4">Lasso</span></a> (最小绝对收缩和选择算子)，可以把一些系数设为0。这些方法称为 <strong class="calibre14">稀疏法</strong>，稀疏可以看作是奥卡姆剃刀的应用：<em class="calibre13">模型越简单越好</em>。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">Lasso</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">set_params</span><span class="calibre4">(</span><span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">alpha</span>
<span class="calibre4">... </span>            <span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_train</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_train</span>
<span class="calibre4">... </span>            <span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_test</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_test</span><span class="calibre4">)</span>
<span class="calibre4">... </span>       <span class="calibre4">for</span> <span class="calibre4">alpha</span> <span class="calibre4">in</span> <span class="calibre4">alphas</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">best_alpha</span> <span class="calibre4">=</span> <span class="calibre4">alphas</span><span class="calibre4">[</span><span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">index</span><span class="calibre4">(</span><span class="calibre4">max</span><span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">))]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">alpha</span> <span class="calibre4">=</span> <span class="calibre4">best_alpha</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">diabetes_X_train</span><span class="calibre4">,</span> <span class="calibre4">diabetes_y_train</span><span class="calibre4">)</span>
<span class="calibre4">Lasso(alpha=0.025118864315095794, copy_X=True, fit_intercept=True,</span>
<span class="calibre4">   max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="calibre4">   random_state=None, selection='cyclic', tol=0.0001, warm_start=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">regr</span><span class="calibre4">.</span><span class="calibre4">coef_</span><span class="calibre4">)</span>
<span class="calibre4">[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.</span>
<span class="calibre4"> -187.19554705   69.38229038  508.66011217   71.84239008]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">同一个问题的不同算法</strong></p>
<p class="calibre10">不同的算法可以用于解决同一个数学问题。比如在 scikit-learn 里 <code class="docutils"><span class="calibre4">Lasso</span></code> 对象使用 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate descent</a> 方法解决 lasso 回归问题，对于大型数据集很有效。但是，scikit-learn 也提供了使用 <em class="calibre13">LARS</em> 算法 的:class:<cite class="calibre13">LassoLars</cite> 对象，对于处理带权向量非常稀疏的数据非常有效(比如，问题的观察值很少)。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1020">
<span id="calibre_link-1021" class="calibre4"></span><h3 class="sigil_not_in_toc1">分类</h3>
<a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_logistic.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_logistic_001.png" class="calibre64" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000043.jpg" /></a>
<p class="calibre10">对于分类，比如标定 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Iris_flower_data_set">鸢尾属植物</a> 任务，线性回归就不是好方法了，因为它会给数据很多远离决策边界的权值。一个线性方法是为了拟合 sigmoid 函数 或 <strong class="calibre14">logistic</strong> 函数：</p>
<div class="toctree-wrapper">
<p class="calibre10"><img src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000031.jpg" alt="y = \textrm{sigmoid}(X\beta - \textrm{offset}) + \epsilon = \frac{1}{1 + \textrm{exp}(- X\beta + \textrm{offset})} + \epsilon" class="math" /></p>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">logistic</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LogisticRegression</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1e5</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">logistic</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris_X_train</span><span class="calibre4">,</span> <span class="calibre4">iris_y_train</span><span class="calibre4">)</span>
<span class="calibre4">LogisticRegression(C=100000.0, class_weight=None, dual=False,</span>
<span class="calibre4">          fit_intercept=True, intercept_scaling=1, max_iter=100,</span>
<span class="calibre4">          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,</span>
<span class="calibre4">          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)</span>
</pre>
</div>
</div>
<p class="calibre10">这就是有名的： <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a></p>
<a class="calibre3 pcalibre" href="../../auto_examples/linear_model/plot_iris_logistic.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_logistic_001.png" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000865.jpg" class="calibre65" /></a>
<div class="toctree-wrapper">
<p class="calibre10">多类分类</p>
<p class="calibre10">如果你有很多类需要预测，一种常用方法就是去拟合一对多分类器，然后使用根据投票为最后做决定。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">使用 logistic 回归进行收缩和稀疏</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="docutils"><span class="calibre4">LogisticRegression</span></code></a> 对象中的 <code class="docutils"><span class="calibre4">C</span></code> 参数控制着正则化数量：<code class="docutils"><span class="calibre4">C</span></code> 值越大，正则化数量越小。<code class="docutils"><span class="calibre4">penalty="l2"</span></code> 提供 <span class="calibre4">收缩`(比如，无稀疏系数)，同时 ``penalty=”l1”`</span> 提供`稀疏化`。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">练习</strong></p>
<p class="calibre10">尝试用最近邻和线性模型分类数字数据集。留出最后 10%的数据，并测试观察值预期效果。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span><span class="calibre4">,</span> <span class="calibre4">neighbors</span><span class="calibre4">,</span> <span class="calibre4">linear_model</span>

<span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">X_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">y_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span>
</pre>
</div>
</div>
<p class="calibre10">方法: <a class="calibre3 pcalibre" href="../../_downloads/plot_digits_classification_exercise.py" download=""><code class="docutils"><span class="calibre4">../../auto_examples/exercises/plot_digits_classification_exercise.py</span></code></a></p>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1022">
<h2 class="sigil_not_in_toc">支持向量积(SVMs)</h2>
<div class="toctree-wrapper" id="calibre_link-1023">
<h3 class="sigil_not_in_toc1">线性 SVMs</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../../modules/svm.html#svm"><span class="calibre4">支持向量机</span></a> 属于判别模型家族：它们尝试通过找到样例的一个组合来构建一个两类之间最大化的平面。通过 <code class="docutils"><span class="calibre4">C</span></code> 参数进行正则化设置：<code class="docutils"><span class="calibre4">C</span></code> 的值小意味着边缘是通过分割线周围的所有观测样例进行计算得到的(更正则化)；<code class="docutils"><span class="calibre4">C</span></code> 的值大意味着边缘是通过邻近分割线的观测样例计算得到的(更少正则化)。</p>
<div class="toctree-wrapper">
<p class="calibre10">例子:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="../../auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py"><span class="calibre4">Plot different SVM classifiers in the iris dataset</span></a></li>
</ul>
</div>
<p class="calibre10">SVMs 可以用于回归 &ndash;:class: <cite class="calibre13">SVR</cite> (支持向量回归)&ndash;，或者分类 &ndash;:class: <cite class="calibre13">SVC</cite> (支持向量分类)。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">iris_X_train</span><span class="calibre4">,</span> <span class="calibre4">iris_y_train</span><span class="calibre4">)</span>    
<span class="calibre4">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="calibre4">    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',</span>
<span class="calibre4">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="calibre4">    tol=0.001, verbose=False)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10"><strong class="calibre14">规格化数据</strong></p>
<p class="calibre10">对很多估计器来说，包括 SVMs，为每个特征值使用单位标准偏差的数据集，是获得好的预测重要前提。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1024">
<span id="calibre_link-1025" class="calibre4"></span><h3 class="sigil_not_in_toc1">使用核</h3>
<p class="calibre2">在特征空间类并不总是线性可分的。解决办法就是构建一个不是线性的但能是多项式的函数做代替。这要使用 <em class="calibre13">核技巧(kernel trick)</em>，它可以被看作通过设置   <em class="calibre13">kernels</em> 在观察样例上创建决策力量：</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><strong class="calibre14">线性核</strong></td>
<td class="label1"><strong class="calibre14">多项式核</strong></td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_linear" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000862.jpg" class="calibre64" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_poly" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000072.jpg" class="calibre64" /></a></td>
</tr>
<tr class="calibre23"><td class="label1"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span>
</pre>
</div>
</div>
</td>
<td class="label1"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'poly'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>              <span class="calibre4">degree</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># degree: polynomial degree</span>
</pre>
</div>
</div>
</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="100%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><strong class="calibre14">RBF 内核(径向基函数)</strong></td>
</tr>
<tr class="row-odd"><td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_rbf" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000802.jpg" class="calibre64" /></a></td>
</tr>
<tr class="calibre23"><td class="label1"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># gamma: inverse of size of</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># radial kernel</span>
</pre>
</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">交互例子</strong></p>
<p class="calibre10">查看 <a class="calibre3 pcalibre" href="../../auto_examples/applications/svm_gui.html#sphx-glr-auto-examples-applications-svm-gui-py"><span class="calibre4">SVM GUI</span></a> 通过下载
<code class="docutils"><span class="calibre4">svm_gui.py</span></code>；通过左右按键添加两类数据点，拟合模型并改变参数和数据。</p>
</div>
<a class="calibre3 pcalibre" href="../../auto_examples/datasets/plot_iris_dataset.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_iris_dataset_001.png" class="align-right6" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000100.jpg" /></a>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">练习</strong></p>
<p class="calibre10">根据特征1和特征2，尝试用 SVMs 把1和2类从鸢尾属植物数据集中分出来。为每一个类留下10%，并测试这些观察值预期效果。</p>
<p class="calibre10"><strong class="calibre14">警告</strong>: 类是有序的，不要留下最后10%，不然你只能测试一个类了。</p>
<p class="calibre10"><strong class="calibre14">提示</strong>: 为了直观显示，你可以在网格上使用 <code class="docutils"><span class="calibre4">decision_function</span></code> 方法。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>

<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">[</span><span class="calibre4">y</span> <span class="calibre4">!=</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">:</span><span class="calibre4">2</span><span class="calibre4">]</span>
<span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">y</span><span class="calibre4">[</span><span class="calibre4">y</span> <span class="calibre4">!=</span> <span class="calibre4">0</span><span class="calibre4">]</span>
</pre>
</div>
</div>
<p class="calibre10">方法: <a class="calibre3 pcalibre" href="../../_downloads/plot_iris_exercise.py" download=""><code class="docutils"><span class="calibre4">../../auto_examples/exercises/plot_iris_exercise.py</span></code></a></p>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-7">
<span id="calibre_link-1026" class="calibre4"></span><h1 class="calibre5">模型选择：选择估计量及其参数</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@森系</a><br class="calibre9" />     
    </div>
<div class="toctree-wrapper" id="calibre_link-1027">
<h2 class="sigil_not_in_toc">分数和交叉验证分数</h2>
<p class="calibre2">如我们所见，每一个估计量都有一个可以在新数据上判定拟合质量(或预期值)的 <code class="docutils"><span class="calibre4">score</span></code> 方法。<strong class="calibre14">越大越好</strong>.</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span><span class="calibre4">,</span> <span class="calibre4">svm</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">svc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">100</span><span class="calibre4">],</span> <span class="calibre4">y_digits</span><span class="calibre4">[:</span><span class="calibre4">-</span><span class="calibre4">100</span><span class="calibre4">])</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">100</span><span class="calibre4">:],</span> <span class="calibre4">y_digits</span><span class="calibre4">[</span><span class="calibre4">-</span><span class="calibre4">100</span><span class="calibre4">:])</span>
<span class="calibre4">0.97999999999999998</span>
</pre>
</div>
</div>
<p class="calibre10">为了更好地预测精度(我们可以用它作为模型的拟合优度代理)，我们可以连续分解用于我们训练和测试用的 <em class="calibre13">折叠数据</em>。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_folds</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array_split</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_folds</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array_split</span><span class="calibre4">(</span><span class="calibre4">y_digits</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">scores</span> <span class="calibre4">=</span> <span class="calibre4">list</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">k</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">3</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4"># 为了稍后的 ‘弹出’ 操作，我们使用 ‘列表’ 来复制数据</span>
<span class="calibre4">... </span>    <span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">X_folds</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">X_test</span>  <span class="calibre4">=</span> <span class="calibre4">X_train</span><span class="calibre4">.</span><span class="calibre4">pop</span><span class="calibre4">(</span><span class="calibre4">k</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">X_train</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">concatenate</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">y_train</span> <span class="calibre4">=</span> <span class="calibre4">list</span><span class="calibre4">(</span><span class="calibre4">y_folds</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">y_test</span>  <span class="calibre4">=</span> <span class="calibre4">y_train</span><span class="calibre4">.</span><span class="calibre4">pop</span><span class="calibre4">(</span><span class="calibre4">k</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">y_train</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">concatenate</span><span class="calibre4">(</span><span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">... </span>    <span class="calibre4">scores</span><span class="calibre4">.</span><span class="calibre4">append</span><span class="calibre4">(</span><span class="calibre4">svc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">scores</span><span class="calibre4">)</span>
<span class="calibre4">[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]</span>
</pre>
</div>
</div>
<p class="calibre10">这被称为 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> 交叉验证.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1028">
<span id="calibre_link-1029" class="calibre4"></span><h2 class="sigil_not_in_toc">交叉验证生成器</h2>
<p class="calibre2">scikit-learn 有可以生成训练/测试索引列表的类，可用于流行的交叉验证策略。</p>
<p class="calibre10">类提供了 <code class="docutils"><span class="calibre4">split</span></code> 方法，方法允许输入能被分解的数据集，并为每次选择的交叉验证策略迭代生成训练/测试集索引。</p>
<p class="calibre10">下面是使用 <code class="docutils"><span class="calibre4">split</span></code> 方法的例子。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">KFold</span><span class="calibre4">,</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">"a"</span><span class="calibre4">,</span> <span class="calibre4">"a"</span><span class="calibre4">,</span> <span class="calibre4">"b"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">,</span> <span class="calibre4">"c"</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">k_fold</span> <span class="calibre4">=</span> <span class="calibre4">KFold</span><span class="calibre4">(</span><span class="calibre4">n_splits</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">train_indices</span><span class="calibre4">,</span> <span class="calibre4">test_indices</span> <span class="calibre4">in</span> <span class="calibre4">k_fold</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">):</span>
<span class="calibre4">... </span>     <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">'Train: </span><span class="calibre4">%s</span><span class="calibre4"> | test: </span><span class="calibre4">%s</span><span class="calibre4">'</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">train_indices</span><span class="calibre4">,</span> <span class="calibre4">test_indices</span><span class="calibre4">))</span>
<span class="calibre4">Train: [2 3 4 5] | test: [0 1]</span>
<span class="calibre4">Train: [0 1 4 5] | test: [2 3]</span>
<span class="calibre4">Train: [0 1 2 3] | test: [4 5]</span>
</pre>
</div>
</div>
<p class="calibre10">然后就可以很容易地执行交叉验证了:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">[</span><span class="calibre4">svc</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[</span><span class="calibre4">train</span><span class="calibre4">],</span> <span class="calibre4">y_digits</span><span class="calibre4">[</span><span class="calibre4">train</span><span class="calibre4">])</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[</span><span class="calibre4">test</span><span class="calibre4">],</span> <span class="calibre4">y_digits</span><span class="calibre4">[</span><span class="calibre4">test</span><span class="calibre4">])</span>
<span class="calibre4">... </span>         <span class="calibre4">for</span> <span class="calibre4">train</span><span class="calibre4">,</span> <span class="calibre4">test</span> <span class="calibre4">in</span> <span class="calibre4">k_fold</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">)]</span>
<span class="calibre4">[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]</span>
</pre>
</div>
</div>
<p class="calibre10">交叉验证分数可以使用 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 直接计算出来。给定一个估计量，交叉验证对象，和输入数据集， <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="docutils"><span class="calibre4">cross_val_score</span></code></a> 函数就会反复分解出训练和测试集的数据，然后使用训练集和为每次迭代交叉验证运算出的基于测试集的分数来训练估计量。</p>
<p class="calibre10">默认情况下，估计器的 <code class="docutils"><span class="calibre4">score</span></code> 方法被用于运算个体分数。</p>
<p class="calibre10">可以参考 <a class="calibre3 pcalibre" href="../../modules/metrics.html#metrics"><span class="calibre4">metrics 模块</span></a> 学习更多可用的评分方法。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">svc</span><span class="calibre4">,</span> <span class="calibre4">X_digits</span><span class="calibre4">,</span> <span class="calibre4">y_digits</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">k_fold</span><span class="calibre4">,</span> <span class="calibre4">n_jobs</span><span class="calibre4">=-</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">array([ 0.93489149,  0.95659432,  0.93989983])</span>
</pre>
</div>
</div>
<p class="calibre10"><cite class="calibre13">n_jobs=-1</cite> 意味着运算会被调度到所有 CPU 上进行。</p>
<p class="calibre10">或者，可以提供 <code class="docutils"><span class="calibre4">scoring</span></code> 参数来指定替换的评分方法。</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">svc</span><span class="calibre4">,</span> <span class="calibre4">X_digits</span><span class="calibre4">,</span> <span class="calibre4">y_digits</span><span class="calibre4">,</span> <span class="calibre4">cv</span><span class="calibre4">=</span><span class="calibre4">k_fold</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                <span class="calibre4">scoring</span><span class="calibre4">=</span><span class="calibre4">'precision_macro'</span><span class="calibre4">)</span>
<span class="calibre4">array([ 0.93969761,  0.95911415,  0.94041254])</span>
</pre>
</div>
</div>
<p class="calibre10"><strong class="calibre14">交叉验证生成器</strong></p>
</div>
</blockquote>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="docutils"><span class="calibre4">KFold</span></code></a> <strong class="calibre14">(n_splits, shuffle, random_state)</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="docutils"><span class="calibre4">StratifiedKFold</span></code></a> <strong class="calibre14">(n_splits, shuffle, random_state)</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold" title="sklearn.model_selection.GroupKFold"><code class="docutils"><span class="calibre4">GroupKFold</span></code></a> <strong class="calibre14">(n_splits)</strong></td>
</tr>
<tr class="row-odd"><td class="label1">将其分解为 K 个折叠，在 K-1 上训练，然后排除测试。</td>
<td class="label1">和 K-Fold 一样，但会保留每个折叠里的类分布。</td>
<td class="label1">确保相同组不会在测试和训练集里。</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code class="docutils"><span class="calibre4">ShuffleSplit</span></code></a> <strong class="calibre14">(n_splits, test_size, train_size, random_state)</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit" title="sklearn.model_selection.StratifiedShuffleSplit"><code class="docutils"><span class="calibre4">StratifiedShuffleSplit</span></code></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.GroupShuffleSplit.html#sklearn.model_selection.GroupShuffleSplit" title="sklearn.model_selection.GroupShuffleSplit"><code class="docutils"><span class="calibre4">GroupShuffleSplit</span></code></a></td>
</tr>
<tr class="row-odd"><td class="label1">生成基于随机排列的训练/测试索引。</td>
<td class="label1">和 shuffle 分解一样，但会保留每个迭代里的类分布。</td>
<td class="label1">确保相同组不会在测试和训练集里。</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code class="docutils"><span class="calibre4">LeaveOneGroupOut</span></code></a> <strong class="calibre14">()</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code class="docutils"><span class="calibre4">LeavePGroupsOut</span></code></a>  <strong class="calibre14">(n_groups)</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code class="docutils"><span class="calibre4">LeaveOneOut</span></code></a> <strong class="calibre14">()</strong></td>
</tr>
<tr class="row-odd"><td class="label1">使用数组分组来给观察分组。</td>
<td class="label1">忽略 P 组。</td>
<td class="label1">忽略一个观察。</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut" title="sklearn.model_selection.LeavePOut"><code class="docutils"><span class="calibre4">LeavePOut</span></code></a> <strong class="calibre14">(p)</strong></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit" title="sklearn.model_selection.PredefinedSplit"><code class="docutils"><span class="calibre4">PredefinedSplit</span></code></a></td>
</tr>
<tr class="row-odd"><td class="label1">忽略 P 观察。</td>
<td class="label1">生成基于预定义分解的训练/测试索引。</td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">练习</strong></p>
<a class="calibre3 pcalibre" href="../../auto_examples/exercises/plot_cv_digits.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_cv_digits_001.png" class="align-right7" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000131.jpg" /></a>
<p class="calibre10">在数字数据集中，用一个线性内核绘制一个 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="docutils"><span class="calibre4">SVC</span></code></a> 估计器的交叉验证分数来作为 <code class="docutils"><span class="calibre4">C</span></code> 参数函数(使用从1到10的点对数网格).</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>
<span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span><span class="calibre4">,</span> <span class="calibre4">svm</span>

<span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span>

<span class="calibre4">svc</span> <span class="calibre4">=</span> <span class="calibre4">svm</span><span class="calibre4">.</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'linear'</span><span class="calibre4">)</span>
<span class="calibre4">C_s</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">logspace</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10"><strong class="calibre14">方法：</strong> <a class="calibre3 pcalibre" href="../../auto_examples/exercises/plot_cv_digits.html#sphx-glr-auto-examples-exercises-plot-cv-digits-py"><span class="calibre4">Cross-validation on Digits Dataset Exercise</span></a></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1030">
<h2 class="sigil_not_in_toc">网格搜索和交叉验证估计量</h2>
<div class="toctree-wrapper" id="calibre_link-1031">
<h3 class="sigil_not_in_toc1">网格搜索</h3>
<p class="calibre2">scikit-learn 提供了一个对象，在给定数据情况下，在一个参数网格，估计器拟合期间计算分数，并选择参数来最大化交叉验证分数。这个对象在构建过程中获取估计器并提供一个估计器 API。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">,</span> <span class="calibre4">cross_val_score</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">Cs</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">logspace</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">6</span><span class="calibre4">,</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">estimator</span><span class="calibre4">=</span><span class="calibre4">svc</span><span class="calibre4">,</span> <span class="calibre4">param_grid</span><span class="calibre4">=</span><span class="calibre4">dict</span><span class="calibre4">(</span><span class="calibre4">C</span><span class="calibre4">=</span><span class="calibre4">Cs</span><span class="calibre4">),</span>
<span class="calibre4">... </span>                   <span class="calibre4">n_jobs</span><span class="calibre4">=-</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[:</span><span class="calibre4">1000</span><span class="calibre4">],</span> <span class="calibre4">y_digits</span><span class="calibre4">[:</span><span class="calibre4">1000</span><span class="calibre4">])</span>        
<span class="calibre4">GridSearchCV(cv=None,...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">best_score_</span>                                  
<span class="calibre4">0.925...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">best_estimator_</span><span class="calibre4">.</span><span class="calibre4">C</span>                            
<span class="calibre4">0.0077...</span>
</pre>
</div>
</div>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Prediction performance on test set is not as good as on train set</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">score</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">[</span><span class="calibre4">1000</span><span class="calibre4">:],</span> <span class="calibre4">y_digits</span><span class="calibre4">[</span><span class="calibre4">1000</span><span class="calibre4">:])</span>      
<span class="calibre4">0.943...</span>
</pre>
</div>
</div>
<p class="calibre10">默认情况下， <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="docutils"><span class="calibre4">GridSearchCV</span></code></a> 使用一个三倍折叠交叉验证。但是，如果它检测到分类器被传递，而不是回归，它就会使用分层的三倍。</p>
<div class="toctree-wrapper">
<p class="calibre10">嵌套交叉验证</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">cross_val_score</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">,</span> <span class="calibre4">X_digits</span><span class="calibre4">,</span> <span class="calibre4">y_digits</span><span class="calibre4">)</span>
<span class="calibre4">... </span>                                              
<span class="calibre4">array([ 0.938...,  0.963...,  0.944...])</span>
</pre>
</div>
</div>
<p class="calibre10">两个交叉验证循环并行执行：一个由 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="docutils"><span class="calibre4">GridSearchCV</span></code></a> 估计器设置 <code class="docutils"><span class="calibre4">gamma</span></code>，另一个 <code class="docutils"><span class="calibre4">cross_val_score</span></code> 则是测量估计器的预期执行情况。结果分数是对新数据上的预期分数的无偏估计。</p>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">你不可以并行运算嵌套对象(<code class="docutils"><span class="calibre4">n_jobs</span></code> 与1不同)。</p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1032">
<span id="calibre_link-1033" class="calibre4"></span><h3 class="sigil_not_in_toc1">交叉验证估计量</h3>
<p class="calibre2">设置参数的交叉验证可以更有效地完成一个基础算法。这就是为什么对某些估计量来说，scikit-learn 提供了 <span class="calibre4">交叉验证</span> 估计量自动设置它们的参数。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span><span class="calibre4">,</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lasso</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LassoCV</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">diabetes</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_diabetes</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_diabetes</span> <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_diabetes</span> <span class="calibre4">=</span> <span class="calibre4">diabetes</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lasso</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_diabetes</span><span class="calibre4">,</span> <span class="calibre4">y_diabetes</span><span class="calibre4">)</span>
<span class="calibre4">LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,</span>
<span class="calibre4">    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,</span>
<span class="calibre4">    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,</span>
<span class="calibre4">    verbose=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># 估计器自动选择它的 lambda:</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">lasso</span><span class="calibre4">.</span><span class="calibre4">alpha_</span> 
<span class="calibre4">0.01229...</span>
</pre>
</div>
</div>
<p class="calibre10">这些估计量和它们的副本称呼类似，在名字后加 ‘CV’。</p>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">练习</strong></p>
<p class="calibre10">在糖尿病数据集中，找到最优正则化参数 α。</p>
<p class="calibre10"><strong class="calibre14">另外：</strong> 你有多相信 α 的选择？</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>
<span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">datasets</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">LassoCV</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">Lasso</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">KFold</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>

<span class="calibre4">diabetes</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_diabetes</span><span class="calibre4">()</span>
</pre>
</div>
</div>
<p class="calibre10"><strong class="calibre14">方法：</strong> <a class="calibre3 pcalibre" href="../../auto_examples/exercises/plot_cv_diabetes.html#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py"><span class="calibre4">Cross-validation on diabetes Dataset Exercise</span></a></p>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-90">
<h1 class="calibre1">无监督学习: 寻求数据表示</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@X</a><br class="calibre9" />  
    </div>
<div class="toctree-wrapper" id="calibre_link-1034">
<h2 class="sigil_not_in_toc">聚类: 对样本数据进行分组</h2>
<div class="toctree-wrapper">
<p class="calibre10">可以利用聚类解决的问题</p>
<p class="calibre10">对于 iris 数据集来说，我们知道所有样本有 3 种不同的类型，但是并不知道每一个样本是那种类型：此时我们可以尝试一个 <strong class="calibre14">clustering task（聚类任务）</strong> 聚类算法: 将样本进行分组，相似的样本被聚在一起，而不同组别之间的样本是有明显区别的，这样的分组方式就是 <em class="calibre13">clusters（聚类）</em></p>
</div>
<div class="toctree-wrapper" id="calibre_link-1035">
<h3 class="sigil_not_in_toc1">K-means 聚类算法</h3>
<p class="calibre2">关于聚类有很多不同的聚类标准和相关算法，其中最简便的算法是 <a class="calibre3 pcalibre" href="../../modules/clustering.html#k-means"><span class="calibre4">K-means</span></a> 。</p>
<a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_cluster_iris.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_cluster_iris_002.png" class="align-right5" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000081.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">cluster</span><span class="calibre4">,</span> <span class="calibre4">datasets</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">iris</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_iris</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_iris</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">y_iris</span> <span class="calibre4">=</span> <span class="calibre4">iris</span><span class="calibre4">.</span><span class="calibre4">target</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">k_means</span> <span class="calibre4">=</span> <span class="calibre4">cluster</span><span class="calibre4">.</span><span class="calibre4">KMeans</span><span class="calibre4">(</span><span class="calibre4">n_clusters</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">k_means</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_iris</span><span class="calibre4">)</span> 
<span class="calibre4">KMeans(algorithm='auto', copy_x=True, init='k-means++', ...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">k_means</span><span class="calibre4">.</span><span class="calibre4">labels_</span><span class="calibre4">[::</span><span class="calibre4">10</span><span class="calibre4">])</span>
<span class="calibre4">[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">y_iris</span><span class="calibre4">[::</span><span class="calibre4">10</span><span class="calibre4">])</span>
<span class="calibre4">[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10">Warning</p>
<p class="calibre10">k_means 算法无法保证聚类结果完全绝对真实的反应实际情况。首先，选择正确合适的聚类数量不是一件容易的事情，第二，该算法对初始值的设置敏感，容易陷入局部最优。尽管 scikit-learn 采取了不同的方式来缓解以上问题，目前仍没有完美的解决方案。</p>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
<col width="33%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_cluster_iris.html"><img alt="k_means_iris_bad_init" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000073.jpg" class="calibre66" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_cluster_iris.html"><img alt="k_means_iris_8" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000222.jpg" class="calibre66" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_cluster_iris.html"><img alt="cluster_iris_truth" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000474.jpg" class="calibre66" /></a></td>
</tr>
<tr class="row-odd"><td class="label1"><strong class="calibre14">Bad initialization</strong></td>
<td class="label1"><strong class="calibre14">8 clusters</strong></td>
<td class="label1"><strong class="calibre14">Ground truth</strong></td>
</tr>
</tbody>
</table>
<p class="calibre10"><strong class="calibre14">Don’t over-interpret clustering results（不要过分解读聚类结果）</strong></p>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">Application example: vector quantization（应用案例:向量量化(vector quantization)）</strong></p>
<p class="calibre10">一般来说聚类，特别是 K_means 聚类可以作为一种用少量样本来压缩信息的方式。这种方式就是 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Vector_quantization">vector quantization</a> 。例如，K_means 算法可以用于对一张图片进行色调分离:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">scipy</span> <span class="calibre4">as</span> <span class="calibre4">sp</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">try</span><span class="calibre4">:</span>
<span class="calibre4">... </span>   <span class="calibre4">face</span> <span class="calibre4">=</span> <span class="calibre4">sp</span><span class="calibre4">.</span><span class="calibre4">face</span><span class="calibre4">(</span><span class="calibre4">gray</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">... </span><span class="calibre4">except</span> <span class="calibre4">AttributeError</span><span class="calibre4">:</span>
<span class="calibre4">... </span>   <span class="calibre4">from</span> <span class="calibre4">scipy</span> <span class="calibre4">import</span> <span class="calibre4">misc</span>
<span class="calibre4">... </span>   <span class="calibre4">face</span> <span class="calibre4">=</span> <span class="calibre4">misc</span><span class="calibre4">.</span><span class="calibre4">face</span><span class="calibre4">(</span><span class="calibre4">gray</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">face</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">((</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">))</span> <span class="calibre4"># We need an (n_sample, n_feature) array</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">k_means</span> <span class="calibre4">=</span> <span class="calibre4">cluster</span><span class="calibre4">.</span><span class="calibre4">KMeans</span><span class="calibre4">(</span><span class="calibre4">n_clusters</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">n_init</span><span class="calibre4">=</span><span class="calibre4">1</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">k_means</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span> 
<span class="calibre4">KMeans(algorithm='auto', copy_x=True, init='k-means++', ...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">values</span> <span class="calibre4">=</span> <span class="calibre4">k_means</span><span class="calibre4">.</span><span class="calibre4">cluster_centers_</span><span class="calibre4">.</span><span class="calibre4">squeeze</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">labels</span> <span class="calibre4">=</span> <span class="calibre4">k_means</span><span class="calibre4">.</span><span class="calibre4">labels_</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">face_compressed</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">choose</span><span class="calibre4">(</span><span class="calibre4">labels</span><span class="calibre4">,</span> <span class="calibre4">values</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">face_compressed</span><span class="calibre4">.</span><span class="calibre4">shape</span> <span class="calibre4">=</span> <span class="calibre4">face</span><span class="calibre4">.</span><span class="calibre4">shape</span>
</pre>
</div>
</div>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="25%" class="label"></col>
<col width="25%" class="label"></col>
<col width="25%" class="label"></col>
<col width="25%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_face_compress.html"><img alt="face" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000089.jpg" class="calibre67" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_face_compress.html"><img alt="face_compressed" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000044.jpg" class="calibre67" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_face_compress.html"><img alt="face_regular" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000670.jpg" class="calibre67" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_face_compress.html"><img alt="face_histogram" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000028.jpg" class="calibre67" /></a></td>
</tr>
<tr class="row-odd"><td class="label1">Raw image</td>
<td class="label1">K-means quantization</td>
<td class="label1">Equal bins</td>
<td class="label1">Image histogram</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1036">
<h3 class="sigil_not_in_toc1">分层聚类算法: 谨慎使用</h3>
<dl class="calibre10">
<dt class="calibre18"><a class="calibre3 pcalibre" href="../../modules/clustering.html#hierarchical-clustering"><span class="calibre4">层次聚类</span></a> （分层聚类算法）是一种旨在构建聚类层次结构的分析方法，一般来说，实现该算法的大多数方法有以下两种：</dt>
<dd class="calibre19"><ul class="first6">
<li class="toctree-l"><strong class="calibre14">Agglomerative（聚合）</strong> - 自底向上的方法: 初始阶段，每一个样本将自己作为单独的一个簇，聚类的簇以最小</li>
</ul>
<p class="last">化距离的标准进行迭代聚合。当感兴趣的簇只有少量的样本时，该方法是很合适的。如果需要聚类的
簇数量很大，该方法比K_means算法的计算效率也更高。
* <strong class="calibre14">Divisive（分裂）</strong> - 自顶向下的方法: 初始阶段，所有的样本是一个簇，当一个簇下移时，它被迭代的进
行分裂。当估计聚类簇数量较大的数据时，该算法不仅效率低(由于样本始于一个簇，需要被递归的进行
分裂)，而且从统计学的角度来讲也是不合适的。</p>
</dd>
</dl>
<div class="toctree-wrapper" id="calibre_link-1037">
<h4 class="sigil_not_in_toc1">连接约束聚类</h4>
<p class="calibre2">对于逐次聚合聚类，通过连接图可以指定哪些样本可以被聚合在一个簇。在 scikit 中，图由邻接矩阵来表示，通常该矩阵是一个稀疏矩阵。这种表示方法是非常有用的，例如在聚类图像时检索连接区域(有时也被称为连接要素):</p>
<a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_face_ward_segmentation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_face_ward_segmentation_001.png" class="calibre58" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000107.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>
<span class="calibre4">import</span> <span class="calibre4">matplotlib.pyplot</span> <span class="calibre4">as</span> <span class="calibre4">plt</span>

<span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.image</span> <span class="calibre4">import</span> <span class="calibre4">grid_to_graph</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.cluster</span> <span class="calibre4">import</span> <span class="calibre4">AgglomerativeClustering</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Generate data</span>
<span class="calibre4">try</span><span class="calibre4">:</span>  <span class="calibre4"># SciPy &gt;= 0.16 have face in misc</span>
    <span class="calibre4">from</span> <span class="calibre4">scipy.misc</span> <span class="calibre4">import</span> <span class="calibre4">face</span>
    <span class="calibre4">face</span> <span class="calibre4">=</span> <span class="calibre4">face</span><span class="calibre4">(</span><span class="calibre4">gray</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>
<span class="calibre4">except</span> <span class="calibre4">ImportError</span><span class="calibre4">:</span>
    <span class="calibre4">face</span> <span class="calibre4">=</span> <span class="calibre4">sp</span><span class="calibre4">.</span><span class="calibre4">face</span><span class="calibre4">(</span><span class="calibre4">gray</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span>

<span class="calibre4"># Resize it to 10% of the original size to speed up the processing</span>
<span class="calibre4">face</span> <span class="calibre4">=</span> <span class="calibre4">sp</span><span class="calibre4">.</span><span class="calibre4">misc</span><span class="calibre4">.</span><span class="calibre4">imresize</span><span class="calibre4">(</span><span class="calibre4">face</span><span class="calibre4">,</span> <span class="calibre4">0.10</span><span class="calibre4">)</span> <span class="calibre4">/</span> <span class="calibre4">255.</span>

<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">face</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">))</span>

<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Define the structure A of the data. Pixels connected to their neighbors.</span>
<span class="calibre4">connectivity</span> <span class="calibre4">=</span> <span class="calibre4">grid_to_graph</span><span class="calibre4">(</span><span class="calibre4">*</span><span class="calibre4">face</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">)</span>

<span class="calibre4"># #############################################################################</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1038">
<h4 class="sigil_not_in_toc1">特征聚集</h4>
<p class="calibre2">我们已经知道，稀疏性可以缓解特征维度带来的问题，<em class="calibre13">i.e</em> 即与特征数量相比，样本数量太少。
另一个解决该问题的方式是合并相似的维度：<strong class="calibre14">feature agglomeration（特征聚集）</strong>。该方法可以通过对特征聚类来实现。换
句话说，就是对样本数据转置后进行聚类。</p>
<a class="calibre3 pcalibre" href="../../auto_examples/cluster/plot_digits_agglomeration.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_agglomeration_001.png" class="align-right8" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000024.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">images</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">images</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">images</span><span class="calibre4">,</span> <span class="calibre4">(</span><span class="calibre4">len</span><span class="calibre4">(</span><span class="calibre4">images</span><span class="calibre4">),</span> <span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">))</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">connectivity</span> <span class="calibre4">=</span> <span class="calibre4">grid_to_graph</span><span class="calibre4">(</span><span class="calibre4">*</span><span class="calibre4">images</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">agglo</span> <span class="calibre4">=</span> <span class="calibre4">cluster</span><span class="calibre4">.</span><span class="calibre4">FeatureAgglomeration</span><span class="calibre4">(</span><span class="calibre4">connectivity</span><span class="calibre4">=</span><span class="calibre4">connectivity</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                     <span class="calibre4">n_clusters</span><span class="calibre4">=</span><span class="calibre4">32</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">agglo</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span> 
<span class="calibre4">FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_reduced</span> <span class="calibre4">=</span> <span class="calibre4">agglo</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_approx</span> <span class="calibre4">=</span> <span class="calibre4">agglo</span><span class="calibre4">.</span><span class="calibre4">inverse_transform</span><span class="calibre4">(</span><span class="calibre4">X_reduced</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">images_approx</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">(</span><span class="calibre4">X_approx</span><span class="calibre4">,</span> <span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<div class="toctree-wrapper">
<p class="calibre10"><code class="docutils"><span class="calibre4">transform</span></code> and <code class="docutils"><span class="calibre4">inverse_transform</span></code> methods</p>
<p class="calibre10">Some estimators expose a <code class="docutils"><span class="calibre4">transform</span></code> method, for instance to reduce
the dimensionality of the dataset.</p>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1039">
<h2 class="sigil_not_in_toc">分解: 将一个信号转换成多个成份并且加载</h2>
<div class="toctree-wrapper">
<p class="calibre10"><strong class="calibre14">Components and loadings（成分和载荷）</strong></p>
<p class="calibre10">如果 X 是多维数据，那么我们试图解决的问题是在不同的观察基础上对数据进行重写。我们希望学习得到载荷 L 和成分 C 使得 <em class="calibre13">X = L C</em> 。提取成分 C 有多种不同的方法。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1040">
<h3 class="sigil_not_in_toc1">主成份分析: PCA</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../../modules/decomposition.html#pca"><span class="calibre4">主成分分析（PCA）</span></a> 将能够解释数据信息最大方差的的连续成分提取出来</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../../auto_examples/decomposition/plot_pca_3d.html"><img alt="pca_3d_axis" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000863.jpg" class="align-right5" /></a> <a class="calibre3 pcalibre" href="../../auto_examples/decomposition/plot_pca_3d.html"><img alt="pca_3d_aligned" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000716.jpg" class="align-right5" /></a></p>
<p class="calibre10">上图中样本点的分布在一个方向上是非常平坦的：即三个单变量特征中的任何一个都可以有另外两个特征来表示。主成分分析法(PCA)可以找到使得数据分布不 <em class="calibre13">flat</em> 的矢量方向(可以反映数据主要信息的特征)。</p>
<p class="calibre10">当用主成分分析(PCA)来 <em class="calibre13">transform（转换）</em> 数据时，可以通过在子空间上投影来降低数据的维数。</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Create a signal with only 2 useful dimensions</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">x1</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">normal</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">x2</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">normal</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">100</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">x3</span> <span class="calibre4">=</span> <span class="calibre4">x1</span> <span class="calibre4">+</span> <span class="calibre4">x2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">c_</span><span class="calibre4">[</span><span class="calibre4">x1</span><span class="calibre4">,</span> <span class="calibre4">x2</span><span class="calibre4">,</span> <span class="calibre4">x3</span><span class="calibre4">]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">decomposition</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pca</span> <span class="calibre4">=</span> <span class="calibre4">decomposition</span><span class="calibre4">.</span><span class="calibre4">PCA</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,</span>
<span class="calibre4">  svd_solver='auto', tol=0.0, whiten=False)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">explained_variance_</span><span class="calibre4">)</span>  
<span class="calibre4">[  2.18565811e+00   1.19346747e+00   8.43026679e-32]</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># As we can see, only the 2 first components are useful</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">n_components</span> <span class="calibre4">=</span> <span class="calibre4">2</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_reduced</span> <span class="calibre4">=</span> <span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_reduced</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(100, 2)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1041">
<h3 class="sigil_not_in_toc1">独立成分分析: ICA</h3>
<p class="calibre2"><a class="calibre3 pcalibre" href="../../modules/decomposition.html#ica"><span class="calibre4">独立成分分析（ICA）</span></a> 可以提取数据信息中的独立成分，这些成分载荷的分布包含了最多的
的独立信息。该方法能够恢复 <strong class="calibre14">non-Gaussian（非高斯）</strong> 独立信号:</p>
<a class="calibre3 pcalibre" href="../../auto_examples/decomposition/plot_ica_blind_source_separation.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_ica_blind_source_separation_001.png" class="calibre38" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000036.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Generate sample data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">scipy</span> <span class="calibre4">import</span> <span class="calibre4">signal</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">time</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">linspace</span><span class="calibre4">(</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">10</span><span class="calibre4">,</span> <span class="calibre4">2000</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">s1</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">sin</span><span class="calibre4">(</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">time</span><span class="calibre4">)</span>  <span class="calibre4"># Signal 1 : sinusoidal signal</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">s2</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">sign</span><span class="calibre4">(</span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">sin</span><span class="calibre4">(</span><span class="calibre4">3</span> <span class="calibre4">*</span> <span class="calibre4">time</span><span class="calibre4">))</span>  <span class="calibre4"># Signal 2 : square signal</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">s3</span> <span class="calibre4">=</span> <span class="calibre4">signal</span><span class="calibre4">.</span><span class="calibre4">sawtooth</span><span class="calibre4">(</span><span class="calibre4">2</span> <span class="calibre4">*</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">pi</span> <span class="calibre4">*</span> <span class="calibre4">time</span><span class="calibre4">)</span>  <span class="calibre4"># Signal 3: saw tooth signal</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">S</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">c_</span><span class="calibre4">[</span><span class="calibre4">s1</span><span class="calibre4">,</span> <span class="calibre4">s2</span><span class="calibre4">,</span> <span class="calibre4">s3</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">S</span> <span class="calibre4">+=</span> <span class="calibre4">0.2</span> <span class="calibre4">*</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">random</span><span class="calibre4">.</span><span class="calibre4">normal</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">S</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">)</span>  <span class="calibre4"># Add noise</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">S</span> <span class="calibre4">/=</span> <span class="calibre4">S</span><span class="calibre4">.</span><span class="calibre4">std</span><span class="calibre4">(</span><span class="calibre4">axis</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">)</span>  <span class="calibre4"># Standardize data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Mix data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">A</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">array</span><span class="calibre4">([[</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">0.5</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">],</span> <span class="calibre4">[</span><span class="calibre4">1.5</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">]])</span>  <span class="calibre4"># Mixing matrix</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">dot</span><span class="calibre4">(</span><span class="calibre4">S</span><span class="calibre4">,</span> <span class="calibre4">A</span><span class="calibre4">.</span><span class="calibre4">T</span><span class="calibre4">)</span>  <span class="calibre4"># Generate observations</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4"># Compute ICA</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">ica</span> <span class="calibre4">=</span> <span class="calibre4">decomposition</span><span class="calibre4">.</span><span class="calibre4">FastICA</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">S_</span> <span class="calibre4">=</span> <span class="calibre4">ica</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">)</span>  <span class="calibre4"># Get the estimated sources</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">A_</span> <span class="calibre4">=</span> <span class="calibre4">ica</span><span class="calibre4">.</span><span class="calibre4">mixing_</span><span class="calibre4">.</span><span class="calibre4">T</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">allclose</span><span class="calibre4">(</span><span class="calibre4">X</span><span class="calibre4">,</span>  <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">dot</span><span class="calibre4">(</span><span class="calibre4">S_</span><span class="calibre4">,</span> <span class="calibre4">A_</span><span class="calibre4">)</span> <span class="calibre4">+</span> <span class="calibre4">ica</span><span class="calibre4">.</span><span class="calibre4">mean_</span><span class="calibre4">)</span>
<span class="calibre4">True</span>
</pre>
</div>
</div>
</div>
</div>
</div>


<div class="calibre" id="calibre_link-161">
<h1 class="calibre1">把它们放在一起</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@X</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper" id="calibre_link-1042">
<h2 class="sigil_not_in_toc">模型管道化</h2>
<p class="calibre2">我们已经知道一些模型可以做数据转换，一些模型可以用来预测变量。我们可以建立一个组合模型同时完成以上工作:</p>
<a class="calibre3 pcalibre" href="../../auto_examples/plot_digits_pipe.html"><img alt="http://sklearn.apachecn.org/cn/0.19.0/_images/sphx_glr_plot_digits_pipe_001.png" class="calibre64" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000117.jpg" /></a>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">import</span> <span class="calibre4">matplotlib.pyplot</span> <span class="calibre4">as</span> <span class="calibre4">plt</span>

<span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">linear_model</span><span class="calibre4">,</span> <span class="calibre4">decomposition</span><span class="calibre4">,</span> <span class="calibre4">datasets</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">Pipeline</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>

<span class="calibre4">logistic</span> <span class="calibre4">=</span> <span class="calibre4">linear_model</span><span class="calibre4">.</span><span class="calibre4">LogisticRegression</span><span class="calibre4">()</span>

<span class="calibre4">pca</span> <span class="calibre4">=</span> <span class="calibre4">decomposition</span><span class="calibre4">.</span><span class="calibre4">PCA</span><span class="calibre4">()</span>
<span class="calibre4">pipe</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">(</span><span class="calibre4">steps</span><span class="calibre4">=</span><span class="calibre4">[(</span><span class="calibre4">'pca'</span><span class="calibre4">,</span> <span class="calibre4">pca</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">'logistic'</span><span class="calibre4">,</span> <span class="calibre4">logistic</span><span class="calibre4">)])</span>

<span class="calibre4">digits</span> <span class="calibre4">=</span> <span class="calibre4">datasets</span><span class="calibre4">.</span><span class="calibre4">load_digits</span><span class="calibre4">()</span>
<span class="calibre4">X_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">y_digits</span> <span class="calibre4">=</span> <span class="calibre4">digits</span><span class="calibre4">.</span><span class="calibre4">target</span>

<span class="calibre4"># Plot the PCA spectrum</span>
<span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">)</span>

<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">figure</span><span class="calibre4">(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">figsize</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">))</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">clf</span><span class="calibre4">()</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">axes</span><span class="calibre4">([</span><span class="calibre4">.</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">2</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">7</span><span class="calibre4">,</span> <span class="calibre4">.</span><span class="calibre4">7</span><span class="calibre4">])</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">plot</span><span class="calibre4">(</span><span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">explained_variance_</span><span class="calibre4">,</span> <span class="calibre4">linewidth</span><span class="calibre4">=</span><span class="calibre4">2</span><span class="calibre4">)</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">axis</span><span class="calibre4">(</span><span class="calibre4">'tight'</span><span class="calibre4">)</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">xlabel</span><span class="calibre4">(</span><span class="calibre4">'n_components'</span><span class="calibre4">)</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">ylabel</span><span class="calibre4">(</span><span class="calibre4">'explained_variance_'</span><span class="calibre4">)</span>

<span class="calibre4"># Prediction</span>
<span class="calibre4">n_components</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">20</span><span class="calibre4">,</span> <span class="calibre4">40</span><span class="calibre4">,</span> <span class="calibre4">64</span><span class="calibre4">]</span>
<span class="calibre4">Cs</span> <span class="calibre4">=</span> <span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">logspace</span><span class="calibre4">(</span><span class="calibre4">-</span><span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">4</span><span class="calibre4">,</span> <span class="calibre4">3</span><span class="calibre4">)</span>

<span class="calibre4"># Parameters of pipelines can be set using ‘__’ separated parameter names:</span>
<span class="calibre4">estimator</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">pipe</span><span class="calibre4">,</span>
                         <span class="calibre4">dict</span><span class="calibre4">(</span><span class="calibre4">pca__n_components</span><span class="calibre4">=</span><span class="calibre4">n_components</span><span class="calibre4">,</span>
                              <span class="calibre4">logistic__C</span><span class="calibre4">=</span><span class="calibre4">Cs</span><span class="calibre4">))</span>
<span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_digits</span><span class="calibre4">,</span> <span class="calibre4">y_digits</span><span class="calibre4">)</span>

<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">axvline</span><span class="calibre4">(</span><span class="calibre4">estimator</span><span class="calibre4">.</span><span class="calibre4">best_estimator_</span><span class="calibre4">.</span><span class="calibre4">named_steps</span><span class="calibre4">[</span><span class="calibre4">'pca'</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">n_components</span><span class="calibre4">,</span>
            <span class="calibre4">linestyle</span><span class="calibre4">=</span><span class="calibre4">':'</span><span class="calibre4">,</span> <span class="calibre4">label</span><span class="calibre4">=</span><span class="calibre4">'n_components chosen'</span><span class="calibre4">)</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">legend</span><span class="calibre4">(</span><span class="calibre4">prop</span><span class="calibre4">=</span><span class="calibre4">dict</span><span class="calibre4">(</span><span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">12</span><span class="calibre4">))</span>
<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">show</span><span class="calibre4">()</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1043">
<h2 class="sigil_not_in_toc">用特征面进行人脸识别</h2>
<p class="calibre2">该实例用到的数据集来自 LFW_(Labeled Faces in the Wild)。数据已经进行了初步预处理</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><a class="calibre3 pcalibre" href="http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz">http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz</a> (233MB)</div>
</blockquote>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">"""</span>
<span class="calibre4">===================================================</span>
<span class="calibre4">Faces recognition example using eigenfaces and SVMs</span>
<span class="calibre4">===================================================</span>

<span class="calibre4">The dataset used in this example is a preprocessed excerpt of the</span>
<span class="calibre4">"Labeled Faces in the Wild", aka LFW_:</span>

<span class="calibre4">  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)</span>

<span class="calibre4">.. _LFW: http://vis-www.cs.umass.edu/lfw/</span>

<span class="calibre4">Expected results for the top 5 most represented people in the dataset:</span>

<span class="calibre4">================== ============ ======= ========== =======</span>
<span class="calibre4">                   precision    recall  f1-score   support</span>
<span class="calibre4">================== ============ ======= ========== =======</span>
<span class="calibre4">     Ariel Sharon       0.67      0.92      0.77        13</span>
<span class="calibre4">     Colin Powell       0.75      0.78      0.76        60</span>
<span class="calibre4">  Donald Rumsfeld       0.78      0.67      0.72        27</span>
<span class="calibre4">    George W Bush       0.86      0.86      0.86       146</span>
<span class="calibre4">Gerhard Schroeder       0.76      0.76      0.76        25</span>
<span class="calibre4">      Hugo Chavez       0.67      0.67      0.67        15</span>
<span class="calibre4">       Tony Blair       0.81      0.69      0.75        36</span>

<span class="calibre4">      avg / total       0.80      0.80      0.80       322</span>
<span class="calibre4">================== ============ ======= ========== =======</span>

<span class="calibre4">"""</span>
<span class="calibre4">from</span> <span class="calibre4">__future__</span> <span class="calibre4">import</span> <span class="calibre4">print_function</span>

<span class="calibre4">from</span> <span class="calibre4">time</span> <span class="calibre4">import</span> <span class="calibre4">time</span>
<span class="calibre4">import</span> <span class="calibre4">logging</span>
<span class="calibre4">import</span> <span class="calibre4">matplotlib.pyplot</span> <span class="calibre4">as</span> <span class="calibre4">plt</span>

<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">train_test_split</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_lfw_people</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">classification_report</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.metrics</span> <span class="calibre4">import</span> <span class="calibre4">confusion_matrix</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.decomposition</span> <span class="calibre4">import</span> <span class="calibre4">PCA</span>
<span class="calibre4">from</span> <span class="calibre4">sklearn.svm</span> <span class="calibre4">import</span> <span class="calibre4">SVC</span>


<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">__doc__</span><span class="calibre4">)</span>

<span class="calibre4"># Display progress logs on stdout</span>
<span class="calibre4">logging</span><span class="calibre4">.</span><span class="calibre4">basicConfig</span><span class="calibre4">(</span><span class="calibre4">level</span><span class="calibre4">=</span><span class="calibre4">logging</span><span class="calibre4">.</span><span class="calibre4">INFO</span><span class="calibre4">,</span> <span class="calibre4">format</span><span class="calibre4">=</span><span class="calibre4">'</span><span class="calibre4">%(asctime)s</span><span class="calibre4"> </span><span class="calibre4">%(message)s</span><span class="calibre4">'</span><span class="calibre4">)</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Download the data, if not already on disk and load it as numpy arrays</span>

<span class="calibre4">lfw_people</span> <span class="calibre4">=</span> <span class="calibre4">fetch_lfw_people</span><span class="calibre4">(</span><span class="calibre4">min_faces_per_person</span><span class="calibre4">=</span><span class="calibre4">70</span><span class="calibre4">,</span> <span class="calibre4">resize</span><span class="calibre4">=</span><span class="calibre4">0.4</span><span class="calibre4">)</span>

<span class="calibre4"># introspect the images arrays to find the shapes (for plotting)</span>
<span class="calibre4">n_samples</span><span class="calibre4">,</span> <span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span> <span class="calibre4">=</span> <span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">images</span><span class="calibre4">.</span><span class="calibre4">shape</span>

<span class="calibre4"># for machine learning we use the 2 data directly (as relative pixel</span>
<span class="calibre4"># positions info is ignored by this model)</span>
<span class="calibre4">X</span> <span class="calibre4">=</span> <span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">n_features</span> <span class="calibre4">=</span> <span class="calibre4">X</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span>

<span class="calibre4"># the label to predict is the id of the person</span>
<span class="calibre4">y</span> <span class="calibre4">=</span> <span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">target</span>
<span class="calibre4">target_names</span> <span class="calibre4">=</span> <span class="calibre4">lfw_people</span><span class="calibre4">.</span><span class="calibre4">target_names</span>
<span class="calibre4">n_classes</span> <span class="calibre4">=</span> <span class="calibre4">target_names</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Total dataset size:"</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"n_samples: </span><span class="calibre4">%d</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">n_samples</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"n_features: </span><span class="calibre4">%d</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">n_features</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"n_classes: </span><span class="calibre4">%d</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">n_classes</span><span class="calibre4">)</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Split into a training set and a test set using a stratified k fold</span>

<span class="calibre4"># split into a training and testing set</span>
<span class="calibre4">X_train</span><span class="calibre4">,</span> <span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">,</span> <span class="calibre4">y_test</span> <span class="calibre4">=</span> <span class="calibre4">train_test_split</span><span class="calibre4">(</span>
    <span class="calibre4">X</span><span class="calibre4">,</span> <span class="calibre4">y</span><span class="calibre4">,</span> <span class="calibre4">test_size</span><span class="calibre4">=</span><span class="calibre4">0.25</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">42</span><span class="calibre4">)</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled</span>
<span class="calibre4"># dataset): unsupervised feature extraction / dimensionality reduction</span>
<span class="calibre4">n_components</span> <span class="calibre4">=</span> <span class="calibre4">150</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Extracting the top </span><span class="calibre4">%d</span><span class="calibre4"> eigenfaces from </span><span class="calibre4">%d</span><span class="calibre4"> faces"</span>
      <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">n_components</span><span class="calibre4">,</span> <span class="calibre4">X_train</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]))</span>
<span class="calibre4">t0</span> <span class="calibre4">=</span> <span class="calibre4">time</span><span class="calibre4">()</span>
<span class="calibre4">pca</span> <span class="calibre4">=</span> <span class="calibre4">PCA</span><span class="calibre4">(</span><span class="calibre4">n_components</span><span class="calibre4">=</span><span class="calibre4">n_components</span><span class="calibre4">,</span> <span class="calibre4">svd_solver</span><span class="calibre4">=</span><span class="calibre4">'randomized'</span><span class="calibre4">,</span>
          <span class="calibre4">whiten</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"done in </span><span class="calibre4">%0.3f</span><span class="calibre4">s"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">time</span><span class="calibre4">()</span> <span class="calibre4">-</span> <span class="calibre4">t0</span><span class="calibre4">))</span>

<span class="calibre4">eigenfaces</span> <span class="calibre4">=</span> <span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">components_</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">((</span><span class="calibre4">n_components</span><span class="calibre4">,</span> <span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span><span class="calibre4">))</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Projecting the input data on the eigenfaces orthonormal basis"</span><span class="calibre4">)</span>
<span class="calibre4">t0</span> <span class="calibre4">=</span> <span class="calibre4">time</span><span class="calibre4">()</span>
<span class="calibre4">X_train_pca</span> <span class="calibre4">=</span> <span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_train</span><span class="calibre4">)</span>
<span class="calibre4">X_test_pca</span> <span class="calibre4">=</span> <span class="calibre4">pca</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"done in </span><span class="calibre4">%0.3f</span><span class="calibre4">s"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">time</span><span class="calibre4">()</span> <span class="calibre4">-</span> <span class="calibre4">t0</span><span class="calibre4">))</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Train a SVM classification model</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Fitting the classifier to the training set"</span><span class="calibre4">)</span>
<span class="calibre4">t0</span> <span class="calibre4">=</span> <span class="calibre4">time</span><span class="calibre4">()</span>
<span class="calibre4">param_grid</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'C'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">1e3</span><span class="calibre4">,</span> <span class="calibre4">5e3</span><span class="calibre4">,</span> <span class="calibre4">1e4</span><span class="calibre4">,</span> <span class="calibre4">5e4</span><span class="calibre4">,</span> <span class="calibre4">1e5</span><span class="calibre4">],</span>
              <span class="calibre4">'gamma'</span><span class="calibre4">:</span> <span class="calibre4">[</span><span class="calibre4">0.0001</span><span class="calibre4">,</span> <span class="calibre4">0.0005</span><span class="calibre4">,</span> <span class="calibre4">0.001</span><span class="calibre4">,</span> <span class="calibre4">0.005</span><span class="calibre4">,</span> <span class="calibre4">0.01</span><span class="calibre4">,</span> <span class="calibre4">0.1</span><span class="calibre4">],</span> <span class="calibre4">}</span>
<span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">SVC</span><span class="calibre4">(</span><span class="calibre4">kernel</span><span class="calibre4">=</span><span class="calibre4">'rbf'</span><span class="calibre4">,</span> <span class="calibre4">class_weight</span><span class="calibre4">=</span><span class="calibre4">'balanced'</span><span class="calibre4">),</span> <span class="calibre4">param_grid</span><span class="calibre4">)</span>
<span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train_pca</span><span class="calibre4">,</span> <span class="calibre4">y_train</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"done in </span><span class="calibre4">%0.3f</span><span class="calibre4">s"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">time</span><span class="calibre4">()</span> <span class="calibre4">-</span> <span class="calibre4">t0</span><span class="calibre4">))</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Best estimator found by grid search:"</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">best_estimator_</span><span class="calibre4">)</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Quantitative evaluation of the model quality on the test set</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"Predicting people's names on the test set"</span><span class="calibre4">)</span>
<span class="calibre4">t0</span> <span class="calibre4">=</span> <span class="calibre4">time</span><span class="calibre4">()</span>
<span class="calibre4">y_pred</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_test_pca</span><span class="calibre4">)</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"done in </span><span class="calibre4">%0.3f</span><span class="calibre4">s"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">time</span><span class="calibre4">()</span> <span class="calibre4">-</span> <span class="calibre4">t0</span><span class="calibre4">))</span>

<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">classification_report</span><span class="calibre4">(</span><span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">target_names</span><span class="calibre4">=</span><span class="calibre4">target_names</span><span class="calibre4">))</span>
<span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">labels</span><span class="calibre4">=</span><span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">n_classes</span><span class="calibre4">)))</span>


<span class="calibre4"># #############################################################################</span>
<span class="calibre4"># Qualitative evaluation of the predictions using matplotlib</span>

<span class="calibre4">def</span> <span class="calibre4">plot_gallery</span><span class="calibre4">(</span><span class="calibre4">images</span><span class="calibre4">,</span> <span class="calibre4">titles</span><span class="calibre4">,</span> <span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span><span class="calibre4">,</span> <span class="calibre4">n_row</span><span class="calibre4">=</span><span class="calibre4">3</span><span class="calibre4">,</span> <span class="calibre4">n_col</span><span class="calibre4">=</span><span class="calibre4">4</span><span class="calibre4">):</span>
    <span class="calibre4">"""Helper function to plot a gallery of portraits"""</span>
    <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">figure</span><span class="calibre4">(</span><span class="calibre4">figsize</span><span class="calibre4">=</span><span class="calibre4">(</span><span class="calibre4">1.8</span> <span class="calibre4">*</span> <span class="calibre4">n_col</span><span class="calibre4">,</span> <span class="calibre4">2.4</span> <span class="calibre4">*</span> <span class="calibre4">n_row</span><span class="calibre4">))</span>
    <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">subplots_adjust</span><span class="calibre4">(</span><span class="calibre4">bottom</span><span class="calibre4">=</span><span class="calibre4">0</span><span class="calibre4">,</span> <span class="calibre4">left</span><span class="calibre4">=.</span><span class="calibre4">01</span><span class="calibre4">,</span> <span class="calibre4">right</span><span class="calibre4">=.</span><span class="calibre4">99</span><span class="calibre4">,</span> <span class="calibre4">top</span><span class="calibre4">=.</span><span class="calibre4">90</span><span class="calibre4">,</span> <span class="calibre4">hspace</span><span class="calibre4">=.</span><span class="calibre4">35</span><span class="calibre4">)</span>
    <span class="calibre4">for</span> <span class="calibre4">i</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">n_row</span> <span class="calibre4">*</span> <span class="calibre4">n_col</span><span class="calibre4">):</span>
        <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">subplot</span><span class="calibre4">(</span><span class="calibre4">n_row</span><span class="calibre4">,</span> <span class="calibre4">n_col</span><span class="calibre4">,</span> <span class="calibre4">i</span> <span class="calibre4">+</span> <span class="calibre4">1</span><span class="calibre4">)</span>
        <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">imshow</span><span class="calibre4">(</span><span class="calibre4">images</span><span class="calibre4">[</span><span class="calibre4">i</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">reshape</span><span class="calibre4">((</span><span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span><span class="calibre4">)),</span> <span class="calibre4">cmap</span><span class="calibre4">=</span><span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">cm</span><span class="calibre4">.</span><span class="calibre4">gray</span><span class="calibre4">)</span>
        <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">title</span><span class="calibre4">(</span><span class="calibre4">titles</span><span class="calibre4">[</span><span class="calibre4">i</span><span class="calibre4">],</span> <span class="calibre4">size</span><span class="calibre4">=</span><span class="calibre4">12</span><span class="calibre4">)</span>
        <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">xticks</span><span class="calibre4">(())</span>
        <span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">yticks</span><span class="calibre4">(())</span>


<span class="calibre4"># plot the result of the prediction on a portion of the test set</span>

<span class="calibre4">def</span> <span class="calibre4">title</span><span class="calibre4">(</span><span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">target_names</span><span class="calibre4">,</span> <span class="calibre4">i</span><span class="calibre4">):</span>
    <span class="calibre4">pred_name</span> <span class="calibre4">=</span> <span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">y_pred</span><span class="calibre4">[</span><span class="calibre4">i</span><span class="calibre4">]]</span><span class="calibre4">.</span><span class="calibre4">rsplit</span><span class="calibre4">(</span><span class="calibre4">' '</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">)[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">]</span>
    <span class="calibre4">true_name</span> <span class="calibre4">=</span> <span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">y_test</span><span class="calibre4">[</span><span class="calibre4">i</span><span class="calibre4">]]</span><span class="calibre4">.</span><span class="calibre4">rsplit</span><span class="calibre4">(</span><span class="calibre4">' '</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">)[</span><span class="calibre4">-</span><span class="calibre4">1</span><span class="calibre4">]</span>
    <span class="calibre4">return</span> <span class="calibre4">'predicted: </span><span class="calibre4">%s</span><span class="calibre4">\n</span><span class="calibre4">true:      </span><span class="calibre4">%s</span><span class="calibre4">'</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">pred_name</span><span class="calibre4">,</span> <span class="calibre4">true_name</span><span class="calibre4">)</span>

<span class="calibre4">prediction_titles</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">title</span><span class="calibre4">(</span><span class="calibre4">y_pred</span><span class="calibre4">,</span> <span class="calibre4">y_test</span><span class="calibre4">,</span> <span class="calibre4">target_names</span><span class="calibre4">,</span> <span class="calibre4">i</span><span class="calibre4">)</span>
                     <span class="calibre4">for</span> <span class="calibre4">i</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">y_pred</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">])]</span>

<span class="calibre4">plot_gallery</span><span class="calibre4">(</span><span class="calibre4">X_test</span><span class="calibre4">,</span> <span class="calibre4">prediction_titles</span><span class="calibre4">,</span> <span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span><span class="calibre4">)</span>

<span class="calibre4"># plot the gallery of the most significative eigenfaces</span>

<span class="calibre4">eigenface_titles</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">"eigenface </span><span class="calibre4">%d</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">i</span> <span class="calibre4">for</span> <span class="calibre4">i</span> <span class="calibre4">in</span> <span class="calibre4">range</span><span class="calibre4">(</span><span class="calibre4">eigenfaces</span><span class="calibre4">.</span><span class="calibre4">shape</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">])]</span>
<span class="calibre4">plot_gallery</span><span class="calibre4">(</span><span class="calibre4">eigenfaces</span><span class="calibre4">,</span> <span class="calibre4">eigenface_titles</span><span class="calibre4">,</span> <span class="calibre4">h</span><span class="calibre4">,</span> <span class="calibre4">w</span><span class="calibre4">)</span>

<span class="calibre4">plt</span><span class="calibre4">.</span><span class="calibre4">show</span><span class="calibre4">()</span>
</pre>
</div>
</div>
<table border="1" class="docutils1">
<colgroup class="calibre21">
<col width="50%" class="label"></col>
<col width="50%" class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><td class="label1"><a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/cn/0.19.0/_images/plot_face_recognition_1.png"><img alt="prediction" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000726.jpg" class="calibre68" /></a></td>
<td class="label1"><a class="calibre3 pcalibre" href="http://sklearn.apachecn.org/cn/0.19.0/_images/plot_face_recognition_2.png"><img alt="eigenfaces" src="https://raw.githubusercontent.com/iOSDevLog/100-days-of-ai/master/r1-ml/day7-scikit-learn-0.19-zh/images/000111.jpg" class="calibre68" /></a></td>
</tr>
<tr class="row-odd"><td class="label1"><strong class="calibre14">Prediction</strong></td>
<td class="label1"><strong class="calibre14">Eigenfaces</strong></td>
</tr>
</tbody>
</table>
<p class="calibre10">数据集中前5名最有代表性样本的预期结果:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>                   <span class="calibre4">precision</span>    <span class="calibre4">recall</span>  <span class="calibre4">f1</span><span class="calibre4">-</span><span class="calibre4">score</span>   <span class="calibre4">support</span>

<span class="calibre4">Gerhard_Schroeder</span>       <span class="calibre4">0.91</span>      <span class="calibre4">0.75</span>      <span class="calibre4">0.82</span>        <span class="calibre4">28</span>
  <span class="calibre4">Donald_Rumsfeld</span>       <span class="calibre4">0.84</span>      <span class="calibre4">0.82</span>      <span class="calibre4">0.83</span>        <span class="calibre4">33</span>
       <span class="calibre4">Tony_Blair</span>       <span class="calibre4">0.65</span>      <span class="calibre4">0.82</span>      <span class="calibre4">0.73</span>        <span class="calibre4">34</span>
     <span class="calibre4">Colin_Powell</span>       <span class="calibre4">0.78</span>      <span class="calibre4">0.88</span>      <span class="calibre4">0.83</span>        <span class="calibre4">58</span>
    <span class="calibre4">George_W_Bush</span>       <span class="calibre4">0.93</span>      <span class="calibre4">0.86</span>      <span class="calibre4">0.90</span>       <span class="calibre4">129</span>

      <span class="calibre4">avg</span> <span class="calibre4">/</span> <span class="calibre4">total</span>       <span class="calibre4">0.86</span>      <span class="calibre4">0.84</span>      <span class="calibre4">0.85</span>       <span class="calibre4">282</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1044">
<h2 class="sigil_not_in_toc">开放性问题: 股票市场结构</h2>
<p class="calibre2">我们可以预测 Google 在特定时间段内的股价变动吗？</p>
<p class="calibre10"><a class="calibre3 pcalibre" href="../../auto_examples/applications/plot_stock_market.html#stock-market"><span class="calibre4">Learning a graph structure</span></a></p>
</div>
</div>


<div class="calibre" id="calibre_link-244">
<h1 class="calibre1">寻求帮助</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@片刻</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@X</a><br class="calibre9" />
    </div>
<div class="toctree-wrapper" id="calibre_link-1045">
<h2 class="sigil_not_in_toc">项目邮件列表</h2>
<p class="calibre2">如果您在使用 scikit 的过程中发现错误或者需要在说明文档中澄清的内容，可以随时通过
<a class="calibre3 pcalibre" href="http://scikit-learn.org/stable/support.html">Mailing List</a> 进行咨询。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1046">
<h2 class="sigil_not_in_toc">机器学习从业者的 Q&amp;A 社区</h2>
<blockquote class="calibre15">
<div class="toctree-wrapper"><table class="docutils1" frame="void" rules="none">
<colgroup class="calibre21"><col class="label"></col>
<col class="label"></col>
</colgroup>
<tbody valign="top" class="calibre22">
<tr class="calibre23"><th class="head">Quora.com:</th>
<td class="label1">Quora有一个和机器学习问题相关的主题以及一些有趣的讨论：
<a class="calibre3 pcalibre" href="https://www.quora.com/topic/Machine-Learning">https://www.quora.com/topic/Machine-Learning</a></td>
</tr>
<tr class="row-odd"><th class="head">Stack Exchange:</th>
<td class="label1">Stack Exchange 系列网站包含 <a href="#calibre_link-245" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-1047">`multiple subdomains for Machine Learning questions（机器学习问题的多个分支）`_</span></a>。</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<p class="calibre10">&ndash; _’斯坦福大学教授 Andrew Ng 教授的机器学习优秀免费在线课程’: <a class="calibre3 pcalibre" href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></p>
<p class="calibre10">&ndash; _’另一个优秀的免费在线课程，对人工智能采取更一般的方法’: <a class="calibre3 pcalibre" href="https://www.udacity.com/course/intro-to-artificial-intelligence--cs271">https://www.udacity.com/course/intro-to-artificial-intelligence&ndash;cs271</a></p>
</div>
</div>


<div class="calibre" id="calibre_link-0">
<span id="calibre_link-1048" class="calibre4"></span><h1 class="calibre5">处理文本数据</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/NellyLuo" class="calibre3 pcalibre">@NellyLuo</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@那伊抹微笑</a><br class="calibre9" />
                 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@微光同尘</a><br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@Lielei</a><br class="calibre9" />  
    </div>
<p class="calibre10">本指南旨在一个单独实际任务中探索一些主要的 <code class="docutils"><span class="calibre4">scikit-learn</span></code> 工具: 分析关于 20 个不同主题的一个文件集合（新闻组帖子）。</p>
<p class="calibre10">在本节中，我们将会学习如何:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l">读取文件内容以及所属的类别</li>
<li class="toctree-l">提取合适于机器学习的特征向量</li>
<li class="toctree-l">训练一个线性模型来进行分类</li>
<li class="toctree-l">使用网格搜索策略找到特征提取组件和分类器的最佳配置</li>
</ul>
</div>
</blockquote>
<div class="toctree-wrapper" id="calibre_link-1049">
<h2 class="sigil_not_in_toc">教程设置</h2>
<p class="calibre2">开始这篇教程之前，你必须首先安装 <em class="calibre13">scikit-learn</em> 以及所有其要求的库。</p>
<p class="calibre10">更多信息和系统安装指导请参考 <a class="calibre3 pcalibre" href="../../install.html#installation-instructions"><span class="calibre4">安装说明</span></a> 。</p>
<p class="calibre10">这篇入门教程的源代码可以在你的 scikit-learn 文件夹下面找到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">scikit</span><span class="calibre4">-</span><span class="calibre4">learn</span><span class="calibre4">/</span><span class="calibre4">doc</span><span class="calibre4">/</span><span class="calibre4">tutorial</span><span class="calibre4">/</span><span class="calibre4">text_analytics</span><span class="calibre4">/</span>
</pre>
</div>
</div>
<p class="calibre10">这个入门教程包含以下的子文件夹:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ul class="calibre6">
<li class="toctree-l"><code class="docutils"><span class="calibre4">*.rst</span> <span class="calibre4">files</span></code> - 用 sphinx 编写的该教程的源代码</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">data</span></code> - 用来存放在该教程中用到的数据集的文件夹</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">skeletons</span></code> - 用来练习的未完成的示例脚本</li>
<li class="toctree-l"><code class="docutils"><span class="calibre4">solutions</span></code> - 练习的答案</li>
</ul>
</div>
</blockquote>
<p class="calibre10">你也可以将这个文件结构拷贝到您的电脑的硬盘里名为 <code class="docutils"><span class="calibre4">sklearn_tut_workspace</span></code> 的文件夹中来编辑你自己的文件完成练习，同时保持原有文件结构不变:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">%</span> <span class="calibre4">cp</span> <span class="calibre4">-</span><span class="calibre4">r</span> <span class="calibre4">skeletons</span> <span class="calibre4">work_directory</span><span class="calibre4">/</span><span class="calibre4">sklearn_tut_workspace</span>
</pre>
</div>
</div>
<p class="calibre10">机器学习算法需要数据。
进入每一个 <code class="docutils"><span class="calibre4">$TUTORIAL_HOME/data</span></code> 子文件夹，然后运行 <code class="docutils"><span class="calibre4">fetch_data.py</span></code> 脚本（需要您先读取这些文件）。</p>
<p class="calibre10">例如:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span>% cd $TUTORIAL_HOME/data/languages
% less fetch_data.py
% python fetch_data.py
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1050">
<h2 class="sigil_not_in_toc">加载这 20 个新闻组的数据集</h2>
<p class="calibre2">该数据集名为 “Twenty Newsgroups” 。
下面是这个数据集的官方介绍, 引自 <a class="calibre3 pcalibre" href="http://people.csail.mit.edu/jrennie/20Newsgroups/">网站</a>:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper">Twenty Newsgroups 数据集是一个包括近 20,000 个新闻组文档的集合，（几乎）平均分成了 20 个不同新闻组。
据我们所知，这最初是由 Ken Lang 收集的 ，很可能是为了他的论文 “Newsweeder: Learning to filter netnews,” 尽管他没有明确提及这个集合。
这 20 个新闻组集合已成为一个流行的数据集，用于机器学习中的文本应用的试验中，如文本分类和文本聚类。</div>
</blockquote>
<p class="calibre10">接下来我们会使用 scikit-learn 中的这个内置数据集加载器来加载这 20 个新闻组。
或者，您也可以手动从网站上下载数据集，使用 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.datasets.load_files.html#sklearn.datasets.load_files" title="sklearn.datasets.load_files"><code class="docutils"><span class="calibre4">sklearn.datasets.load_files</span></code></a> 功能，并将其指向未压缩文件夹下的 <code class="docutils"><span class="calibre4">20news-bydate-train</span></code> 子文件夹。</p>
<p class="calibre10">在第一个示例中，为了节约时间，我们将使用部分数据：从 20 个类别的数据集中选出 4 个来进行训练:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">categories</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'alt.atheism'</span><span class="calibre4">,</span> <span class="calibre4">'soc.religion.christian'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>              <span class="calibre4">'comp.graphics'</span><span class="calibre4">,</span> <span class="calibre4">'sci.med'</span><span class="calibre4">]</span>
</pre>
</div>
</div>
<p class="calibre10">如下所示，我们现在能够加载对应这些类别的文件列表:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.datasets</span> <span class="calibre4">import</span> <span class="calibre4">fetch_20newsgroups</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">twenty_train</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'train'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">,</span> <span class="calibre4">shuffle</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">42</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">返回的数据集是一个 <code class="docutils"><span class="calibre4">scikit-learn</span></code> “bunch”: 一个简单的包含多个 “field” 的存储对象，可以方便的使用 python 中的 <code class="docutils"><span class="calibre4">dict</span></code> keys 或 <code class="docutils"><span class="calibre4">object</span></code> 属性来读取, 比如 <code class="docutils"><span class="calibre4">target_names</span></code> 包含了所请求的类别名称:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span>
<span class="calibre4">['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']</span>
</pre>
</div>
</div>
<p class="calibre10">这些文件本身被读进内存的 <code class="docutils"><span class="calibre4">data</span></code> 属性中。
另外，这些文件名称也可以容易获取到:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">len</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">2257</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">len</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">filenames</span><span class="calibre4">)</span>
<span class="calibre4">2257</span>
</pre>
</div>
</div>
<p class="calibre10">让我们打印出所加载的第一个文件的前几行:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">\n</span><span class="calibre4">"</span><span class="calibre4">.</span><span class="calibre4">join</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]</span><span class="calibre4">.</span><span class="calibre4">split</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">\n</span><span class="calibre4">"</span><span class="calibre4">)[:</span><span class="calibre4">3</span><span class="calibre4">]))</span>
<span class="calibre4">From: sd345@city.ac.uk (Michael Collier)</span>
<span class="calibre4">Subject: Converting images to HP LaserJet III?</span>
<span class="calibre4">Nntp-Posting-Host: hampton</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[</span><span class="calibre4">0</span><span class="calibre4">]])</span>
<span class="calibre4">comp.graphics</span>
</pre>
</div>
</div>
<p class="calibre10">监督学习需要让训练集中的每个文档对应一个类别标签。
在这个例子中，类别是每个新闻组的名称，也刚好是每个储存文本文件的文件夹的名称。</p>
<p class="calibre10">由于速度和空间上效率的原因 <code class="docutils"><span class="calibre4">scikit-learn</span></code> 加载目标属性为一个整型数列，
它与 <code class="docutils"><span class="calibre4">target_names</span></code> 列表中类别名称的 index（索引）相对应。
每个样本的类别的整数型 id 存放在 <code class="docutils"><span class="calibre4">target</span></code> 属性中:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">10</span><span class="calibre4">]</span>
<span class="calibre4">array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])</span>
</pre>
</div>
</div>
<p class="calibre10">也可以通过如下方式取得类别名称:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">t</span> <span class="calibre4">in</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">10</span><span class="calibre4">]:</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">t</span><span class="calibre4">])</span>
<span class="calibre4">...</span>
<span class="calibre4">comp.graphics</span>
<span class="calibre4">comp.graphics</span>
<span class="calibre4">soc.religion.christian</span>
<span class="calibre4">soc.religion.christian</span>
<span class="calibre4">soc.religion.christian</span>
<span class="calibre4">soc.religion.christian</span>
<span class="calibre4">soc.religion.christian</span>
<span class="calibre4">sci.med</span>
<span class="calibre4">sci.med</span>
<span class="calibre4">sci.med</span>
</pre>
</div>
</div>
<p class="calibre10">你可以发现所有的样本都被随机打乱（使用了修正的 RNG 种子）: 当你在重新训练整个数据集之前，这样可以帮助你只选取前几个样本来快速训练一个模型以及获得初步结果。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1051">
<h2 class="sigil_not_in_toc">从文本文件中提取特征</h2>
<p class="calibre2">为了在文本文件中执行机器学习算法, 我们首先要做的是将文本内容转化成数值形式的特征向量。</p>
<div class="toctree-wrapper" id="calibre_link-1052">
<h3 class="sigil_not_in_toc1">词袋</h3>
<p class="calibre2">最直观的方法就是用词袋来表示:</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><ol class="arabic">
<li class="toctree-l">在训练集中每一个出现在任意文中的单词分配一个特定的整数 id（比如，通过建立一个从单词到整数索引的字典）。</li>
<li class="toctree-l">对于每个文档 <code class="docutils"><span class="calibre4">#i</span></code>，计算每个单词 <code class="docutils"><span class="calibre4">w</span></code> 的出现次数并将其存储在 <code class="docutils"><span class="calibre4">X[i,</span> <span class="calibre4">j]</span></code> 中作为特征 <code class="docutils"><span class="calibre4">#j</span></code> 的值，其中 <code class="docutils"><span class="calibre4">j</span></code> 是在字典中词 <code class="docutils"><span class="calibre4">w</span></code> 的索引。</li>
</ol>
</div>
</blockquote>
<p class="calibre10">在这种方法中 <code class="docutils"><span class="calibre4">n_features</span></code> 是在整个文集（文章集合的缩写，下同）中不同单词的数量: 这个值一般来说超过 100,000 。</p>
<p class="calibre10">如果 <code class="docutils"><span class="calibre4">n_samples</span> <span class="calibre4">==</span> <span class="calibre4">10000</span></code> ， 存储 <code class="docutils"><span class="calibre4">X</span></code> 为 “float32” 型的 numpy 数组将会需要 10000 x 100000 x 4 bytes = <strong class="calibre14">4GB内存</strong> ，在当前的计算机中非常不好管理的。</p>
<p class="calibre10">幸运的是, <strong class="calibre14">X 数组中大多数的值为 0</strong> ，是因为特定的文档中使用的单词数量远远少于总体的词袋单词个数。 因此我们可以称词袋模型是典型的 <strong class="calibre14">high-dimensional sparse datasets（高维稀疏数据集）</strong> 。 我们可以通过只在内存中保存特征向量中非 0 的部分以节省大量内存。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">scipy.sparse</span></code> 矩阵正是能完成上述操作的数据结构，同时 <code class="docutils"><span class="calibre4">scikit-learn</span></code> 有对这样的数据结构的内置支持。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1053">
<h3 class="sigil_not_in_toc1">使用 <code class="docutils2"><span class="calibre4">scikit-learn</span></code> 来对文本进行分词</h3>
<p class="calibre2">文本的预处理, 分词以及过滤停用词都包含在可以构建特征字典和将文档转换成特征向量的高级组件中</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">CountVectorizer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">count_vect</span> <span class="calibre4">=</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_counts</span> <span class="calibre4">=</span> <span class="calibre4">count_vect</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_counts</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(2257, 35788)</span>
</pre>
</div>
</div>
<p class="calibre10"><a class="calibre3 pcalibre" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 提供了 N-gram 模型以及连续字符模型。
一旦拟合， 向量化程序就会构建一个包含特征索引的字典:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">count_vect</span><span class="calibre4">.</span><span class="calibre4">vocabulary_</span><span class="calibre4">.</span><span class="calibre4">get</span><span class="calibre4">(</span><span class="calibre4">u</span><span class="calibre4">'algorithm'</span><span class="calibre4">)</span>
<span class="calibre4">4690</span>
</pre>
</div>
</div>
<p class="calibre10">在词汇表中一个单词的索引值对应的是该单词在整个训练的文集中出现的频率。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1054">
<h3 class="sigil_not_in_toc1">从出现次数到出现频率</h3>
<p class="calibre2">出现次数的统计是非常好的开始，但是有个问题：长的文本相对于短的文本有更高的单词平均出现次数，尽管他们可能在描述同一个主题。</p>
<p class="calibre10">为了避免这些潜在的差异，可以将文档中每个单词的出现次数除以文档中单词的总数：这些新的特征称之为词频 <code class="docutils"><span class="calibre4">tf</span></code> （Term Frequencies）。</p>
<p class="calibre10">另一个在词频的基础上改良是，降低在语料库的很多文档中均出现的单词的权重，因此可以突出那些仅在语料库中一小部分文档中出现的单词的信息量。</p>
<p class="calibre10">这种方法称为 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Tf–idf">tf&ndash;idf</a> ，全称为 “Term Frequency times Inverse Document Frequency” 。</p>
<p class="calibre10"><strong class="calibre14">tf</strong> 和 <strong class="calibre14">tf&ndash;idf</strong> 都可以按照下面的方式计算:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.feature_extraction.text</span> <span class="calibre4">import</span> <span class="calibre4">TfidfTransformer</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tf_transformer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">(</span><span class="calibre4">use_idf</span><span class="calibre4">=</span><span class="calibre4">False</span><span class="calibre4">)</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train_counts</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_tf</span> <span class="calibre4">=</span> <span class="calibre4">tf_transformer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_train_counts</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_tf</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(2257, 35788)</span>
</pre>
</div>
</div>
<p class="calibre10">在上面的样例代码中，我们首先使用了 <code class="docutils"><span class="calibre4">fit(..)</span></code> 方法来拟合数据的 estimator（估算器），接下来使用 <code class="docutils"><span class="calibre4">transform(..)</span></code> 方法来把我们的次数矩阵转换成 <cite class="calibre13">tf-idf</cite> 型。
通过跳过冗余处理，这两步可以结合起来，来更快地得到同样的结果。这种操作可以使用上节提到过的 <code class="docutils"><span class="calibre4">fit_transform(..)</span></code> 方法来完成，如下:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">tfidf_transformer</span> <span class="calibre4">=</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">()</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_tfidf</span> <span class="calibre4">=</span> <span class="calibre4">tfidf_transformer</span><span class="calibre4">.</span><span class="calibre4">fit_transform</span><span class="calibre4">(</span><span class="calibre4">X_train_counts</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_train_tfidf</span><span class="calibre4">.</span><span class="calibre4">shape</span>
<span class="calibre4">(2257, 35788)</span>
</pre>
</div>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1055">
<h2 class="sigil_not_in_toc">训练分类器</h2>
<p class="calibre2">现在我们有了我们的特征，我们可以训练一个分类器来预测一个帖子所属的类别。
让我们从 <a class="calibre3 pcalibre" href="../../modules/naive_bayes.html#naive-bayes"><span class="calibre4">朴素贝叶斯</span></a> 分类器开始. 该分类器为该任务提供了一个好的基线（baseline）.
<code class="docutils"><span class="calibre4">scikit-learn</span></code> 包含了该分类器的若干变种；最适用在该问题上的变种是多项式分类器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.naive_bayes</span> <span class="calibre4">import</span> <span class="calibre4">MultinomialNB</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">clf</span> <span class="calibre4">=</span> <span class="calibre4">MultinomialNB</span><span class="calibre4">()</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">X_train_tfidf</span><span class="calibre4">,</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">为了尝试预测新文档所属的类别，我们需要使用和之前同样的步骤来抽取特征。
不同之处在于，我们在transformer调用 <code class="docutils"><span class="calibre4">transform</span></code> 而不是 <code class="docutils"><span class="calibre4">fit_transform</span></code> ，因为这些特征已经在训练集上进行拟合了:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">docs_new</span> <span class="calibre4">=</span> <span class="calibre4">[</span><span class="calibre4">'God is love'</span><span class="calibre4">,</span> <span class="calibre4">'OpenGL on the GPU is fast'</span><span class="calibre4">]</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new_counts</span> <span class="calibre4">=</span> <span class="calibre4">count_vect</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">docs_new</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">X_new_tfidf</span> <span class="calibre4">=</span> <span class="calibre4">tfidf_transformer</span><span class="calibre4">.</span><span class="calibre4">transform</span><span class="calibre4">(</span><span class="calibre4">X_new_counts</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">predicted</span> <span class="calibre4">=</span> <span class="calibre4">clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">X_new_tfidf</span><span class="calibre4">)</span>

<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">doc</span><span class="calibre4">,</span> <span class="calibre4">category</span> <span class="calibre4">in</span> <span class="calibre4">zip</span><span class="calibre4">(</span><span class="calibre4">docs_new</span><span class="calibre4">,</span> <span class="calibre4">predicted</span><span class="calibre4">):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">'</span><span class="calibre4">%r</span><span class="calibre4"> =&gt; </span><span class="calibre4">%s</span><span class="calibre4">'</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">doc</span><span class="calibre4">,</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">category</span><span class="calibre4">]))</span>
<span class="calibre4">...</span>
<span class="calibre4">'God is love' =&gt; soc.religion.christian</span>
<span class="calibre4">'OpenGL on the GPU is fast' =&gt; comp.graphics</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1056">
<h2 class="sigil_not_in_toc">构建 Pipeline（管道）</h2>
<p class="calibre2">为了使得 向量化（vectorizer） =&gt; 转换器（transformer） =&gt; 分类器（classifier） 过程更加简单,``scikit-learn`` 提供了一个 <code class="docutils"><span class="calibre4">Pipeline</span></code> 类，操作起来像一个复合分类器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.pipeline</span> <span class="calibre4">import</span> <span class="calibre4">Pipeline</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text_clf</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([(</span><span class="calibre4">'vect'</span><span class="calibre4">,</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">()),</span>
<span class="calibre4">... </span>                     <span class="calibre4">(</span><span class="calibre4">'tfidf'</span><span class="calibre4">,</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">()),</span>
<span class="calibre4">... </span>                     <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">MultinomialNB</span><span class="calibre4">()),</span>
<span class="calibre4">... </span><span class="calibre4">])</span>
</pre>
</div>
</div>
<p class="calibre10">名称 <code class="docutils"><span class="calibre4">vect</span></code>, <code class="docutils"><span class="calibre4">tfidf</span></code> 和 <code class="docutils"><span class="calibre4">clf</span></code> （分类器）都是任意的。
我们将会在下面的网格搜索（grid search）小节中看到它们的用法。
现在我们可以使用下面的一行命令来训练模型:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text_clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>  
<span class="calibre4">Pipeline(...)</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1057">
<h2 class="sigil_not_in_toc">在测试集上的性能评估</h2>
<p class="calibre2">评估模型的预测准确度同样很简单:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">import</span> <span class="calibre4">numpy</span> <span class="calibre4">as</span> <span class="calibre4">np</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">twenty_test</span> <span class="calibre4">=</span> <span class="calibre4">fetch_20newsgroups</span><span class="calibre4">(</span><span class="calibre4">subset</span><span class="calibre4">=</span><span class="calibre4">'test'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">categories</span><span class="calibre4">=</span><span class="calibre4">categories</span><span class="calibre4">,</span> <span class="calibre4">shuffle</span><span class="calibre4">=</span><span class="calibre4">True</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">42</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">docs_test</span> <span class="calibre4">=</span> <span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">data</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">predicted</span> <span class="calibre4">=</span> <span class="calibre4">text_clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">docs_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">(</span><span class="calibre4">predicted</span> <span class="calibre4">==</span> <span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>            
<span class="calibre4">0.834...</span>
</pre>
</div>
</div>
<p class="calibre10">如上, 我们模型的准确度为 83.4%.
我们使用线性分类模型 <a class="calibre3 pcalibre" href="../../modules/svm.html#svm"><span class="calibre4">支持向量机（SVM）</span></a> ， 一种公认的最好的文本分类算法（尽管训练速度比朴素贝叶斯慢一点）。
我们仅需要在 Pipeline（管道）中插入不同的分类器对象就可以改变我们的学习器:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.linear_model</span> <span class="calibre4">import</span> <span class="calibre4">SGDClassifier</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text_clf</span> <span class="calibre4">=</span> <span class="calibre4">Pipeline</span><span class="calibre4">([(</span><span class="calibre4">'vect'</span><span class="calibre4">,</span> <span class="calibre4">CountVectorizer</span><span class="calibre4">()),</span>
<span class="calibre4">... </span>                     <span class="calibre4">(</span><span class="calibre4">'tfidf'</span><span class="calibre4">,</span> <span class="calibre4">TfidfTransformer</span><span class="calibre4">()),</span>
<span class="calibre4">... </span>                     <span class="calibre4">(</span><span class="calibre4">'clf'</span><span class="calibre4">,</span> <span class="calibre4">SGDClassifier</span><span class="calibre4">(</span><span class="calibre4">loss</span><span class="calibre4">=</span><span class="calibre4">'hinge'</span><span class="calibre4">,</span> <span class="calibre4">penalty</span><span class="calibre4">=</span><span class="calibre4">'l2'</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                           <span class="calibre4">alpha</span><span class="calibre4">=</span><span class="calibre4">1e-3</span><span class="calibre4">,</span> <span class="calibre4">random_state</span><span class="calibre4">=</span><span class="calibre4">42</span><span class="calibre4">,</span>
<span class="calibre4">... </span>                                           <span class="calibre4">max_iter</span><span class="calibre4">=</span><span class="calibre4">5</span><span class="calibre4">,</span> <span class="calibre4">tol</span><span class="calibre4">=</span><span class="calibre4">None</span><span class="calibre4">)),</span>
<span class="calibre4">... </span><span class="calibre4">])</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">text_clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">,</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>  
<span class="calibre4">Pipeline(...)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">predicted</span> <span class="calibre4">=</span> <span class="calibre4">text_clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">(</span><span class="calibre4">docs_test</span><span class="calibre4">)</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">np</span><span class="calibre4">.</span><span class="calibre4">mean</span><span class="calibre4">(</span><span class="calibre4">predicted</span> <span class="calibre4">==</span> <span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">)</span>            
<span class="calibre4">0.912...</span>
</pre>
</div>
</div>
<p class="calibre10"><code class="docutils"><span class="calibre4">scikit-learn</span></code> 同样提供了更加细节化的模型评估工具:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn</span> <span class="calibre4">import</span> <span class="calibre4">metrics</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">classification_report</span><span class="calibre4">(</span><span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">predicted</span><span class="calibre4">,</span>
<span class="calibre4">... </span>    <span class="calibre4">target_names</span><span class="calibre4">=</span><span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">))</span>
<span class="calibre4">... </span>                                        
<span class="calibre4">                        precision    recall  f1-score   support</span>

<span class="calibre4">           alt.atheism       0.95      0.81      0.87       319</span>
<span class="calibre4">         comp.graphics       0.88      0.97      0.92       389</span>
<span class="calibre4">               sci.med       0.94      0.90      0.92       396</span>
<span class="calibre4">soc.religion.christian       0.90      0.95      0.93       398</span>

<span class="calibre4">           avg / total       0.92      0.91      0.91      1502</span>


<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">metrics</span><span class="calibre4">.</span><span class="calibre4">confusion_matrix</span><span class="calibre4">(</span><span class="calibre4">twenty_test</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">,</span> <span class="calibre4">predicted</span><span class="calibre4">)</span>
<span class="calibre4">array([[258,  11,  15,  35],</span>
<span class="calibre4">       [  4, 379,   3,   3],</span>
<span class="calibre4">       [  5,  33, 355,   3],</span>
<span class="calibre4">       [  5,  10,   4, 379]])</span>
</pre>
</div>
</div>
<p class="calibre10">从 confusion matrix（混淆矩阵）中可以看出，在 atheism 和 christian 两个类别的新闻比他们中某一类与 computer graphics 类别的新闻相比更容易混淆彼此。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1058">
<h2 class="sigil_not_in_toc">使用网格搜索进行参数调优</h2>
<p class="calibre2">我们已经接触了类似于 <code class="docutils"><span class="calibre4">TfidfTransformer</span></code> 中 <code class="docutils"><span class="calibre4">use_idf</span></code> 这样的参数 ，分类器有多种这样的参数；
比如， <code class="docutils"><span class="calibre4">MultinomialNB</span></code> 包含了平滑参数 <code class="docutils"><span class="calibre4">alpha</span></code> 以及 <code class="docutils"><span class="calibre4">SGDClassifier</span></code> 有惩罚参数 <code class="docutils"><span class="calibre4">alpha</span></code> 和设置损失以及惩罚因子（更多信息请使用 python 的 <code class="docutils"><span class="calibre4">help</span></code> 文档）。</p>
<p class="calibre10">通过构建巨大的网格搜索，而不是调整 chain（链）的各种组件的参数，来寻找最佳参数。
我们尝试所有情况的分类器：使用词袋或者二元模型，使用或者不使用 idf ，在线性 SVM 上设置 0.01 或者 0.001 的惩罚参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">from</span> <span class="calibre4">sklearn.model_selection</span> <span class="calibre4">import</span> <span class="calibre4">GridSearchCV</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">parameters</span> <span class="calibre4">=</span> <span class="calibre4">{</span><span class="calibre4">'vect__ngram_range'</span><span class="calibre4">:</span> <span class="calibre4">[(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">1</span><span class="calibre4">),</span> <span class="calibre4">(</span><span class="calibre4">1</span><span class="calibre4">,</span> <span class="calibre4">2</span><span class="calibre4">)],</span>
<span class="calibre4">... </span>              <span class="calibre4">'tfidf__use_idf'</span><span class="calibre4">:</span> <span class="calibre4">(</span><span class="calibre4">True</span><span class="calibre4">,</span> <span class="calibre4">False</span><span class="calibre4">),</span>
<span class="calibre4">... </span>              <span class="calibre4">'clf__alpha'</span><span class="calibre4">:</span> <span class="calibre4">(</span><span class="calibre4">1e-2</span><span class="calibre4">,</span> <span class="calibre4">1e-3</span><span class="calibre4">),</span>
<span class="calibre4">... </span><span class="calibre4">}</span>
</pre>
</div>
</div>
<p class="calibre10">很明显, 如此的搜索是非常耗时的。
如果我们有多个 CPU 核心可以使用，通过设置 <code class="docutils"><span class="calibre4">n_jobs</span></code> 参数能够进行并行处理。
如果我们将该参数设置为 <code class="docutils"><span class="calibre4">-1</span></code> ， 该方法会使用机器的所有 CPU 核心:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gs_clf</span> <span class="calibre4">=</span> <span class="calibre4">GridSearchCV</span><span class="calibre4">(</span><span class="calibre4">text_clf</span><span class="calibre4">,</span> <span class="calibre4">parameters</span><span class="calibre4">,</span> <span class="calibre4">n_jobs</span><span class="calibre4">=-</span><span class="calibre4">1</span><span class="calibre4">)</span>
</pre>
</div>
</div>
<p class="calibre10">网格搜索在 <code class="docutils"><span class="calibre4">scikit-learn</span></code> 中是非常常见的。
让我们来选择训练集中的一小部分进行搜索以加速计算:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gs_clf</span> <span class="calibre4">=</span> <span class="calibre4">gs_clf</span><span class="calibre4">.</span><span class="calibre4">fit</span><span class="calibre4">(</span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">data</span><span class="calibre4">[:</span><span class="calibre4">400</span><span class="calibre4">],</span> <span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target</span><span class="calibre4">[:</span><span class="calibre4">400</span><span class="calibre4">])</span>
</pre>
</div>
</div>
<p class="calibre10">上面的操作在 <code class="docutils"><span class="calibre4">GridSearchCV</span></code> 中调用了 <code class="docutils"><span class="calibre4">fit</span></code>，因此我们能够调用 <code class="docutils"><span class="calibre4">predict</span></code>:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">twenty_train</span><span class="calibre4">.</span><span class="calibre4">target_names</span><span class="calibre4">[</span><span class="calibre4">gs_clf</span><span class="calibre4">.</span><span class="calibre4">predict</span><span class="calibre4">([</span><span class="calibre4">'God is love'</span><span class="calibre4">])[</span><span class="calibre4">0</span><span class="calibre4">]]</span>
<span class="calibre4">'soc.religion.christian'</span>
</pre>
</div>
</div>
<p class="calibre10">对象的 <code class="docutils"><span class="calibre4">best_score_</span></code> 和 <code class="docutils"><span class="calibre4">best_params_</span></code> 属性存放了最佳的平均分数以及其所对应的参数:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">gs_clf</span><span class="calibre4">.</span><span class="calibre4">best_score_</span>                                  
<span class="calibre4">0.900...</span>
<span class="calibre4">&gt;&gt;&gt; </span><span class="calibre4">for</span> <span class="calibre4">param_name</span> <span class="calibre4">in</span> <span class="calibre4">sorted</span><span class="calibre4">(</span><span class="calibre4">parameters</span><span class="calibre4">.</span><span class="calibre4">keys</span><span class="calibre4">()):</span>
<span class="calibre4">... </span>    <span class="calibre4">print</span><span class="calibre4">(</span><span class="calibre4">"</span><span class="calibre4">%s</span><span class="calibre4">: </span><span class="calibre4">%r</span><span class="calibre4">"</span> <span class="calibre4">%</span> <span class="calibre4">(</span><span class="calibre4">param_name</span><span class="calibre4">,</span> <span class="calibre4">gs_clf</span><span class="calibre4">.</span><span class="calibre4">best_params_</span><span class="calibre4">[</span><span class="calibre4">param_name</span><span class="calibre4">]))</span>
<span class="calibre4">...</span>
<span class="calibre4">clf__alpha: 0.001</span>
<span class="calibre4">tfidf__use_idf: True</span>
<span class="calibre4">vect__ngram_range: (1, 1)</span>
</pre>
</div>
</div>
<p class="calibre10">更多的详细信息可以在 <code class="docutils"><span class="calibre4">gs_clf.cv_results_</span></code> 中得到。</p>
<p class="calibre10"><code class="docutils"><span class="calibre4">cv_results_</span></code> 参数能够容易的被导入到 pandas 的 <a href="#calibre_link-1" class="calibre3 pcalibre"><span class="calibre4" id="calibre_link-1059">``</span></a>DataFrame``中，供后期使用。</p>
<div class="toctree-wrapper" id="calibre_link-1060">
<h3 class="sigil_not_in_toc1">练习</h3>
<p class="calibre2">为了做这个练习，请拷贝 ‘skeletons’ 文件夹到新的文件夹，并将其命名为 ‘workspace’:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">%</span> <span class="calibre4">cp</span> <span class="calibre4">-</span><span class="calibre4">r</span> <span class="calibre4">skeletons</span> <span class="calibre4">workspace</span>
</pre>
</div>
</div>
<p class="calibre10">这时候可以任意更改练习的代码而不会破坏原始的代码结构。</p>
<p class="calibre10">然后启动 ipython 交互环境，并键入以下代码:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">[</span><span class="calibre4">1</span><span class="calibre4">]</span> <span class="calibre4">%</span><span class="calibre4">run</span> <span class="calibre4">workspace</span><span class="calibre4">/</span><span class="calibre4">exercise_XX_script</span><span class="calibre4">.</span><span class="calibre4">py</span> <span class="calibre4">arg1</span> <span class="calibre4">arg2</span> <span class="calibre4">arg3</span>
</pre>
</div>
</div>
<p class="calibre10">如果出现错误, 请使用 <code class="docutils"><span class="calibre4">%debug</span></code> 来启动 ipdb 调试环境。</p>
<p class="calibre10">迭代更改答案直到练习完成。</p>
<p class="calibre10"><strong class="calibre14">在每个练习中, skeleton 文件提供了所有必需的import语句，加载数据的模板代码以及评估模型准确度的样例代码.</strong></p>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1061">
<h2 class="sigil_not_in_toc">练习 1：语言识别</h2>
<ul class="calibre6">
<li class="toctree-l">请使用自定义的预处理器和 <code class="docutils"><span class="calibre4">CharNGramAnalyzer</span></code> ，并且使用维基百科中的文章作为训练集，来编写一个文本分类的 Pipeline（管道）。</li>
<li class="toctree-l">评估某些测试集的性能.</li>
</ul>
<p class="calibre10">ipython command line:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">%</span><span class="calibre4">run</span> <span class="calibre4">workspace</span><span class="calibre4">/</span><span class="calibre4">exercise_01_language_train_model</span><span class="calibre4">.</span><span class="calibre4">py</span> <span class="calibre4">data</span><span class="calibre4">/</span><span class="calibre4">languages</span><span class="calibre4">/</span><span class="calibre4">paragraphs</span><span class="calibre4">/</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1062">
<h2 class="sigil_not_in_toc">练习 2：电影评论的情感分析</h2>
<ul class="calibre6">
<li class="toctree-l">编写一个文本分类 Pipeline（管道）来将电影评论分类为积极的（positive）还是消极的（negative）。</li>
<li class="toctree-l">使用网格搜索来找到好的参数集。</li>
<li class="toctree-l">使用测试集进行性能评估。</li>
</ul>
<p class="calibre10">ipython 命令行:</p>
<div class="toctree-wrapper"><div class="toctree-wrapper"><pre class="calibre12"><span class="calibre4"></span><span class="calibre4">%</span><span class="calibre4">run</span> <span class="calibre4">workspace</span><span class="calibre4">/</span><span class="calibre4">exercise_02_sentiment</span><span class="calibre4">.</span><span class="calibre4">py</span> <span class="calibre4">data</span><span class="calibre4">/</span><span class="calibre4">movie_reviews</span><span class="calibre4">/</span><span class="calibre4">txt_sentoken</span><span class="calibre4">/</span>
</pre>
</div>
</div>
</div>
<div class="toctree-wrapper" id="calibre_link-1063">
<h2 class="sigil_not_in_toc">练习 3：CLI 文本分类实用程序</h2>
<p class="calibre2">使用前面的练习结果以及标准库的 <code class="docutils"><span class="calibre4">cPickle``模块，编写一个命令行工具来检测由</span> <span class="calibre4">``stdin</span></code> 输入的文本，并评估该文本的极性（积极的还是消极的），如果输入的文本是英文的话。</p>
<p class="calibre10">加分项：该工具能够给出其预测的置信水平。</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1064">
<h2 class="sigil_not_in_toc">快速链接</h2>
<p class="calibre2">当你完成这个章节时，下面是几个建议帮助你进一步使用 scikit-learn :</p>
<ul class="calibre6">
<li class="toctree-l">尝试使用 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 类下的 <code class="docutils"><span class="calibre4">analyzer</span></code> 以及 <code class="docutils"><span class="calibre4">token</span> <span class="calibre4">normalisation</span></code> 。</li>
<li class="toctree-l">如果你没有标签，使用 <a class="calibre3 pcalibre" href="../../auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py"><span class="calibre4">聚类</span></a> 来解决你的问题。</li>
<li class="toctree-l">如果每篇文章有多个标签，请参考 <a class="calibre3 pcalibre" href="../../modules/multiclass.html#multiclass"><span class="calibre4">多类别和多标签部分</span></a> _ 。</li>
<li class="toctree-l">使用 <a class="calibre3 pcalibre" href="../../modules/decomposition.html#lsa"><span class="calibre4">Truncated SVD</span></a> 解决 <a class="calibre3 pcalibre" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">隐语义分析</a>.</li>
<li class="toctree-l">使用 <a class="calibre3 pcalibre" href="../../auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="calibre4">Out-of-core Classification</span></a> 来学习数据，不会存入计算机的主存储器中。</li>
<li class="toctree-l">使用 <a class="calibre3 pcalibre" href="../../modules/feature_extraction.html#hashing-vectorizer"><span class="calibre4">Hashing Vectorizer</span></a> 来节省内存，以代替 <a class="calibre3 pcalibre" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="docutils"><span class="calibre4">CountVectorizer</span></code></a> 。</li>
</ul>
</div>
</div>


<div class="calibre" id="calibre_link-3">
<h1 class="calibre1">选择正确的评估器(estimator)</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@李孟禹</a><br class="calibre9" /> 
    </div>
<p class="calibre10">通常，解决机器学习问题的最困难的部分可能是找到恰当的的评估器(estimator)。</p>
<p class="calibre10">不同的评估器更适合不同类型的数据和不同的问题。</p>
<p class="calibre10">下面的流程图是一些粗略的指导，可以让用户根据自己的数据来选择应该尝试哪些评估器。</p>
<p class="calibre10">点击下图的任何评估器，查看其文档。</p>

</div>


<div class="calibre" id="calibre_link-9">
<h1 class="calibre1">外部资源，视频和谈话</h1>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@巴黎灬メの雨季</a><br class="calibre9" /> 
    </div>
<div class="toctree-wrapper">
      校验者: <br class="calibre9" />
      翻译者: <br class="calibre9" />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/apachecn/scikit-learn-doc-zh" class="calibre3 pcalibre">@巴黎灬メの雨季</a><br class="calibre9" />
    </div>
<p class="calibre10">For written tutorials, see the <a class="calibre3 pcalibre" href="tutorial/index.html#tutorial-menu"><span class="calibre4">Tutorial section</span></a> of
the documentation.</p>
<div class="toctree-wrapper" id="calibre_link-1065">
<h2 class="sigil_not_in_toc">Scientific Python 的新手？</h2>
<p class="calibre2">For those that are still new to the scientific Python ecosystem, we highly
recommend the <a class="calibre3 pcalibre" href="http://www.scipy-lectures.org/">Python Scientific Lecture Notes</a>. This will help you find your footing a
bit and will definitely improve your scikit-learn experience.  A basic
understanding of NumPy arrays is recommended to make the most of scikit-learn.</p>
</div>
<div class="toctree-wrapper" id="calibre_link-1066">
<h2 class="sigil_not_in_toc">外部教程</h2>
<p class="calibre2">There are several online tutorials available which are geared toward
specific subject areas:</p>
<ul class="calibre6">
<li class="toctree-l"><a class="calibre3 pcalibre" href="http://nilearn.github.io/">Machine Learning for NeuroImaging in Python</a></li>
<li class="toctree-l"><a class="calibre3 pcalibre" href="https://github.com/astroML/sklearn_tutorial">Machine Learning for Astronomical Data Analysis</a></li>
</ul>
</div>
<div class="toctree-wrapper" id="calibre_link-1067">
<span id="calibre_link-1068" class="calibre4"></span><h2 class="sigil_not_in_toc">视频</h2>
<ul class="calibre6">
<li class="toctree-l"><p class="first">An introduction to scikit-learn <a class="calibre3 pcalibre" href="https://conference.scipy.org/scipy2013/tutorial_detail.php?id=107">Part
I</a> and
<a class="calibre3 pcalibre" href="https://conference.scipy.org/scipy2013/tutorial_detail.php?id=111">Part II</a> at Scipy 2013
by <a class="calibre3 pcalibre" href="http://gael-varoquaux.info">Gael Varoquaux</a>, <a class="calibre3 pcalibre" href="http://staff.washington.edu/jakevdp">Jake Vanderplas</a>  and <a class="calibre3 pcalibre" href="https://twitter.com/ogrisel">Olivier Grisel</a>. Notebooks on
<a class="calibre3 pcalibre" href="https://github.com/jakevdp/sklearn_scipy2013">github</a>.</p>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="http://videolectures.net/icml2010_varaquaux_scik/">Introduction to scikit-learn</a> by <a class="calibre3 pcalibre" href="http://gael-varoquaux.info">Gael Varoquaux</a> at
ICML 2010</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">A three minute video from a very early stage of the scikit, explaining the
basic idea and approach we are following.</p>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="http://archive.org/search.php?query=scikit-learn">Introduction to statistical learning with scikit-learn</a>
by <a class="calibre3 pcalibre" href="http://gael-varoquaux.info">Gael Varoquaux</a> at SciPy 2011</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">An extensive tutorial, consisting of four sessions of one hour.
The tutorial covers the basics of machine learning,
many algorithms and how to apply them using scikit-learn. The
material corresponding is now in the scikit-learn documentation
section <a class="calibre3 pcalibre" href="tutorial/statistical_inference/index.html#stat-learn-tut-index"><span class="calibre4">关于科学数据处理的统计学习教程</span></a>.</p>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="http://www.pyvideo.org/video/417/pycon-2011--statistical-machine-learning-for-text">Statistical Learning for Text Classification with scikit-learn and NLTK</a>
(and <a class="calibre3 pcalibre" href="http://www.slideshare.net/ogrisel/statistical-machine-learning-for-text-classification-with-scikitlearn-and-nltk">slides</a>)
by <a class="calibre3 pcalibre" href="https://twitter.com/ogrisel">Olivier Grisel</a> at PyCon 2011</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">Thirty minute introduction to text classification. Explains how to
use NLTK and scikit-learn to solve real-world text classification
tasks and compares against cloud-based solutions.</p>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="https://www.youtube.com/watch?v=Zd5dfooZWG4">Introduction to Interactive Predictive Analytics in Python with scikit-learn</a>
by <a class="calibre3 pcalibre" href="https://twitter.com/ogrisel">Olivier Grisel</a> at PyCon 2012</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">3-hours long introduction to prediction tasks using scikit-learn.</p>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="https://newcircle.com/s/post/1152/scikit-learn_machine_learning_in_python">scikit-learn - Machine Learning in Python</a>
by <a class="calibre3 pcalibre" href="http://staff.washington.edu/jakevdp">Jake Vanderplas</a> at the 2012 PyData workshop at Google</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">Interactive demonstration of some scikit-learn features. 75 minutes.</p>
</div>
</blockquote>
</li>
<li class="toctree-l"><p class="first"><a class="calibre3 pcalibre" href="https://vimeo.com/53062607">scikit-learn tutorial</a> by <a class="calibre3 pcalibre" href="http://staff.washington.edu/jakevdp">Jake Vanderplas</a> at PyData NYC 2012</p>
<blockquote class="calibre15">
<div class="toctree-wrapper"><p class="calibre10">Presentation using the online tutorial, 45 minutes.</p>
</div>
</blockquote>
</li>
</ul>
</div>
</div>


</body></html>